<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DataScience | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/datascience/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-04-30T21:19:27+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习12: Logisic回归]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/27/logistic-regression/"/>
    <updated>2014-04-27T10:34:11+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/27/logistic-regression</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical27.jpg" alt="artical 27" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在数据科学系列的开头，花了三篇介绍了线性回归。线性回归模型应该是定量分析中最常用的一种统计分析方法。但是线性回归处理数据时，要求因变量是连续型变量。但是很多时候，需要处理的数据，其因变量并不是连续的。像性别、对错等等，这种离散的因变量，一般称为分类响应变量。</p>

<p>在机器学习的<a href="http://jackycode.github.io/blog/2014/03/30/data-science-an-introduction-to-machine-learning/">介绍篇</a>中，已经介绍了监督学习与非监督学习。在监督学习中，最主要的两类问题，一个就是回归，另一个就是分类。而Logistic回归就是处理二元分类的一种方法，当然其也存在自身的问题，这个后面再讲。</p>

<h4 id="sigmoid">Sigmoid函数</h4>
<hr />

<p>要了解Logistic回归，首先需要了解一下Sigmoid函数。为什么呢？</p>

<p>一般来说，我们会定义二元分类变量的输出为0和1，这种函数叫做单位阶跃函数，也称Heaviside step function。这个函数的特点就是其取值可以从0突变到1，反之也可。学过数分的话就知道，这种函数有时候会非常难以处理，因为带跳突变，导致了不可微不可导。在这里，就可以借助Sigmoid函数，因为这个函数可以近似地描述单位阶跃函数的特点。首先看看Sigmoid函数：</p>

<script type="math/tex; mode=display"> p = \frac{exp(y)}{1+exp(y)} = \frac{1}{1 + exp(-y)}</script>

<p>简单看一下这个函数，当$y=0$时，$p=0.5$；当$y$变大，趋近于无穷时，$y$趋近1；反之，$y$趋近0。而且，这种趋近的速度是非常快的。正是因为这个趋近速度非常快，我们可以使用Sigmoid函数来处理这边的单位阶跃函数。</p>

<h4 id="logistic">Logistic回归</h4>
<hr />

<p>利用Sigmoid函数可以将单位阶跃函数做个近似，而Sigmoid函数是连续的，那么就可以利用之前的线性回归来建立模型。</p>

<p>令$ y = X\beta $，又<script type="math/tex"> p = \frac{1}{1 + exp(-y)} </script>，变形可得Logistic回归模型：</p>

<script type="math/tex; mode=display"> logit(p) = ln(\frac{p}{1-p}) = X\beta </script>

<p>上述的$logit(p)$称为$logit$变换，此时$p$就是响应变量，$X$就是自变量。到这，我们就可以利用线性模型对参数进行估计了。</p>

<h5 id="section">统计中的解释</h5>
<hr />

<p>这里面的$p$除了利用Sigmoid函数来解释之外，还可以利用统计中的二项分布来解释，而且从某种角度来说，这个解释会更便于理解。试想，我们这边需要处理的二元分类变量就是0和1。我们考虑0就是“不发生”，1就是“发生”，那么我们可以将前面的$p$理解成发生的概率。通过对已知数据建立模型，估计出参数，我们就可以利用模型去预测在不同的自变量条件下，“发生”的概率是多大，从而达到一个分类的目的。</p>

<p>从这边的分析就可以看到，Logistic回归的缺点：那就是欠拟合，会导致分类的精度下降。</p>

<h5 id="r">R语言实现</h5>
<hr />

<p>因为这个属于线性回归的一种变形，所以求解的方式直接<strong>借用线性回归</strong>即可。</p>

<p>当然，也存在其它的方法。因为Logistic回归是属于广义线性回归模型的，在R中有专门处理广义线性模型的函数<code>glm</code>：</p>

<p><code>
model &lt;- glm(formula, family=binominal(link = logit), data=data.frame)
</code></p>

<p>这里处理的方式中，利用了连接函数(link=logit)，感兴趣的话可以找找广义线性模型的内容看看，当然，以后如果介绍统计模型的话，这个肯定也是必讲得内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习11: 聚类分析2]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/24/cluster-analysis2/"/>
    <updated>2014-04-24T09:39:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/24/cluster-analysis2</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical26.jpg" alt="artical 26" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇介绍了聚类分析的定义，给出了很多不同的相似性的度量方法。这一篇主要想介绍一下除了快速聚类之外的另外一种聚类方法：<strong>系统聚类法</strong>。</p>

<h3 id="section">二、系统聚类法</h3>
<hr />

<p>系统聚类法，hierarchical clustering method，是聚类分析方法中用的较多的一种。其具体过程如下：</p>

<ol>
  <li>对于n个样品，构造n个类，每个样品单独作为一类。计算每个类之间的距离；</li>
  <li>合并距离最近的两个类为一个新类；</li>
  <li>计算新类与其它类之间的距离，重复2直到所有类合并称为1个类为止。</li>
</ol>

<p>那么类与类之间的距离如何定义呢？</p>

<p>其实，类与类之间的距离有很多中定义方式常见的有：</p>

<ol>
  <li>最短距离法，single linkage method，<script type="math/tex"> D_{KL} = \min_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>最长距离法，complete linkage method，<script type="math/tex"> D_{KL} = \max_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>中间距离法，median linkage method，即取最远距离与最近距离两者的中间距离；</li>
  <li>类平均法，average linkage method，<script type="math/tex"> D_{KL} = \frac{1}{n_Kn_L} \sum_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>重心法，centroid hierarchical method，即取类重心之间的距离；</li>
  <li>离差平方和法，Ward’s minimum variance method, 定义较为繁琐，可以自行Google；</li>
</ol>

<p><strong>注</strong>：系统聚类的方法并不困难，但是实现时会存在计算量的问题。系统聚类法一般是在样品间距离矩阵的基础上进行的，它需要计算所有点到所有点之间的距离，当样品量很大时，这个计算量会变得非常的大。因而，很多时候人们会采用动态聚类的方法去处理数据，动态聚类法中一种最常用的方法就是之前已经介绍过的KMeans方法。</p>

<h4 id="r">R语言实现</h4>

<p>在R语言中，自带了一个函数可以实现系统聚类：<code>hclust</code>。可以自己查阅help。</p>

<h3 id="section-1">三、聚类的一些问题</h3>
<hr />

<ol>
  <li>量纲问题。实际问题中，由于数据采用的量纲不同，很多时候需要对数据进行一些变换，最常用的就是标准化。但也有一些其它方式：极差变换（数据除以极差）；主成分变换（用主成分代替本身数据）；对数变换等等。</li>
  <li>kmeans算法只有在类的平均值可以被定义的情况下使用，所以在一些特殊的场合，kmeans并不适用。比如分类数据等等!!</li>
  <li>kmeans算法使用平均值作为衡量，这就造成了一个新的问题。即kmeans不适用于含有异常值的数据，非凸面的数据以及大小值相差很大的数据。</li>
  <li>聚类的一个<strong>难点</strong>在于：确定类的个数。通过上面介绍的算法来看，所有的方法都需要自己去定义类的个数。那么如何去定义类的个数呢？这是一个到现在还没有满意解决的问题。常用的方式就是观察样品散点图，查看变化率，以及使用一些假设检验的方式（感兴趣可以翻阅专业的书籍材料，比如上海财经出版社的应用多元分析中就有讲到这部分内容）。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习10: 聚类分析1]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/21/cluster-analysis/"/>
    <updated>2014-04-21T18:46:04+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/21/cluster-analysis</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical25.jpg" alt="artical 25" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇介绍了聚类分析中的KMeans算法，这一节就来具体地说说聚类分析。聚类分析，cluster analysis，是一种研究“物以类聚”现代统计学分析方法，其目的是要把分类对象按照一定的规则分成若干个类。这些类别并非事先给定的，而是根据数据的特征确定的。</p>

<hr />

<h5 id="note">NOTE：聚类的划分：</h5>

<ol>
  <li>
    <p>根据分类对象的不同，可以分为：<strong>Q型聚类分析</strong>和<strong>R型聚类分析</strong>。Q型是指对样品对象进行聚类；而R型则是对变量(属性)进行聚类。</p>
  </li>
  <li>
    <p>按照分析方法的不同，又可以分为：<strong>系统聚类法</strong>、<strong>快速聚类法</strong>和<strong>模糊聚类法</strong>。上一篇介绍的KMeans法就是快速聚类法中的一种。</p>
  </li>
</ol>

<hr />

<h4 id="section">一、相似性的度量</h4>
<hr />

<p>在上一篇中，我们已经介绍过，聚类其实就是将相似度高的样品啊属性啊合并成一个类别。但是，上一篇我们仅仅给出了一种也是最简单的一种相似性的度量方式——欧式距离。这里我们详细看看相似性有哪些度量方式：</p>

<p>除了使用<strong>有序尺度变量</strong>（将属性划分为一级、二级等等的有次序关系的量来表示）和<strong>名义尺度变量</strong>（使用既没有等级关系，又不存在数量关系的量来表示。比如男女）之外，一般采用的测量尺度的方式就是<strong>间隔尺度变量</strong>。</p>

<p><strong>间隔尺度变量</strong>即是使用连续的量来表示测量尺度，一般都是连续型的，比如欧式距离、重量等等。一般来讲，在应对Q型聚类时会使用<strong>距离</strong>去度量；而对R型聚类来说，则会使用<strong>相似系数</strong>这种方式去度量。下面来分别看一看：</p>

<hr />

<h5 id="a-">a. 距离</h5>
<hr />

<p>上一篇中使用的欧氏距离即是这里的一种，在介绍各种不同的距离定义之前，首先看看距离的定义需要满足哪些条件：</p>

<ol>
  <li>首先，距离必须是非负的。即：<script type="math/tex">d_{ij} \geqslant 0, \forall i,j</script>;</li>
  <li>对于相同取值的样品，之间的距离必须为0。即：<script type="math/tex">d_{ij} = 0</script>，当且仅当，第i个样品与第j个样品的各变量值相同；</li>
  <li>i样品到j样品的距离与j样品到i样品的距离相等。即：<script type="math/tex">d_{ij} = d_{ji}, \forall i, j</script>；</li>
  <li>满足：<script type="math/tex">d_{ij} \leqslant d_{ik} + d_{kj}, \forall i,j,k</script>。</li>
</ol>

<p>下面就来看看常用的距离定义，首先看看最常用的Minkowski距离：</p>

<hr />

<h6 id="minkowski">(1). Minkowski距离</h6>

<script type="math/tex; mode=display"> d_{ij}(q) = [\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid ^ q}]^{1/q} </script>

<p>观察这个距离可以看到，当$q=2$时，上面定义的距离就是常用的欧氏距离。另外：</p>

<ul>
  <li><script type="math/tex">q=1</script>时，<script type="math/tex">d_{ij}=\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid}</script>称为<strong>绝对值距离</strong>；</li>
  <li><script type="math/tex">q=\infty</script>时，<script type="math/tex">d_{ij}=\max_{1 \leqslant k \leqslant p} {\mid x_{ik} - x_{jk} \mid}</script>称为<strong>切比雪夫距离</strong>。</li>
</ul>

<p>Minkowski距离存在一个问题，就是当变量的单位不同或者测量值范围相差很大时，直接使用Minkowski距离效果不佳。这个时候，应该先对数据进行<strong>标准化</strong>（就是减去均值除上标准差）之后再计算距离(这个后面还会说到)。</p>

<hr />

<h6 id="lancelance-and-williams">(2). Lance距离(Lance and Williams)</h6>

<p>当$x_{ji} &gt; 0$时，定义第i个样品到第j个样品的距离为：</p>

<script type="math/tex; mode=display"> d_{ij} = \sum_{k=1}^{p} {\frac{\mid x_{ik} - x_{jk} \mid}{x_{ik} + x_{jk}}} </script>

<p>从公式就可以看出来，这个距离与变量之间的单位没有什么关系；而且其对异常值也不敏感，因而适用于一些高度偏斜的数据。</p>

<hr />

<h6 id="mahalanobis">(3). Mahalanobis距离(马氏距离)</h6>

<p>上面的两种距离都没有考虑变量之间的相关性问题，马氏距离就可以考虑到这个问题。但是由于马氏距离定义的问题，在聚类分析中使用马氏距离并不合适。但是这里也还是给出马氏距离的定义：</p>

<script type="math/tex; mode=display"> d_{ij} = \sqrt{(x_i - x_j)^TS^{-1}(x_i - x_j)} </script>

<p>其中<script type="math/tex">x_i = (x_{i1}, \dots, x_{ip})^T</script>，<script type="math/tex">x_j = (x_{j1}, \dots, x_{jp})^T</script>，<script type="math/tex">S</script>为样本协方差阵。</p>

<p><strong><em>注：</em></strong>为什么说马氏距离不适用与聚类分析呢？</p>

<p>聚类分析是无监督算法中的一种，无监督算法是什么？无监督算法是没有先验信息的，所有的数据拿过来是没有什么目标信息啊什么的。没有不同类之间的先验信息，那么协方差阵<script type="math/tex">S</script>就无法计算。因而，在实际聚类分析中，马氏距离并不适用。</p>

<hr />

<h6 id="section-1">(4). 斜交空间距离</h6>

<script type="math/tex; mode=display"> d_{ij} = [ \frac{1}{p^2} \sum_{k=1}^{p} \sum_{l=1}^{p} (x_{ik} -x_{jk})(x_{il} - x_{jl})r_{kl} ] ^ {1/2} </script>

<p>其中<script type="math/tex">r_{kl}</script>是变量<script type="math/tex">x_k</script>与变量<script type="math/tex">x_l</script>的相关系数。学过高等代数的应该可以很容易看明白这个定义。此外，当变量之间互不相关的时候，这里的<script type="math/tex">d_{ij} = [d_{ij}(2)/p]_{Minkowski}</script>，也就是退化到了欧氏距离（相差一个常数倍）。</p>

<hr />

<h5 id="b-">b. 相似系数</h5>
<hr />

<p>对变量进行聚类时，通常使用相似系数来考量其间的相似度。那么相似系数的定义有需要满足哪些条件呢？</p>

<ol>
  <li>完全相关。即：$c_{ij} = \pm 1$，当且仅当$x_i = ax_j + b;a(\neq 0),b$是常数；</li>
  <li><script type="math/tex">\mid c_{ij} \mid \leqslant 1, \forall i, j </script>；</li>
  <li><script type="math/tex">c_{ij} = c_{ji}, \forall i, j </script>。</li>
</ol>

<p>下面看看常用的两种相似系数：</p>

<hr />

<h6 id="section-2">(1). 夹角余弦</h6>
<hr />

<p>变量$x_i$和$x_j$的夹角余弦的定义为：</p>

<script type="math/tex; mode=display"> c_{ij} = \frac{\sum_{k=1}^{n} {x_{ki}x_{kj}} }{ [ (\sum_{k=1}^{n}{ x^2_{ki} })(\sum_{k=1}^{n} {x^2_{kj}} ) ]^{1/2} } </script>

<p>学过解析几何应该很容易看出这个定义的含义所在，其实<script type="math/tex">c_{ij} = \cos \theta_{ij}</script>。</p>

<hr />

<h6 id="section-3">(2). 相关系数</h6>
<hr />

<script type="math/tex; mode=display"> c_{ij} = \frac { \sum _{ k=1 }^{ n }{ ({ x }_{ ki }-\overline{x_i})({ x }_{ kj }-\overline{x_j}) }  }{ \{ [\sum_{k=1}^{n}({ x }_{ ki }-\overline{x_i})^2][\sum_{k=1}^{n}({ x }_{ kj }-\overline{x_j})^2] \}^{1/2} } </script>

<p>这里的相关系数其实就是统计里面通常所说的相关系数。其实，如果变量都是标准化了的，那么夹角余弦就是相关系数，看出来了吗？</p>

<hr />

<h4 id="section-4">小节</h4>
<hr />

<p>到这边，就把统计中常用的用于度量相似性的定义讲了一些。这些定义，大都有其自身的数学背景。有些来自于几何学，有些来自于线性空间理论。对于使用者来说，搞明白什么时候选择什么样的度量方式更加重要！下一篇，我们讲一讲聚类分析中的一个常用方法：<strong>系统聚类法</strong>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习9: 聚类算法之KMeans]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/19/kmeans/"/>
    <updated>2014-04-19T13:01:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/19/kmeans</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical24.jpg" alt="artical 24" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h2 id="the-k-means-algorithm">The k-means algorithm</h2>
<hr />

<p>前面写了很多监督学习的东西，除了回归就是分类，今儿有点想换换口味，写写无监督学习的算法。k-means算法是一种聚类算法，聚类就是无监督学习里面的内容。那么先来说说聚类：</p>

<h3 id="section">一、聚类简介</h3>
<hr />

<ol>
  <li>
    <p>聚类是一种无监督学习方法，它主要就是将相似的对象归并到一个类别中。</p>
  </li>
  <li>
    <p>聚类分析的目的是把分类对象按照一定的规则，分成若干个类。这些类并不是事先给定的，而是在处理数据时，根据数据的特征确定的。因而，在处理之前无需对类的数目、结构等等作出假定(理论如此，实际应用时并不完全这样，以后讲聚类分析时再具体说)。</p>
  </li>
  <li>
    <p>在同一类别中，对象具有某种意义下的相似性；不同的类别中，对象具有某种意义下的不相似。</p>
  </li>
  <li>
    <p><strong>聚类与分类的最大不同</strong>：分类的目标事先已知，而聚类未知。正因为聚类产生的结构与分类相同，只是类别没有预先定义，所以聚类也被称为“无监督分类”。</p>
  </li>
</ol>

<h3 id="k-means">二、k-means法</h3>
<hr />

<p>k-means法是由MacQueen提出并命名的一种聚类算法。其使用聚类中的均值进行聚类划分，这样说不大好理解，可以先看一下算法的基本步骤：</p>

<ol>
  <li>从n个数据对象中任意取出k个样品对象作为初始聚类点（或者将所有数据分成k份，计算每一份中的重心（均值）作为初始聚类点）；</li>
  <li>对所有样品对象逐个归类，将每一个对象归入距离他最近的那个类（距离一般使用欧式距离），并将该类的凝聚点更新为这个类当前的均值；</li>
  <li>重复步骤2，直至所有对象都不能再分配为止。</li>
</ol>

<h3 id="r">三、R语言实现</h3>
<hr />

<h4 id="section-1">1. 自定义函数</h4>
<hr />

<p>自定义一个函数<code>se_kmeans</code>，使用这个函数可以通过输入需要聚类的数据集以及类别数目k，即可得到每一个样品的类别以及各个类别的中心。该函数使用欧式距离作为相似性的度量（当然还有很多其它方式，之后的聚类分析中会讲到），具体的程序可以在<a href="/datascience">我的项目</a>中找到，也可以直接到我的<a href="https://github.com/JackyCode/Data_Science">github</a>中查看源代码。这边给出测试代码以及结果：</p>

<p>``` r
x1 &lt;- matrix(rnorm(500, 1, 0.5), 100, 5)
x2 &lt;- matrix(rnorm(500, 2, 0.5), 100, 5)
x &lt;- rbind(x1, x2)</p>

<p>clusters &lt;- se_kmeans(x, 2)
clusters
plot(x, col=clusters$cluster, pch=as.character(clusters$cluster), cex=0.5)
points(clusters$center, col=’green’, pch=’o’, cex = 2)
```</p>

<p>得到如下的一张图</p>

<p><img src="\images\a24\kmeans1.jpg" alt="kmeans1" /></p>

<p>可以看到，分类效果是很好的。</p>

<h4 id="kmeans">2. 使用<code>kmeans</code>函数</h4>
<hr />

<p>可以使用<code>stats</code>包中的<code>kmeans</code>函数来实现，示例如下：</p>

<p>``` r
x1 &lt;- matrix(rnorm(500, 1, 0.5), 100, 5)
x2 &lt;- matrix(rnorm(500, 2, 0.5), 100, 5)
x &lt;- rbind(x1, x2)</p>

<p>clusters &lt;- kmeans(x, 2)
clusters
plot(x, col=clusters$cluster, pch=as.character(clusters$cluster), cex=0.5)
points(clusters$centers, col=’green’, pch=’o’, cex = 2)
```</p>

<p>得到这样一张图</p>

<p><img src="\images\a24\kmeans2.jpg" alt="kmeans2" /></p>

<p>从图中可以看出，kmeans的分类效果还是蛮不错的，当然我们可以计算一下误判，不过这显然是很小的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习8: 决策树之ID3]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/14/id3/"/>
    <updated>2014-04-14T16:02:45+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/14/id3</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical23.jpg" alt="artical 23" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>ID3算法的核心问题就在于：如何选取在决策树的每个节点处要测试的属性。那么如何去选择呢？当然，我们要选取<strong>分类能力最好的属性</strong>，那么怎么去确定哪个属性是分类能力最好的呢？ID3算法中，使用<strong>信息增益</strong>作为评判标准。在看信息增益之前，我们先看看这个决策树的构造过程：</p>

<h3 id="section">一、构造过程</h3>
<hr />

<ol>
  <li>选取<strong>分类能力最好的属性</strong>作为决策树根部节点的测试；</li>
  <li>为根节点属性的每一个可能值产生一个分支；</li>
  <li>以各个分支节点为根节点，重复上述过程。</li>
</ol>

<h3 id="section-1">二、信息增益</h3>
<hr />

<h4 id="section-2">1. 熵</h4>
<hr />

<p>在看信息增益之前，首先需要介绍一个概念，那就是<strong>香农熵</strong>，简称为<strong>熵</strong>。相信学过物理的应该大都听过这个名词，在热力学中不就有个熵增原理嘛。其实，<strong>熵是信息论中广泛使用的一个度量标准，刻画了任意样例集合的纯度。</strong></p>

<p><strong>熵是信息的期望值</strong>，所以可以用熵来刻画一个数据集的纯度。若用$x_i,i=1,2,\dots,n$来表示数据集所包含的属性，那么这个数据集的熵为：</p>

<script type="math/tex; mode=display"> H = - \sum_{i=1}^{n}{p(x_i)l(x_i)} </script>

<p>其中，$p(x_i)$表示选取$x_i$作为分类的最终类别的概率；$l(x_i)$为$x_i$的信息，定义为：<script type="math/tex"> l(x_i) = - \log_2p(x_i)</script>。</p>

<h4 id="section-3">2. 信息增益</h4>
<hr />

<p>有了熵之后就可以刻画一个数据集的纯度，也就是熵值。那么什么信息增益呢？</p>

<p>简单来说，<strong>一个属性的信息增益就是：使用这个属性分割样例集合而导致的熵值降低</strong>。那么要选取分类能力最好的属性，就是要选取使得信息增益最大的那个属性。</p>

<p>一个属性A对样例集合S的信息增益定义为：</p>

<script type="math/tex; mode=display"> Gain(S, A) = H(S) - \sum_{v \in A} { \frac{\# S_v}{\# S} H(S_v) } </script>

<p>其中，<script type="math/tex">S_v</script>表示集合S中，属性A取值为$v$的那部分数据；<script type="math/tex">\# S_v</script>表示，集合S中，属性A取值为$v$的个数；<script type="math/tex">\# S</script>表示集合S中观测的个数。</p>

<h4 id="section-4">3. 简单的例子</h4>
<hr />

<table>
  <thead>
    <tr>
      <th style="text-align: center">序号</th>
      <th style="text-align: center">age</th>
      <th style="text-align: center">income</th>
      <th style="text-align: center">buy_iphone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">no</td>
    </tr>
  </tbody>
</table>

<p>考虑上面这个问题，我们来计算一下各个属性的信息增益。</p>

<p>首先，我们可以看到，这个数据集S最终分类buy_iphone有两种取值：$yes,no$。则数据集S的熵值为：</p>

<script type="math/tex; mode=display"> H(S) = -\frac{4}{7} \log_2{\frac{4}{7} } - \frac{3}{7} \log_2{\frac{3}{7} }  \approx 0.985</script>

<p>若按照age分类，age有两个属性：$senior, youth$，分别有4个和3个。age = senior时，yes有2个，no有2个则有：</p>

<script type="math/tex; mode=display"> H_{age}(S_{senior}) = -\frac{2}{4} \log_2{\frac{2}{4}} - \frac{2}{4} \log_2{\frac{2}{4}} \approx 1 </script>

<script type="math/tex; mode=display"> H_{age}(S_{youth}) = -\frac{2}{3} \log_2 {\frac{2}{3}} - \frac{1}{3} \log_2 {\frac{1}{3}} \approx 0.918 </script>

<p>则有：</p>

<script type="math/tex; mode=display"> H_{age}(S) = \frac{4}{7} \times 1 + \frac{3}{7} \times 0.918 = 0.965 </script>

<p>则age属性的信息增益为：</p>

<script type="math/tex; mode=display"> H(S) - H_{age}(S) = 0.985 - 0.965 = 0.020 </script>

<p>那么属性income的信息增益怎么去计算，可以动手试试。</p>

<h3 id="id3">三、ID3算法的伪代码</h3>
<hr />

<h5 id="section-5">定义：</h5>
<ul>
  <li>data：为训练样本集</li>
  <li>label：为目标属性 （比如例子中的属性buy_iphone）</li>
  <li>attrs：出目标属性外，供算法学习测试使用的其它属性 （比如例子中的age和income属性）</li>
</ul>

<h5 id="section-6">伪代码：</h5>
<p>ID3(data, label, attrs)：</p>

<ol>
  <li>创建决策树的Root节点；</li>
  <li>若lable中取值单一，则返回 <code>label=label</code> 的单节点树；</li>
  <li>若attrs为空，则返回 <code>label=（data中取值最多的那个label）</code> 的单节点树；</li>
  <li>否则：
    <ol>
      <li>选取attrs中分类能力最好的属性作为Root的决策属性，记为A；</li>
      <li>对A的每一个可能取值vi：
        <ol>
          <li>在Root添加一个分支对应 <code>A = vi </code>；</li>
          <li>data_vi = data中 <code>A = vi</code> 的子集，label_vi 表示 data_vi 所对应的目标属性取值；</li>
          <li>若 data_vi 为空集：
            <ol>
              <li>在新分支下加一个叶子节点，节点 <code>label =（data中取值最多的那个label）</code> ;</li>
              <li>否则，加一个子树：ID3(data_vi, label_vi, attrs);</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>结束</li>
  <li>返回Root</li>
</ol>

<h3 id="r">四、R语言实现</h3>
<hr />

<p>见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
</feed>
