<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DataScience | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/datascience/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-04-14T16:26:47+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习8: 决策树之ID3]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/14/id3/"/>
    <updated>2014-04-14T16:02:45+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/14/id3</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical23.jpg" alt="artical 23" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>ID3算法的核心问题就在于：选取在决策树的每个节点处要测试的属性。那么如何去选择呢？当然，我们要选取<strong>分类能力最好的属性</strong>，那么怎么去确定哪个属性是分类能力最好的呢？ID3算法中，使用<strong>信息增益</strong>作为评判标准。在看信息增益之前，我们先看看这个决策树的构造过程：</p>

<h3 id="section">一、构造过程</h3>
<hr />

<ol>
  <li>选取<strong>分类能力最好的属性</strong>作为决策树根部节点的测试；</li>
  <li>为根节点属性的每一个可能值产生一个分支；</li>
  <li>以各个分支节点为根节点，重复上述过程。</li>
</ol>

<h3 id="section-1">二、信息增益</h3>
<hr />

<h4 id="section-2">1. 熵</h4>
<hr />

<p>在看信息增益之前，首先需要介绍一个概念，那就是<strong>香农熵</strong>，简称为<strong>熵</strong>。相信学过物理的应该大都听过这个名词，在热力学中不就有个熵增原理嘛。其实，<strong>熵是信息论中广泛使用的一个度量标准，刻画了任意样例集合的纯度。</strong></p>

<p><strong>熵是信息的期望值</strong>，所以可以用熵来刻画一个数据集的纯度。若用$x_i,i=1,2,\dots,n$来表示数据集所包含的属性，那么这个数据集的熵为：</p>

<script type="math/tex; mode=display"> H = - \sum_{i=1}^{n}{p(x_i)l(x_i)} </script>

<p>其中，$p(x_i)$表示选取$x_i$作为分类的最终类别的概率；$l(x_i)$为$x_i$的信息，定义为：<script type="math/tex"> l(x_i) = - \log_2p(x_i)</script>。</p>

<h4 id="section-3">2. 信息增益</h4>
<hr />

<p>有了熵之后就可以刻画一个数据集的纯度，也就是熵值。那么什么信息增益呢？</p>

<p>简单来说，<strong>一个属性的信息增益就是：使用这个属性分割样例集合而导致的熵值降低</strong>。那么要选取分类能力最好的属性，就是要选取使得信息增益最大的那个属性。</p>

<p>一个属性A对样例集合S的信息增益定义为：</p>

<script type="math/tex; mode=display"> Gain(S, A) = H(S) - \sum_{v \in A} { \frac{\# S_v}{\# S} H(S_v) } </script>

<p>其中，<script type="math/tex">S_v</script>表示集合S中，属性A取值为$v$的那部分数据；<script type="math/tex">\# S_v</script>表示，集合S中，属性A取值为$v$的个数；<script type="math/tex">\# S</script>表示集合S中观测的个数。</p>

<h4 id="section-4">3. 简单的例子</h4>
<hr />

<table>
  <thead>
    <tr>
      <th style="text-align: center">序号</th>
      <th style="text-align: center">age</th>
      <th style="text-align: center">income</th>
      <th style="text-align: center">buy_iphone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">no</td>
    </tr>
  </tbody>
</table>

<p>考虑上面这个问题，我们来计算一下各个属性的信息增益。</p>

<p>首先，我们可以看到，这个数据集S最终分类buy_iphone有两种取值：$yes,no$。则数据集S的熵值为：</p>

<script type="math/tex; mode=display"> H(S) = -\frac{4}{7} \log_2{\frac{4}{7} } - \frac{3}{7} \log_2{\frac{3}{7} }  \approx 0.985</script>

<p>若按照age分类，age有两个属性：$senior, youth$，分别有4个和3个。age = senior时，yes有2个，no有2个则有：</p>

<script type="math/tex; mode=display"> H_{age}(S_{senior}) = -\frac{2}{4} \log_2{\frac{2}{4}} - \frac{2}{4} \log_2{\frac{2}{4}} \approx 1 </script>

<script type="math/tex; mode=display"> H_{age}(S_{youth}) = -\frac{2}{3} \log_2 {\frac{2}{3}} - \frac{1}{3} \log_2 {\frac{1}{3}} \approx 0.918 </script>

<p>则有：</p>

<script type="math/tex; mode=display"> H_{age}(S) = \frac{4}{7} \times 1 + \frac{3}{7} \times 0.918 = 0.965 </script>

<p>则age属性的信息增益为：</p>

<script type="math/tex; mode=display"> H(S) - H_{age}(S) = 0.985 - 0.965 = 0.020 </script>

<p>那么属性income的信息增益怎么去计算，可以动手试试。</p>

<h3 id="id3">三、ID3算法的伪代码</h3>
<hr />

<h5 id="section-5">定义：</h5>
<ul>
  <li>data：为训练样本集</li>
  <li>label：为目标属性 （比如例子中的属性buy_iphone）</li>
  <li>attrs：出目标属性外，供算法学习测试使用的其它属性 （比如例子中的age和income属性）</li>
</ul>

<h5 id="section-6">伪代码：</h5>
<p>ID3(data, label, attrs)：</p>

<ol>
  <li>创建决策树的Root节点；</li>
  <li>若lable中取值单一，则返回 <code>label=label</code> 的单节点树；</li>
  <li>若attrs为空，则返回 <code>label=（data中取值最多的那个label）</code> 的单节点树；</li>
  <li>否则：
    <ol>
      <li>选取attrs中分类能力最好的属性作为Root的决策属性，记为A；</li>
      <li>对A的每一个可能取值vi：
        <ol>
          <li>在Root添加一个分支对应 <code>A = vi </code>；</li>
          <li>data_vi = data中 <code>A = vi</code> 的子集，label_vi 表示 data_vi 所对应的目标属性取值；</li>
          <li>若 data_vi 为空集：
            <ol>
              <li>在新分支下加一个叶子节点，节点 <code>label =（data中取值最多的那个label）</code> ;</li>
              <li>否则，加一个子树：ID3(data_vi, label_vi, attrs);</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>结束</li>
  <li>返回Root</li>
</ol>

<h3 id="r">四、R语言实现</h3>
<hr />

<p>见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习7: 决策树]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/11/decision-trees/"/>
    <updated>2014-04-11T19:00:32+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/11/decision-trees</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical21.jpg" alt="artical 21" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h3 id="section">一、开始</h3>
<hr />

<p>在介绍决策树的概念内容之前，先来初步了解一下决策树的流程。这是一个很简单的概念，通过一张简单的流程图就可以大致了解决策树是干什么的，怎么干的。</p>

<p><img src="\images\a21\decisiontrees.jpg" alt="decision trees" /></p>

<h3 id="section-1">二、相关概念</h3>
<hr />

<h4 id="section-2">1. 一些概念</h4>
<hr />

<ul>
  <li>决策树学习是一种逼近离散值目标函数的方法。</li>
  <li>决策树通过把实例从根节点排列到某个叶子节点来分类实例，叶子的节点即为实例所属的分类。</li>
  <li>决策树上的每一个节点，指定了对实例的某一个属性的测试，并且，该节点的每一个后续分支对应该属性的一个可能值。</li>
</ul>

<h4 id="section-3">2. 分类方法</h4>
<hr />
<p>从树的根节点开始，测试这个节点指定的属性，然后按照给定实例的该属性值对应的分支向下移动。然后以新节点作为根节点重复上面的过程直至结束。</p>

<h3 id="section-4">三、 评价</h3>
<hr />

<p>通过决策树的流程，可以发现决策树的计算复杂度不高，而且其输出的结果易于理解，并且对缺失值不敏感。</p>

<p>但是，正是由于其划分过于细致，可能会导致过度匹配问题(与回归中的overfitting类似)。</p>

<h3 id="section-5">四、主要的决策树算法</h3>
<hr />

<p>从决策树的流程可以看出，<strong>如何选择属性作为节点以测试实例</strong>是最为关键的一步。不同的算法采取了不同的方法，主要的决策树算法有这样几个：</p>

<ul>
  <li>ID3</li>
  <li>C4.5 （数据挖掘十大算法之一，也是ID3算法的改进）</li>
  <li>C5.0 （C4.5的改进，适用于处理大数据集，采用Boosting方式提高模型准确率，因而又称BoostingTrees。）</li>
  <li>CART（数据挖掘十大算法之一）</li>
</ul>

<p>下一篇就开始讲讲一些决策树的算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习6: 分类之朴素贝叶斯]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/08/naive-bayes/"/>
    <updated>2014-04-08T15:54:37+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/08/naive-bayes</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical20.jpg" alt="artical 20" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h3 id="knn">0、kNN算法的优缺点</h3>
<p>与kNN算法一样，朴素贝叶斯算法也是数据挖掘十大算法之一。我们介绍kNN算法时，并没有讨论kNN算法的优缺点，这边首先看看这个问题。</p>

<p>从构造kNN算法的过程可以看到，这个分类算法的<strong>精度很高</strong>。因为这个算法计算了所有点与待分类点之间的相似度，然后去确定带分类点的类别。由此可见，这个算法对<strong>异常值并不敏感</strong>。但是正是因为它需要计算所有点之间的距离，所以其<strong>复杂度是很高的</strong>，换句话就是，如果数据量大的话，这个算法会很费时，并不高效。</p>

<h3 id="section">一、贝叶斯决策</h3>
<hr />

<p>贝叶斯定理给出了条件概率之间的关系，是一个非常重要的定理。这里直接给出贝叶斯定理的结论：</p>

<script type="math/tex; mode=display"> P(B  \mid A) = \frac{P(A \mid B)P(B) }{P(A)} </script>

<p>在贝叶斯决策理论里面，要判断点$x$是否属于$C_i$类，只要验证是否存在：</p>

<script type="math/tex; mode=display"> p(C_i \mid x) = \max_{j} \{ P(C_j \mid x) \} </script>

<p>即是，使得$p(C_i \mid x)$达到最大的那个$C_i$就是$x$所属的类别。</p>

<h3 id="section-1">二、具体流程</h3>
<hr />

<p>要计算<script type="math/tex">P(C_i \mid x)</script>，那么就需要计算<script type="math/tex">\frac{P(x \mid C_i)P(C_i )}{P(x)}</script>，我们知道对于每一个<script type="math/tex">P(C_i \mid x)</script>，其计算公式中的分母都是<script type="math/tex">P(x)</script>，所以有：</p>

<script type="math/tex; mode=display"> P(C_i \mid x) \propto P(x \mid C_i)P(C_i ) </script>

<p>所以我们实际计算时，只需要考虑上式右侧的大小即可。首先我们来确定一些符号的意思：</p>

<ul>
  <li><script type="math/tex">x = [x_1, x_2, \dots, x_p]</script>为一个带分类的项，$x_i$为其特征；</li>
  <li>data表示一个已知分类的数据集($n \times p$的矩阵)，其每一行代表一个观测，每一列代表一个特征；</li>
  <li>label表示data中每一个数据对应的类别标签($ n \times 1$的矩阵)，比如data的第一行观测的类别就是label中的第一个取值；</li>
  <li><script type="math/tex">C = [ C_1, C_2, \dots, C_m ]</script>为一个类别集合，一般来说$ m &lt; p $。</li>
</ul>

<h5 id="section-2">步骤：</h5>

<ol>
  <li>
    <p>在已知分类的数据集data中统计：</p>

<script type="math/tex; mode=display">P(C_i), i = 1, 2, \dots, m</script>

<script type="math/tex; mode=display">P(x_j \mid C_i), i = 1, 2, \dots, m; j = 1, 2, \dots p</script>
  </li>
  <li>
    <p>计算</p>

<script type="math/tex; mode=display">P(x \mid C_i)P(C_i ) = P(C_i) \prod_{j=1}^{p} {P(x_j \mid C_i)}, i = 1, 2, \dots, m</script>
  </li>
  <li>
    <p>若</p>

<script type="math/tex; mode=display">P(x \mid C_k)P(C_k) = \max\{ P(x \mid C_i)P(C_i ) \}</script>

    <p>则<script type="math/tex"> x \in C_k</script>。</p>
  </li>
</ol>

<h3 id="section-3">三、一些存在的问题</h3>
<hr />

<ol>
  <li>当步骤的第2步中，<script type="math/tex">P(x \mid C_i)P(C_i ) = P(C_i) \prod_{j=1}^{p} {P(x_j \mid C_i)}, i = 1, 2, \dots, m</script>，中<script type="math/tex">P(x_j \mid C_i)</script>可能在样本较小时取值出现0，那么就会影响乘积；</li>
  <li>计算机计算时会出现精度问题，比如，如果<script type="math/tex">P(x_j \mid C_i)</script>的值有很多都是非常小的（像0.000001），那么计算机在计算是会将其四舍五入成0。</li>
</ol>

<p>出现上面的情况应该怎么办呢？</p>

<p>学数学的应该都清楚，遇到这种问题有一个很简单的处理方式，那就是取个对数。虽说取对数后会改变值的大小，但是取对数不会改变原本数据趋势，即原来大的数，取对数后还是大的。</p>

<p>此外，对于<script type="math/tex">P(x_j \mid C_i)</script>可能在样本较小时取值出现0的情况，处理也很简单，那就是将每一个$x_j$的初值都设置成1，所有特征的基数都从1开始，不会影响结果。（这时需要注意，所有特征的初值都是1，对应的总数初值也会发生变化。）</p>

<h3 id="r">四、R语言实现</h3>
<hr />

<p>见<a href="/datascience">我的项目</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习5：分类之k-近邻算法]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/05/knn/"/>
    <updated>2014-04-05T19:59:29+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/05/knn</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical19.jpg" alt="artical 19" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">基本思想</h4>
<hr />

<p>kNN，k-Nearest Neighbor algorithm，也就这边的k-近邻算法，是数据挖掘十大算法之一，是一个比较简单的分类方法。</p>

<p>其基本的思想是：对于一个输入样本（未知分类的样本），考虑其与测试样本中与之距离最近（特征最相似）的k个样本，用这k个样本中出现最多的分类作为输入样本的分类。</p>

<h4 id="section-1">具体流程</h4>
<hr />

<p>对于输入样本中的每一个点，进行以下操作：</p>

<ol>
  <li>计算点与测试样本中点的距离；</li>
  <li>取出与当前点距离最小的k个点；</li>
  <li>确定k个点的分类，计算各个分类的频数；</li>
  <li>返回频数最高的类别，作为该输入点的预测分类。</li>
</ol>

<h4 id="section-2">距离的计算</h4>
<hr />

<p>上面一直在说，计算输入样本中点与测试样本中点之间的距离，那么这个距离应该怎么计算呢？这个距离一般就是使用欧式距离：</p>

<script type="math/tex; mode=display"> d = \sqrt{(x - y)^T(x - y)} </script>

<p>其中<script type="math/tex">x^T=[x_1, x_2,\dots,x_n], y^T=[y_1,y_2,\dots,y_n]</script>。二维的表示就是：</p>

<script type="math/tex; mode=display"> d = \sqrt{(x - y)^T(x - y)} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} </script>

<h4 id="r">R语言实现</h4>
<hr />

<p>见<a href="https://github.com/JackyCode/Data_Science/tree/master/kNN">我的github</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习4：线性回归3]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/02/linear-regression3/"/>
    <updated>2014-04-02T18:51:29+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/02/linear-regression3</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical18.jpg" alt="artical 18" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>这是介绍线性回归的最后一篇，首先回顾一下之前的两篇。第一篇主要就是介绍了如何去估计回归系数得到回归方程，以及在R语言中如何使用自带的函数去实现。第二篇主要介绍了对于回归方程和回归系数的显著性检验，以及给出了我自己写的一个处理线性回归的函数。</p>

<p>这一篇介绍线性回归中回归诊断的一些问题，也就是估计出回归方程，检验了回归方程的显著性以及回归系数的显著性后，对这个模型所做的进一步的诊断分析。然后对存在的问题进行探讨，选择不同的方式去解决这些问题。这大致可以分成三块：残差分析，影响分析以及共线性问题。</p>

<hr />

<h4 id="section">一、残差分析</h4>

<hr />

<h5 id="section-1">1. 残差</h5>

<hr />

<p>首先看一看残差的定义，常用的残差大致有三种：残差，标准化残差以及学生化残差：</p>

<ul>
  <li>残差：<script type="math/tex">\hat{\varepsilon} = y - \hat{y} = (1-H)y</script>,其中<script type="math/tex">H=X(X^TX)^{-1}X</script>称作帽子矩阵；</li>
  <li>标准化残差：<script type="math/tex">ZRE = \hat{\varepsilon} / \hat{\sigma}</script></li>
  <li>学生化残差：<script type="math/tex">SRE_i = \hat{\varepsilon}_i / (\hat{\sigma}\sqrt{1-h_{ii}})</script>，其中<script type="math/tex">h_{ii}</script>为帽子矩阵对角线上第$i$个元素。</li>
</ul>

<hr />

<p><strong>R语言中</strong></p>

<p>使用<code>residuals(),rstandard(),rstudent()</code>函数计算残差，标准化残差以及学生化残差。具体用法，请使用<code>help</code>函数查看。</p>

<hr />

<h5 id="section-2">2. 残差图</h5>

<hr />

<p>以残差为纵坐标，观测值、预测值活则观测时间等等作为横坐标的散点图，称为<strong>残差图</strong>。</p>

<hr />

<p><strong>R语言中</strong></p>

<p><code>r
model = lm(y~x1+x2)
y_pred = predict(model)
y_res = residuals(model)
plot(y_res ~ y_pred)
</code>
***</p>

<h5 id="section-3">3. 异方差问题</h5>

<hr />

<h6 id="a-">a. 问题的提出</h6>

<hr />

<p>在进行回归方程估计之前，一般都会假设误差的方差是齐性的。如果残差图出现类似下面的情况，则这批数据可能存在异方差问题，即方差非齐性。</p>

<ol>
  <li>随着横坐标的增大，纵坐标的值波动越来越大；</li>
  <li>随着横坐标的增大，纵坐标的值波动越来越小；</li>
  <li>随着横坐标的增大，纵坐标的值波动复杂多变，没有系统关系；</li>
</ol>

<p>大部分时候，考虑前两种就可以了。那么具体如何数值化地检验异方差呢，一般使用的是等级相关系数法，这里不做介绍(可到线性回归的书籍中寻找)。</p>

<hr />

<h6 id="b-">b. 问题的解决</h6>

<hr />

<p>一般有两种方式解决异方差问题，一种是加权最小二乘；另一种是对因变量作适当的变化。</p>

<ol>
  <li>
    <p>加权最小二乘</p>

    <p>即是将回归系数的估计转化成：<script type="math/tex">\hat{\beta}=(X^TWX)^{-1}X^TWy</script>，其中<script type="math/tex">W</script>是一个对角矩阵，用于给每一个数据点加上一个权重。一般使用“核”来对附近的点赋予较高的权重，常用的核就是高斯核，其对应的权重为：</p>

<script type="math/tex; mode=display">w_{ii}=exp(\frac{\|x^{(i)}-x\|}{-2k^2})</script>

    <p>从上式可以看出，点$x$离$x^{(i)}$越近，所得到的权重越高。</p>
  </li>
  <li>
    <p>对因变量作适当变化</p>

    <p>常用的变换有：</p>

    <ul>
      <li>$z = \sqrt{y}$</li>
      <li>$z = ln(y)$（对数变换）</li>
      <li>$z = 1/y$</li>
      <li>$z = \frac{x^{\lambda}-1}{\lambda}$(Box-Cox变换)，其中$\lambda=0$时，即是对数变换</li>
    </ul>
  </li>
</ol>

<hr />

<h5 id="section-4">4. 异常点</h5>

<hr />

<h6 id="a--1">a. 问题的提出</h6>

<hr />

<p>一般将标准化残差的绝对值大于等于2的称为可疑点；将标准化残差的绝对值大于等于3的称为异常点。</p>

<hr />

<h6 id="b--1">b. 问题的解决</h6>

<hr />

<p>一般来说，剔除异常数据即可。</p>

<hr />

<h5 id="section-5">5. 自相关问题</h5>

<hr />

<p>在作回归之前，总是会假设<script type="math/tex">cov(\varepsilon_i, \varepsilon_j)=0,\forall i \neq j</script>。但是实际情况下，可能并不满足这个假设，这就是存在了自相关问题。对于自相关问题，一般使用残差图，自相关系数以及DW检验去进行检验；而处理的方式一般是：迭代法和差分法。这里不做详细介绍，感兴趣的可以去找找相关材料。</p>

<hr />

<h4 id="section-6">二、影响分析</h4>

<hr />

<p>分析观测值对回归结果的影响，从而找出对回归结果影响较大的观测点的分析方法叫做影响分析。一般使用Cook距离去度量第$i$个观测值对回归影响大小，Cook距离的定义如下：</p>

<script type="math/tex; mode=display">D_i(M,MSE) = \frac{(\hat{\beta}_{(i)}-\hat{\beta})^TM(\hat{\beta}_{(i)}-\hat{\beta})}{MSE}</script>

<p>其中，$M$为观测数据的离差阵，$MSE$为回归模型的均方误差。一般<script type="math/tex">\|D_i\| \geqslant 4/n</script>时，称其为强影响点。</p>

<hr />

<p><strong>R语言中</strong></p>

<p>使用<code>cooks.distance()</code>函数计算Cook距离。</p>

<hr />

<h4 id="section-7">三、共线性诊断</h4>

<hr />

<p>共线性是指，在多元线性回归中，自变量之间存在线性关系或者近似线性关系。如果出现这种情况，那么在模型内部就会隐藏部分变量的显著性，也会导致参数估计的误差增大，影响模型的稳定性。</p>

<hr />

<h5 id="a--2">a. 检验方法</h5>

<hr />

<p>常用的检验方法有特征值法，条件数和方差膨胀因子（VIF）。</p>

<hr />

<h6 id="section-8">特征值法</h6>

<hr />

<p>首先介绍一个结论：当矩阵$X^TX$至少有一个特征根为0时，$X$的列向量间必存在多重共线性。</p>

<p>即可证：$X^TX$有多少个特征根接近于零，设计阵$X$就有多少个多重共线性。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>eigen()</code>函数去计算特征值和特征向量。</p>

<hr />

<h6 id="section-9">条件数</h6>

<hr />

<p>上述的特征值法中，特征根近似为0，这个标准好想并不明确。那么这边就给出一个条件数的定义：</p>

<script type="math/tex; mode=display">k_i = \frac{\lambda_m}{\lambda_i}</script>

<p>其中，<script type="math/tex">\lambda_m</script>为最大的那个特征根。一般认为，若$k_i$介于10到30之间为弱相关；在30到100之间为中等相关；超过100为强相关。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>kappa()</code>函数计算条件数。</p>

<hr />

<h6 id="vif">VIF</h6>

<hr />

<p>定义VIF为：</p>

<script type="math/tex; mode=display">VIF_i = \frac{\text{第i个回归系数的方差}}{\text{自变量不相关时第i个回归系数的方差}} = \frac{1}{1-R^2_i} = \frac{1}{TOL_i}</script>

<p>其中$TOL_i$称为容忍度；$R^2_i$为自变量$x_i$对其余自变量的复决定系数。一般认为，VIF超过10，模型就存在共线性问题。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>vif()</code>函数计算VIF的值。</p>

<hr />

<h5 id="b--2">b. 多重共线性的处理</h5>

<hr />

<p>一般有这样几种处理方式：</p>

<ol>
  <li>
    <p>剔除一些不重要的解释变量</p>

    <ol>
      <li>使用变量选择的方式剔除部分变量，作回归；</li>
      <li>检验VIF，若存在共线性，删除VIF值最大的变量，作回归；</li>
      <li>再次检验VIF，若还存在共线性，再删除其中VIF值最大的那个；</li>
      <li>重复直至消除共线性。</li>
    </ol>
  </li>
  <li>
    <p>增大样本容量</p>

    <p>当变量的个数接近样本容量的数值时，自变量间容易产生多重共线性。所以增大样本容量是解决多重共线性的一种方式，但是在现实中，这种做法基本不可能。</p>
  </li>
  <li>
    <p>主成分回归</p>

    <p>这是一个比较大的主题，这里不做介绍。</p>
  </li>
  <li>
    <p>有偏估计等等。</p>
  </li>
</ol>

<hr />

<h3 id="section-10">最后</h3>

<hr />

<p>到这里，除了变量选择问题，线性回归的内容基本上就已经梳理了一遍。变量选择问题，方法简单的非常简单，难的非常难（像lasso），所以暂时还不想写这些内容。</p>
]]></content>
  </entry>
  
</feed>
