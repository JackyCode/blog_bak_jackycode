<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DataScience | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/datascience/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-08-18T17:10:00+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学20：文本挖掘2]]></title>
    <link href="http://jackycode.github.io/blog/2014/06/26/text-mining2/"/>
    <updated>2014-06-26T15:02:09+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/06/26/text-mining2</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article42.jpg" alt="article 42" />
<!-- more --></p>

<p>图片由本文中数据生产~</p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h2 id="section">一、对词条-文档矩阵的操作</h2>

<hr />

<p>在’tm’包中，提供了一些常用的函数，可以对得到的Document Term
Matrix进行一些操作。当然，我们也可以使用自己的方式，对该矩阵进行一些探索，比如，我们先来看看词条的频数：</p>

<h3 id="section-1">1.1 词条频数</h3>

<hr />

<p>``` r
freq &lt;- colSums(as.matrix(dtm))
length(freq)</p>

<h2 id="section-2">[1] 780</h2>
<p>```</p>

<p>如果我们想看看，哪些频数是大于等于30的，我们可以：</p>

<p>``` r
names(freq[freq &gt;= 30])</p>

<h2 id="market-mln----oil----opec---price--said">[1] “market” “mln”    “oil”    “opec”   “price”  “said”</h2>
<p>```</p>

<p>但其实，在’tm’包中有自带的函数可以解决：</p>

<p>``` r
findFreqTerms(dtm, lowfreq = 30)</p>

<h2 id="market-mln----oil----opec---price--said-1">[1] “market” “mln”    “oil”    “opec”   “price”  “said”</h2>
<p>```</p>

<p>如果得到了每个词条的频数，我们可以列一个表查看一些其它的东西：</p>

<p>``` r
ord &lt;- order(freq, decreasing = TRUE)
## 查看频数排在前十的词条及其频数
head(freq[ord], 10)</p>

<h2 id="oil---said--price---opec----mln-market-barrel---last----bpd---dlrs">oil   said  price   opec    mln market barrel   last    bpd   dlrs</h2>
<p>##     86     73     63     47     31     30     26     24     23     23</p>

<h2 id="section-3">查看频数的频数</h2>
<p>tail(table(freq), 10)</p>

<h2 id="freq">freq</h2>
<p>## 21 23 24 26 30 31 47 63 73 86
##  2  2  1  1  1  1  1  1  1  1
```</p>

<h3 id="section-4">1.2 相关性</h3>

<hr />

<p>除了词条的频数，我们大都还会对词条之间的相关性感兴趣。如何计算，其实很简单：</p>

<p>``` r
corr &lt;- cor(as.matrix(dtm))
corr[1:5, 1:5]</p>

<h2 id="abdul-----abil------abl---abroad---accept">abdul     abil      abl   abroad   accept</h2>
<p>## abdul   1.00000 -0.08812 -0.05263 -0.05263 -0.05263
## abil   -0.08812  1.00000 -0.08812  0.79309 -0.08812
## abl    -0.05263 -0.08812  1.00000 -0.05263 -0.05263
## abroad -0.05263  0.79309 -0.05263  1.00000 -0.05263
## accept -0.05263 -0.08812 -0.05263 -0.05263  1.00000</p>

<p>dim(corr)</p>

<h2 id="section-5">[1] 780 780</h2>
<p>```</p>

<p>比如我们想知道，与词条’oil’相关性超过0.8的词条有哪些：</p>

<p>``` r
op &lt;- options()
options(digits = 2)
name &lt;- setdiff(names(corr[, “oil”][corr[, “oil”] &gt; 0.8]), “oil”)
value &lt;- corr[name, “oil”]
matrix &lt;- matrix(value[order(value, decreasing = TRUE)])
rownames(matrix) = names(value)
colnames(matrix) = “oil”
matrix</p>

<h2 id="oil">oil</h2>
<p>## name 0.87
## opec 0.81
## tri  0.81
## want 0.81</p>

<p>options(op)
```</p>

<p>但其实，’tm’包中提供了现成函数：</p>

<p>``` r
findAssocs(dtm, “oil”, 0.8)</p>

<h2 id="oil-1">oil</h2>
<p>## opec 0.87
## name 0.81
## tri  0.81
## want 0.81
```</p>

<p>可以看到，这一个函数实现了上面我们那一长串的功能。<strong>但是，我觉得，提供的现成函数有好处，但还是需要自己想想如何实现这个现成的函数。这不仅对概念理解有帮助，也对程序编写以及了解别人写的函数有更加深刻的理解。</strong></p>

<h3 id="section-6">1.3 删减稀疏条目</h3>

<hr />

<p>大部分时候，Document Term
Matrix是一个很大的矩阵，而且是行数远小于列数。也就是说，这个矩阵实际上是很稀疏的，不妨来看看我们这个只有20个文档的矩阵，稀疏程度如何：</p>

<p>``` r
Sparse &lt;- length(which(matrix(dtm) == 0))
Sparse</p>

<h2 id="section-7">[1] 14030</h2>

<p>Non &lt;- length(which(matrix(dtm) != 0))
Non</p>

<h2 id="section-8">[1] 1570</h2>

<p>sprintf(“%d%%”, round(Sparse/(Non + Sparse), 2) * 100)</p>

<h2 id="section-9">[1] “90%”</h2>
<p>```</p>

<p>可以看到，矩阵中等于零的数目是14030，不为零的为1570，稀疏程度达到了90%，显然是非常稀疏的。当然，其实这个可以不用自己算的：</p>

<p>``` r
dtm</p>

<h2 id="documenttermmatrix-documents-20-terms-780">«DocumentTermMatrix (documents: 20, terms: 780)»</h2>
<p>## Non-/sparse entries: 1570/14030
## Sparsity           : 90%
## Maximal term length: 13
## Weighting          : term frequency (tf)
```</p>

<p>上面的输出，前两个就知道它的意思了。那么另外两个呢？“Maximal term
length”，顾名思义，就是最长的那个词条的长度。不妨来看一下：</p>

<p>``` r
name &lt;- colnames(as.matrix(dtm))
name.length &lt;- sapply(name, nchar)
max.name &lt;- name[name.length == max(name.length)]
max.value &lt;- name.length[max.name]
max.value</p>

<h2 id="responsibilit">responsibilit</h2>
<p>##            13
```</p>

<p>可以看到是吻合的。至于最后一个“Weighting”，是指Document Term
Matrix中的每一条目的值是如何计算的。这里用的是“term
frequency”，就是词条频率，简写为“tf”。除了这个，还有一些其它计算方法，比如：tf-idf，即term
frequency-inverse document frequency。这个以后再说。</p>

<hr />

<p><strong>回到主题：</strong>计算出了稀疏条目的比例，证实了矩阵的确是很稀疏的，那么下面就是要去删除一些条目。有些条目在很少的文档中出现，甚至只在一篇文档中出现了。一般来说，删除这样的条目不会对矩阵的信息继承带来显著的影响。</p>

<p>``` r
remove.sparseterms &lt;- function(dtm, per) {
    dtm.Matrix &lt;- as.matrix(dtm)
    term.per &lt;- apply(dtm.Matrix, 2, function(item) length(which(item == 0))/length(item))
    name &lt;- names(term.per[which(term.per &lt; per)])
    return(dtm.Matrix[, name])
}
remove.sparseterms(dtm, 0.4)</p>

<h2 id="terms">Terms</h2>
<p>## Docs  barrel oil one price reuter said
##   127      2   5   0     5      1    3
##   144      0  12   1     6      2   11
##   191      1   2   0     2      1    1
##   194      1   1   1     2      1    1
##   211      0   1   0     0      1    3
##   236      4   7   1     8      1   10
##   237      0   4   1     1      1    1
##   242      0   3   2     2      1    3
##   246      1   5   1     2      1    5
##   248      3   9   1    10      1    7
##   273      3   5   1     5      1    8
##   349      0   4   0     1      1    1
##   352      1   5   0     5      1    2
##   353      1   4   1     2      1    1
##   368      0   3   0     0      1    3
##   489      3   4   2     3      1    2
##   502      3   5   2     3      1    2
##   543      1   3   1     3      1    4
##   704      0   3   4     3      1    4
##   708      2   1   0     0      1    1
```</p>

<p>这边的计算就是：检查词条的稀疏程度是否低于给定的系数<code>per</code>，如果是，就保留该词条；如果不是，则舍弃。</p>

<p>当然，在’tm’包中也有自带的函数可以解决：</p>

<p>``` r
rsterms &lt;- removeSparseTerms(dtm, 0.4)
inspect(rsterms)</p>

<h2 id="documenttermmatrix-documents-20-terms-6">«DocumentTermMatrix (documents: 20, terms: 6)»</h2>
<p>## Non-/sparse entries: 103/17
## Sparsity           : 14%
## Maximal term length: 6
## Weighting          : term frequency (tf)
##
##      Terms
## Docs  barrel oil one price reuter said
##   127      2   5   0     5      1    3
##   144      0  12   1     6      2   11
##   191      1   2   0     2      1    1
##   194      1   1   1     2      1    1
##   211      0   1   0     0      1    3
##   236      4   7   1     8      1   10
##   237      0   4   1     1      1    1
##   242      0   3   2     2      1    3
##   246      1   5   1     2      1    5
##   248      3   9   1    10      1    7
##   273      3   5   1     5      1    8
##   349      0   4   0     1      1    1
##   352      1   5   0     5      1    2
##   353      1   4   1     2      1    1
##   368      0   3   0     0      1    3
##   489      3   4   2     3      1    2
##   502      3   5   2     3      1    2
##   543      1   3   1     3      1    4
##   704      0   3   4     3      1    4
##   708      2   1   0     0      1    1
```</p>

<p>与之前的结果相同。</p>

<h2 id="section-10">二、相关的绘图</h2>

<hr />

<p>介绍完Document Term
Matrix之后，其实已经可以开始使用模型去处理了。之前介绍过聚类、分类啊等等的内容，有了这个矩阵之后，其实都可以去做了。不过，在建立模型之前，我想先说说有关绘图的问题。</p>

<p>我们前面算了很多的量，比如频数啊、相关系数啊等等。但其实，我们大多时候需要用图形将它们展示出来。在R中，我们可以使用<code>Rgraphviz</code>包来绘制词条之间的关系图：</p>

<p><code>r
require(Rgraphviz)
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 20))
</code></p>

<p><img src="/images/a42/TM2.2.11.png" alt="plot of chunk
TM2.2.1" /></p>

<p><code>r
## 指定相关性必须在0.5之上才可连线
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 20), corThreshold = 0.5)
</code></p>

<p><img src="/images/a42/TM2.2.12.png" alt="plot of chunk
TM2.2.1" /></p>

<p>在默认不指定<code>terms</code>和<code>corThreshold</code>参数的情况下，<code>plot</code>函数会绘制随机20个词条，相关性至少0.7的图形。</p>

<hr />

<p>当然，我们还可以绘制熟悉的频数直方图：</p>

<p>``` r
freq &lt;- sort(colSums(as.matrix(dtm)), decreasing = TRUE)
tf &lt;- data.frame(term = names(freq), freq = freq)</p>

<p>g &lt;- ggplot(subset(tf, freq &gt; 20), aes(term, freq))
g &lt;- g + geom_bar(stat = “identity”, aes(fill = freq))
print(g)
```</p>

<p><img src="/images/a42/TM2.2.2.png" alt="plot of chunk TM2.2.2" /></p>

<hr />

<p>另外一个比较流行好看的图，应该就是文本云(wordcloud)了，可以使用<code>wordcloud</code>包实现：</p>

<p><code>r
require(wordcloud)
wordcloud(names(freq), freq, min.freq = 5, max.words = 80)
</code></p>

<p><img src="/images/a42/TM2.2.31.png" alt="plot of chunk
TM2.2.3" /></p>

<p><code>r
## 添加颜色
require(RColorBrewer)
wordcloud(names(freq), freq, min.freq = 5, max.words = 80, colors = brewer.pal(8,
    "Paired"))
</code></p>

<p><img src="/images/a42/TM2.2.32.png" alt="plot of chunk
TM2.2.3" /></p>

<p>当然，<code>wordcloud()</code>函数提供了很多选项，可以自行查看，我就不多讲了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学19：文本挖掘1-更新]]></title>
    <link href="http://jackycode.github.io/blog/2014/06/25/text-mining1-update/"/>
    <updated>2014-06-25T19:21:26+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/06/25/text-mining1-update</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article41.jpg" alt="article 41" />
<!-- more --></p>

<p>图片由本文中数据生产~</p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>前几天，R中的’tm’包从0.5-10更新到了0.6版本。其中更新了不少的东西，对于上一篇中的代码，已经是不能够正确运行了。所以这里需要先更新一下上一篇中的一些代码，正好可以回顾一些之前的流程。</p>

<p>``` r
## Load Packages needed
require(tm)
require(SnowballC)
require(ggplot2)</p>

<h2 id="loading-corpus-">Loading Corpus 这里直接读成文本格式</h2>
<p>xml.path &lt;- system.file(“texts”, “crude”, package = “tm”)
docs &lt;- Corpus(DirSource(xml.path), readerControl = list(reader = readReut21578XMLasPlain))</p>

<h2 id="section">查看</h2>
<p>docs[[1]]
```</p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## Diamond Shamrock Corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     The reduction brings its posted price for West Texas
## Intermediate to 16.00 dlrs a barrel, the copany said.
##     "The price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     Diamond is the latest in a line of U.S. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  Reuter
</code></p>

<p><code>r
## 去除特殊字符 上一篇中的代码已不能胜任
## 因为上一篇中的代码会修改Corpus中每个文档的class
## 所以这里需要使用一个新的函数'content_transformer'来保证class还是TextDoument
removeSpecialCharacter &lt;- function(x, pattern) {
    gsub(pattern, " ", x)
}
docs &lt;- tm_map(docs, content_transformer(removeSpecialCharacter), "[@/-]")
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## Diamond Shamrock Corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     The reduction brings its posted price for West Texas
## Intermediate to 16.00 dlrs a barrel, the copany said.
##     "The price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     Diamond is the latest in a line of U.S. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  Reuter
</code></p>

<p><code>r
## 转换成小写字母 与上面问题类似，需要添加'content_transformer'函数
docs &lt;- tm_map(docs, content_transformer(tolower))
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     the reduction brings its posted price for west texas
## intermediate to 16.00 dlrs a barrel, the copany said.
##     "the price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     diamond is the latest in a line of u.s. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  reuter
</code></p>

<p><code>r
## 去除数字
docs &lt;- tm_map(docs, removeNumbers)
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said that
## effective today it had cut its contract prices for crude oil by
## . dlrs a barrel.
##     the reduction brings its posted price for west texas
## intermediate to . dlrs a barrel, the copany said.
##     "the price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     diamond is the latest in a line of u.s. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  reuter
</code></p>

<p><code>r
## 去除停止词
docs &lt;- tm_map(docs, removeWords, stopwords("english"))
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said
## effective today   cut  contract prices  crude oil
## . dlrs  barrel.
##      reduction brings  posted price  west texas
## intermediate  . dlrs  barrel,  copany said.
##     " price reduction today  made   light  falling
## oil product prices   weak crude oil market,"  company
## spokeswoman said.
##     diamond   latest   line  u.s. oil companies
##  cut  contract,  posted, prices   last two days
## citing weak oil markets.
##  reuter
</code></p>

<p><code>r
## 去除标点符号
docs &lt;- tm_map(docs, removePunctuation)
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said
## effective today   cut  contract prices  crude oil
##  dlrs  barrel
##      reduction brings  posted price  west texas
## intermediate   dlrs  barrel  copany said
##      price reduction today  made   light  falling
## oil product prices   weak crude oil market  company
## spokeswoman said
##     diamond   latest   line  us oil companies
##  cut  contract  posted prices   last two days
## citing weak oil markets
##  reuter
</code></p>

<p><code>r
## 去除多余的空格
docs &lt;- tm_map(docs, stripWhitespace)
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said effective today cut contract prices crude oil dlrs barrel reduction brings posted price west texas intermediate dlrs barrel copany said price reduction today made light falling oil product prices weak crude oil market company spokeswoman said diamond latest line us oil companies cut contract posted prices last two days citing weak oil markets reuter
</code></p>

<p><code>r
## 词干化
docs &lt;- tm_map(docs, stemDocument)
docs[[1]]
</code></p>

<p><code>
## &lt;&lt;PlainTextDocument (metadata: 15)&gt;&gt;
## diamond shamrock corp said effect today cut contract price crude oil dlrs barrel reduct bring post price west texa intermedi dlrs barrel copani said price reduct today made light fall oil product price weak crude oil market compani spokeswoman said diamond latest line us oil compani cut contract post price last two day cite weak oil market reuter
</code></p>

<p><code>r
## 创建Document Term Matrix
dtm &lt;- DocumentTermMatrix(docs)
dtm
</code></p>

<p><code>
## &lt;&lt;DocumentTermMatrix (documents: 20, terms: 780)&gt;&gt;
## Non-/sparse entries: 1570/14030
## Sparsity           : 90%
## Maximal term length: 13
## Weighting          : term frequency (tf)
</code></p>

<p><code>r
inspect(dtm[1:5, 1:10])
</code></p>

<p><code>
## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 10)&gt;&gt;
## Non-/sparse entries: 2/48
## Sparsity           : 96%
## Maximal term length: 7
## Weighting          : term frequency (tf)
##
##      Terms
## Docs  abdul abil abl abroad accept accord across activ add address
##   127     0    0   0      0      0      0      0     0   0       0
##   144     0    2   0      0      0      0      0     0   0       4
##   191     0    0   0      0      0      0      0     0   0       0
##   194     0    0   0      0      0      0      0     0   0       0
##   211     0    0   0      0      0      0      0     0   0       0
</code></p>

<p>到这里，就已经完成了上一篇所做的事情了。留意一下，也可以发现，’tm’包更新过之后，处理的结果与之前有一些不同，可以自己动手试试看，看看哪里不一样？</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学18：文本挖掘1]]></title>
    <link href="http://jackycode.github.io/blog/2014/06/18/text-mining1/"/>
    <updated>2014-06-18T15:08:39+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/06/18/text-mining1</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article40.jpg" alt="article 40" />
<!-- more --></p>

<p>图片由本文中数据生产~</p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p><strong>文本挖掘</strong>，也称为文本数据挖掘，意思就如字面，对文本数据进行挖掘分析。文本挖掘一般包含：<em>文本分类</em>、<em>文本聚类</em>、<em>概念实体挖掘</em>、<em>自然语言处理</em>等等。接下来，我打算用一个简单的例子，介绍一下R语言文本挖掘的一般过程，顺便介绍一些文本挖掘中的概念。</p>

<p>这边主要使用R中的<code>tm</code>包进行文本挖掘，首先加载Package：</p>

<p><code>r
require(tm)  ## R中处理文本挖掘的框架包
require(ggplot2)
</code></p>

<h3 id="corpus">1. 载入Corpus</h3>
<hr />

<p><strong>Corpus</strong>（语料库），指一系列文档的集合，是<code>tm</code>包管理文件的数据结构。通常，我们需要将一批文档导入成Corpus结构的数据，然后才能进行进一步的处理分析。</p>

<p>文档的格式有非常多的格式，<code>tm</code>包支持的格式其实只占很少的一部分，大致有：text, PDF, Mircosoft Word和XML。所以，如果需要处理的文档，其格式不在这里面的话，就需要对格式进行一些转换。个人建议，将文档格式转换成text或者XML会比较容易处理。我们可以查看一下，<code>tm</code>包支持的文档格式：</p>

<p><code>r
getReaders()
</code></p>

<p><code>
## [1] "readDOC"                 "readPDF"
## [3] "readReut21578XML"        "readReut21578XMLasPlain"
## [5] "readPlain"               "readRCV1"
## [7] "readRCV1asPlain"         "readTabular"
## [9] "readXML"
</code></p>

<p>在<code>tm</code>包中，Corpus可以分为两种。一种是Volatile Corpus，这种数据结构是作为R对象保存在内存中,使用<code>VCorpus()</code>或者<code>Corpus()</code>函数；另一种就是Permanent Corpus，作为R的外部保存，使用<code>PCorpus()</code>函数。显然，如何选择取决于内存大小以及运算速率的要求了。</p>

<p>我们这里使用<code>tm</code>包自带的XML文档数据进行演示：</p>

<p><code>r
xml &lt;- system.file("texts", "crude", package = "tm")  ## 数据所在的目录
docs &lt;- Corpus(DirSource(xml), readerControl = list(reader = readReut21578XML))
</code></p>

<p>这里使用的数据源是<code>DirSource</code>，当然也可以从其他的数据源导入，可以使用<code>getSources()</code>查看：</p>

<p><code>r
getSources()
</code></p>

<p><code>
## [1] "DataframeSource" "DirSource"       "ReutersSource"   "URISource"
## [5] "VectorSource"
</code></p>

<p>如果读取的是其他格式的，就需要指定一些其他的参数，用<code>path</code>表示数据所在的目录：</p>

<p><code>r
## txt
docs &lt;- Corpus(DirSource(&lt;path&gt;))
## PDF
docs &lt;- Corpus(DirSource(&lt;path&gt;), readerControl = list(reader = readPDF))
## 其它的类似
</code></p>

<h3 id="corpus-1">2. 查看Corpus</h3>
<hr />

<p>将数据导入成Corpus之后，我们就需要查看Corpus。</p>

<p><code>r
docs  ## 只显示了Corpus中含有的文档数据数量
</code></p>

<p><code>
## A corpus with 20 text documents
</code></p>

<p><code>r
names(docs)[1:3]  ## 显示前3个文档的名称
</code></p>

<p><code>
## [1] "reut-00001.xml" "reut-00002.xml" "reut-00004.xml"
</code></p>

<p><code>r
summary(docs)  ## 显示更多的meta data，但不显示源信息
</code></p>

<p><code>
## A corpus with 20 text documents
##
## The metadata consists of 2 tag-value pairs and a data frame
## Available tags are:
##   create_date creator
## Available variables in the data frame are:
##   MetaID
</code></p>

<p><code>r
inspect(docs[1])  ## 提取第一篇文档的完整信息、
</code></p>

<p><code>
## A corpus with 1 text document
##
## The metadata consists of 2 tag-value pairs and a data frame
## Available tags are:
##   create_date creator
## Available variables in the data frame are:
##   MetaID
##
## $`reut-00001.xml`
## $doc
## $file
## [1] "&lt;buffer&gt;"
##
## $version
## [1] "1.0"
##
## $children
## $children$REUTERS
## &lt;REUTERS TOPICS="YES" LEWISSPLIT="TRAIN" CGISPLIT="TRAINING-SET" OLDID="5670" NEWID="127"&gt;
##  &lt;DATE&gt;26-FEB-1987 17:00:56.04&lt;/DATE&gt;
##  &lt;TOPICS&gt;
##   &lt;D&gt;crude&lt;/D&gt;
##  &lt;/TOPICS&gt;
##  &lt;PLACES&gt;
##   &lt;D&gt;usa&lt;/D&gt;
##  &lt;/PLACES&gt;
##  &lt;PEOPLE/&gt;
##  &lt;ORGS/&gt;
##  &lt;EXCHANGES/&gt;
##  &lt;COMPANIES/&gt;
##  &lt;UNKNOWN&gt;Y
##    f0119 reute
## u f BC-DIAMOND-SHAMROCK-(DIA   02-26 0097&lt;/UNKNOWN&gt;
##  &lt;TEXT&gt;
##   &lt;TITLE&gt;DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES&lt;/TITLE&gt;
##   &lt;DATELINE&gt;NEW YORK, FEB 26 -&lt;/DATELINE&gt;
##   &lt;BODY&gt;Diamond Shamrock Corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     The reduction brings its posted price for West Texas
## Intermediate to 16.00 dlrs a barrel, the copany said.
##     &amp;quot;The price reduction today was made in the light of falling
## oil product prices and a weak crude oil market,&amp;quot; a company
## spokeswoman said.
##     Diamond is the latest in a line of U.S. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  Reuter&lt;/BODY&gt;
##  &lt;/TEXT&gt;
## &lt;/REUTERS&gt;
##
##
## attr(,"class")
## [1] "XMLDocumentContent"
##
## $dtd
## $external
## NULL
##
## $internal
## NULL
##
## attr(,"class")
## [1] "DTDList"
##
## attr(,"Author")
## character(0)
## attr(,"DateTimeStamp")
## [1] "1987-02-26 17:00:56 GMT"
## attr(,"Description")
## [1] ""
## attr(,"Heading")
## [1] "DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES"
## attr(,"ID")
## [1] "127"
## attr(,"Language")
## [1] "en"
## attr(,"LocalMetaData")
## attr(,"LocalMetaData")$TOPICS
## [1] "YES"
##
## attr(,"LocalMetaData")$LEWISSPLIT
## [1] "TRAIN"
##
## attr(,"LocalMetaData")$CGISPLIT
## [1] "TRAINING-SET"
##
## attr(,"LocalMetaData")$OLDID
## [1] "5670"
##
## attr(,"LocalMetaData")$Topics
## [1] "crude"
##
## attr(,"LocalMetaData")$Places
## [1] "usa"
##
## attr(,"LocalMetaData")$People
## character(0)
##
## attr(,"LocalMetaData")$Orgs
## character(0)
##
## attr(,"LocalMetaData")$Exchanges
## character(0)
##
## attr(,"Origin")
## [1] "Reuters-21578 XML"
## attr(,"class")
## [1] "Reuters21578Document" "TextDocument"         "XMLDocument"
## [4] "XMLAbstractDocument"  "oldClass"
</code></p>

<p><code>r
## inspect(docs) 可以提取所有文档的完整信息，不过数据量会很大
docs[[1]]  ## 提取第一个文档
</code></p>

<p><code>
## $doc
## $file
## [1] "&lt;buffer&gt;"
##
## $version
## [1] "1.0"
##
## $children
## $children$REUTERS
## &lt;REUTERS TOPICS="YES" LEWISSPLIT="TRAIN" CGISPLIT="TRAINING-SET" OLDID="5670" NEWID="127"&gt;
##  &lt;DATE&gt;26-FEB-1987 17:00:56.04&lt;/DATE&gt;
##  &lt;TOPICS&gt;
##   &lt;D&gt;crude&lt;/D&gt;
##  &lt;/TOPICS&gt;
##  &lt;PLACES&gt;
##   &lt;D&gt;usa&lt;/D&gt;
##  &lt;/PLACES&gt;
##  &lt;PEOPLE/&gt;
##  &lt;ORGS/&gt;
##  &lt;EXCHANGES/&gt;
##  &lt;COMPANIES/&gt;
##  &lt;UNKNOWN&gt;Y
##    f0119 reute
## u f BC-DIAMOND-SHAMROCK-(DIA   02-26 0097&lt;/UNKNOWN&gt;
##  &lt;TEXT&gt;
##   &lt;TITLE&gt;DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES&lt;/TITLE&gt;
##   &lt;DATELINE&gt;NEW YORK, FEB 26 -&lt;/DATELINE&gt;
##   &lt;BODY&gt;Diamond Shamrock Corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     The reduction brings its posted price for West Texas
## Intermediate to 16.00 dlrs a barrel, the copany said.
##     &amp;quot;The price reduction today was made in the light of falling
## oil product prices and a weak crude oil market,&amp;quot; a company
## spokeswoman said.
##     Diamond is the latest in a line of U.S. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  Reuter&lt;/BODY&gt;
##  &lt;/TEXT&gt;
## &lt;/REUTERS&gt;
##
##
## attr(,"class")
## [1] "XMLDocumentContent"
##
## $dtd
## $external
## NULL
##
## $internal
## NULL
##
## attr(,"class")
## [1] "DTDList"
##
## attr(,"Author")
## character(0)
## attr(,"DateTimeStamp")
## [1] "1987-02-26 17:00:56 GMT"
## attr(,"Description")
## [1] ""
## attr(,"Heading")
## [1] "DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES"
## attr(,"ID")
## [1] "127"
## attr(,"Language")
## [1] "en"
## attr(,"LocalMetaData")
## attr(,"LocalMetaData")$TOPICS
## [1] "YES"
##
## attr(,"LocalMetaData")$LEWISSPLIT
## [1] "TRAIN"
##
## attr(,"LocalMetaData")$CGISPLIT
## [1] "TRAINING-SET"
##
## attr(,"LocalMetaData")$OLDID
## [1] "5670"
##
## attr(,"LocalMetaData")$Topics
## [1] "crude"
##
## attr(,"LocalMetaData")$Places
## [1] "usa"
##
## attr(,"LocalMetaData")$People
## character(0)
##
## attr(,"LocalMetaData")$Orgs
## character(0)
##
## attr(,"LocalMetaData")$Exchanges
## character(0)
##
## attr(,"Origin")
## [1] "Reuters-21578 XML"
## attr(,"class")
## [1] "Reuters21578Document" "TextDocument"         "XMLDocument"
## [4] "XMLAbstractDocument"  "oldClass"
</code></p>

<p><code>r
## docs[['reut-00001.xml']] 同样可以提取第一个文档
</code></p>

<h3 id="section">3. 信息转化</h3>
<hr />

<p>创建好Corpus后，就需要对其进行一些修改，比如去除标点、停止词等等。这里就需要使用到一个函数<code>tm_map()</code>，其可以将转化函数作用到每一个文档数据上。</p>

<h4 id="section-1">1. 转化为纯文本</h4>

<p>如果Corpus中存储的是非纯文本的数据，比如XML格式的数据，那么就需要先将这些数据转换成纯文本格式:</p>

<p><code>r
docs &lt;- tm_map(docs, as.PlainTextDocument)
docs[[1]]
</code></p>

<p><code>
## DIAMOND SHAMROCK (DIA) CUTS CRUDE PRICES
## NEW YORK, FEB 26 -
## Diamond Shamrock Corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     The reduction brings its posted price for West Texas
## Intermediate to 16.00 dlrs a barrel, the copany said.
##     "The price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     Diamond is the latest in a line of U.S. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  Reuter
</code></p>

<h4 id="section-2">2. 去除特殊字符</h4>

<p>在文档数据中，可能会存在这样的字符：”/”、”@”、”-“等等。大部分时候，我们需要将其去除掉：</p>

<p><code>r
for (i in seq(docs)) {
    docs[[i]] &lt;- gsub("/", " ", docs[[i]])
    docs[[i]] &lt;- gsub("@", " ", docs[[i]])
    docs[[i]] &lt;- gsub("-", " ", docs[[i]])
}
</code></p>

<p>如果存在更复杂的替换，可以使用正则表达式去解决，这里不做介绍。</p>

<h4 id="section-3">3. 转换成小写</h4>

<p>顾名思义，就是将所有的数据转换成小写字母，这样以便更加容易分析。</p>

<p><code>r
docs &lt;- tm_map(docs, tolower)
docs[[1]]  ## 查看效果
</code></p>

<p><code>
## diamond shamrock (dia) cuts crude prices
## new york, feb 26
## diamond shamrock corp said that
## effective today it had cut its contract prices for crude oil by
## 1.50 dlrs a barrel.
##     the reduction brings its posted price for west texas
## intermediate to 16.00 dlrs a barrel, the copany said.
##     "the price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     diamond is the latest in a line of u.s. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  reuter
</code></p>

<h4 id="section-4">4. 去除数字</h4>

<p>有些时候，我们需要将文档中的数字去除掉：</p>

<p><code>r
docs &lt;- tm_map(docs, removeNumbers)
docs[[1]]
</code></p>

<p><code>
## diamond shamrock (dia) cuts crude prices
## new york, feb
## diamond shamrock corp said that
## effective today it had cut its contract prices for crude oil by
## . dlrs a barrel.
##     the reduction brings its posted price for west texas
## intermediate to . dlrs a barrel, the copany said.
##     "the price reduction today was made in the light of falling
## oil product prices and a weak crude oil market," a company
## spokeswoman said.
##     diamond is the latest in a line of u.s. oil companies that
## have cut its contract, or posted, prices over the last two days
## citing weak oil markets.
##  reuter
</code></p>

<h4 id="section-5">5. 去除停止词</h4>

<p><code>r
docs &lt;- tm_map(docs, removeWords, stopwords("english"))
docs[[1]]
</code></p>

<p><code>
## diamond shamrock (dia) cuts crude prices
## new york, feb
## diamond shamrock corp said
## effective today   cut  contract prices  crude oil
## . dlrs  barrel.
##      reduction brings  posted price  west texas
## intermediate  . dlrs  barrel,  copany said.
##     " price reduction today  made   light  falling
## oil product prices   weak crude oil market,"  company
## spokeswoman said.
##     diamond   latest   line  u.s. oil companies
##  cut  contract,  posted, prices   last two days
## citing weak oil markets.
##  reuter
</code></p>

<h4 id="section-6">6. 去除标点</h4>

<p><code>r
docs &lt;- tm_map(docs, removePunctuation)
docs[[1]]
</code></p>

<p><code>
## diamond shamrock dia cuts crude prices
## new york feb
## diamond shamrock corp said
## effective today   cut  contract prices  crude oil
##  dlrs  barrel
##      reduction brings  posted price  west texas
## intermediate   dlrs  barrel  copany said
##      price reduction today  made   light  falling
## oil product prices   weak crude oil market  company
## spokeswoman said
##     diamond   latest   line  us oil companies
##  cut  contract  posted prices   last two days
## citing weak oil markets
##  reuter
</code></p>

<h4 id="section-7">7. 去除多余的空格</h4>

<p><code>r
docs &lt;- tm_map(docs, stripWhitespace)
docs[[1]]
</code></p>

<p><code>
## diamond shamrock dia cuts crude prices
## new york feb
## diamond shamrock corp said effective today cut contract prices crude oil dlrs barrel reduction brings posted price west texas intermediate dlrs barrel copany said price reduction today made light falling oil product prices weak crude oil market company spokeswoman said diamond latest line us oil companies cut contract posted prices last two days citing weak oil markets reuter
</code></p>

<h4 id="stemming">8. Stemming(词干化)</h4>

<p>首先介绍一下什么是Stemming，我们知道在英文中一个单词会存在很多形式，比如说复数形式、过去分词等等。但其实它们外表看起来虽不一样，但实际上是一样的。所以在处理分析的时候，就需要将这些单词都转换成其本身。在R中可以使用<code>SnowballC</code>这个包来处理Stemming，举个例子：</p>

<p><code>r
require(SnowballC)
</code></p>

<p><code>
## Loading required package: SnowballC
</code></p>

<p><code>r
exam &lt;- c("prices, price, doing")
stemDocument(exam)
</code></p>

<p><code>
## [1] "prices, price, do"
</code></p>

<p>对于我们的例子：</p>

<p><code>r
require(SnowballC)
docs &lt;- tm_map(docs, stemDocument)
docs[[1]]
</code></p>

<p><code>
## diamond shamrock dia cut crude price
## new york feb
## diamond shamrock corp said effect today cut contract price crude oil dlrs barrel reduct bring post price west texa intermedi dlrs barrel copani said price reduct today made light fall oil product price weak crude oil market compani spokeswoman said diamond latest line us oil compani cut contract post price last two day cite weak oil market reuter
</code></p>

<h3 id="section-8">4. 创建词条-文档关系矩阵</h3>
<hr />

<p>文本挖掘中，词条-文档关系矩阵是构建模型的基础，后续分析建模都是建立在这个矩阵之上的。首先来了解一下这个矩阵，举个例子：</p>

<p>我们有两篇文档，内容分别为：text mining 和 data mining and text mining。 那么对应的矩阵为：</p>

<p><code>r
d.Exam &lt;- c("text mining", "data mining and text mining")
doc.Exam &lt;- Corpus(VectorSource(d.Exam))
dtm.Exam &lt;- DocumentTermMatrix(doc.Exam)
inspect(dtm.Exam)
</code></p>

<p><code>
## A document-term matrix (2 documents, 4 terms)
##
## Non-/sparse entries: 6/2
## Sparsity           : 25%
## Maximal term length: 6
## Weighting          : term frequency (tf)
##
##     Terms
## Docs and data mining text
##    1   0    0      1    1
##    2   1    1      2    1
</code></p>

<p>可以看到，词条-文档关系矩阵其实就是将文档作为列，词条作为行，矩阵的每个位置就是对应的词条在对应的文档中出现的次数。</p>

<p>对于我们的例子，可以这样来生产词条-文档矩阵：</p>

<p><code>r
dtm &lt;- DocumentTermMatrix(docs)
inspect(dtm[1:5, 1:10])
</code></p>

<p><code>
## A document-term matrix (5 documents, 10 terms)
##
## Non-/sparse entries: 1/49
## Sparsity           : 98%
## Maximal term length: 6
## Weighting          : term frequency (tf)
##
##      Terms
## Docs  abdul abil abl abroad abu accept accord across activ add
##   127     0    0   0      0   0      0      0      0     0   0
##   144     0    2   0      0   0      0      0      0     0   0
##   191     0    0   0      0   0      0      0      0     0   0
##   194     0    0   0      0   0      0      0      0     0   0
##   211     0    0   0      0   0      0      0      0     0   0
</code></p>

<p>到这里，我们就已经生成了词条-文档矩阵。下一次，就来看看如何对这个矩阵进行一些操作，以及如何利用这个矩阵进行后续的建模分析。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[因子分析2]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/19/factor-analysis2/"/>
    <updated>2014-05-19T15:00:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/19/factor-analysis2</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article35_new.jpg" alt="article 35" />
<!-- more --></p>

<p>图片来源于<a href="http://software.ssri.co.jp/statweb2/column/column0811.html">网址</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>这两天来了个同学，大家聚了聚，我也乘机休息了两天（好奢侈！）。这两天属于什么都没有写，就翻看了两本书。一本是二月河的<a href="http://www.amazon.cn/gp/product/B0032K0X6Q/ref=olp_product_details?ie=UTF8&amp;me=&amp;seller=">康熙大帝</a>，另外一本是<a href="http://www.ituring.com.cn/book/894">推荐系统实践</a>，这本书的电子版，图灵正在打折，有兴趣可以买本看看。</p>

<hr />

<p>好了，不废话了，下面就接着上一篇讲的继续！上一篇简单介绍了因子分析的一些概念，以及最基础的因子模型：<strong>正交因子模型</strong>。那么这一篇，就来说说正交因子模型的<strong>参数估计问题</strong>。</p>

<p>对于一组p维样本，有n个观测值：<script type="math/tex">x_1, x_2, \dots, x_n</script>，则其均值和协方差矩阵可以使用样本均值和样本协方差矩阵来估计：</p>

<script type="math/tex; mode=display"> \hat{\mu} = \overline{x} = \frac{1}{n} {\sum_{i=1}^{n} x_i} </script>

<script type="math/tex; mode=display"> \hat{\Sigma} = S = \frac{1}{n-1} {\sum_{i=1}^{n} (x_i-\overline{x})(x_i-\overline{x})'} </script>

<p>在因子模型中，需要估计的有两个参数：<strong>因子载荷矩阵</strong><script type="math/tex">A=(a_{ij}:p \times m)</script>以及<strong>特殊方差矩阵</strong><script type="math/tex">D = diag(\sigma_1^2, \sigma_2^2, \dots, \sigma_p^2)</script>。</p>

<hr />

<h4 id="section">1. 主成分法</h4>
<hr />

<p>主成分法的思想取自于主成分分析，即是取出前m个成分作为主成分，然后以此来得到因子载荷矩阵的估计；然后再以协方差阵和因子载荷矩阵为条件，直接推出特殊方差矩阵。具体如下：</p>

<ol>
  <li>求出协方差矩阵<script type="math/tex">S</script>的特征值：<script type="math/tex">\hat{\lambda}_1 \geqslant \hat{\lambda}_2 \geqslant \dots \geqslant \hat{\lambda}_p \geqslant 0</script>，其对应的特征向量为：<script type="math/tex">\hat{t}_1, \hat{t}_2, \dots, \hat{t}_p</script>。</li>
  <li>选一个较小因子数<script type="math/tex">m</script>，并且使得前m项累计贡献率<script type="math/tex">\frac{\sum_{i=1}^{m} \hat{\lambda}_i}{\sum_{i=1}^{p} \hat{\lambda}_i} </script>高于设定值。</li>
  <li>将协方差矩阵<script type="math/tex">S</script>做这样的近似：<script type="math/tex">S = \sum_{i=1}^{m} \hat{\lambda}_i + \sum_{i=m+1}^{p} \hat{\lambda}_i \approx \sum_{i=1}^{m} \hat{\lambda}_i + \hat{D} := \hat{A}\hat{A}' + \hat{D} </script></li>
</ol>

<p>其中，<script type="math/tex">A = (\sqrt{\hat{\lambda}_1}t_1, \sqrt{\hat{\lambda}_2}t_2, \dots, \sqrt{\hat{\lambda}_m}t_m) </script>,<script type="math/tex">\hat{D} = diag(\hat{\sigma}_1^2, \hat{\sigma}_2^2, \dots, \hat{\sigma}_p^2) </script>,<script type="math/tex">\hat{\sigma}_i^2 = s_{ii} - \sum_{j=1}^{m} \hat{a}_{ij}^2 </script></p>

<p>从上述的过程来看，我们是使用了一种近似的方法估计出了<script type="math/tex">A</script>和<script type="math/tex">D</script>，那么这就有一个<strong>残差矩阵</strong><script type="math/tex">S - (\hat{A}\hat{A}' + \hat{D}) </script>，显然这个矩阵的对角线元素为0。既然是一种近似，那么，如果这个残差矩阵的非对角线元素都非常小的时候，我们就可以认为取<script type="math/tex">m</script>个因子的模型就可以很好地解释或者是拟合原始的数据了。</p>

<hr />

<h4 id="section-1">2. 主因子法</h4>
<hr />

<p>对于因子模型，我们先对原始向量进行标准化，则有：<script type="math/tex"> R = AA' + D</script>。取<strong>约相关矩阵</strong><script type="math/tex">R^* = R - D = AA'</script>，假设特殊方差<script type="math/tex">\sigma_i^2</script>的一个估计值<script type="math/tex">\hat{\sigma}^2</script>为初始估计，则有约相关矩阵的估计值为：</p>

<p><img src="/images/a35/eq2_1.png" alt="" /></p>

<p>其中<script type="math/tex">\hat{R}</script>为样本相关矩阵，<script type="math/tex">\hat{D} = diag(\hat{\sigma^2_1},\hat{\sigma_2^2}, \dots, \hat{\sigma_p^2}) </script>, <script type="math/tex">\hat{h_i^2} = 1 - \hat{\sigma_i^2} </script>为<script type="math/tex">h_i^2</script>的初始估计。</p>

<p>计算<script type="math/tex">\hat{R}^*</script>的特征值，取足够小，但累计贡献率达到要求的m：<script type="math/tex">\hat{\lambda^*_1} \geqslant \hat{\lambda^*_2} \geqslant \dots \geqslant \hat{\lambda^*_m} > 0 </script>，其对应的特征向量为：<script type="math/tex">\hat{t^*_1}, \hat{t^*_2}, \dots, \hat{t^*_m}</script>，则得到 <script type="math/tex">A</script> 的估计值：</p>

<script type="math/tex; mode=display"> \hat{A} = (\sqrt{\hat{\lambda^*_1}}t^*_1, \sqrt{\hat{\lambda^*_2}}t^*_2, \dots, \sqrt{\hat{\lambda^*_m}}t^*_m ) </script>

<p>那么<script type="math/tex">\sigma^2_i</script>的最终估计为： <script type="math/tex"> \hat{\sigma_i^2} = 1-\hat{h_i^2} = 1 - \sum_{j=1}^{m} \hat{a_{ij}^2} </script>。</p>

<p>可以看到，这是一个可以迭代的过程，我们可以一直迭代，直到结果达到稳定为止！从过程来看，这里其实也是利用了主成分，因而，主因子法也是主成分法的一种修正！</p>

<p>那么接下来的问题就是，这个特殊方差<script type="math/tex">\sigma^2_i</script>的初始估计值应该如何取呢？最常用的取法：<script type="math/tex"> \hat{\sigma_i^2} = 1/r^{ii} </script>, 其中<script type="math/tex"> r^{ii} </script>为<script type="math/tex">\hat{R}^{-1}</script>对角线元素的第<script type="math/tex">i</script>个。</p>

<hr />

<h4 id="section-2">3. 极大似然法</h4>
<hr />

<p>使用极大似然估计，那么就肯定需要使用样本的分布，这里我们假定公共因子<script type="math/tex">f \sim N_m(0, I) </script>，特殊因子<script type="math/tex"> \varepsilon \sim N_p(0, D) </script>，并且相互独立！这里的假设其实就是来源于模型的正交性假设，只不过是将正交性假设进一步限定，假设它们都是属于多元正态分布！</p>

<p>有了上述假设，通过模型就可以知道<script type="math/tex">x \sim N_p(\mu, \Sigma) </script>，有了这个就可以计算样本的似然函数了，这里涉及到较为复杂的矩阵计算，不想多说，有兴趣的话可以查找一些资料；或者学习一下线性模型中关于矩阵求导的知识，然后自己推导一下。</p>

<p>一般情况下，极大似然法使用得并不太多，因为这个方法是算不出显式解的，在没有限制条件的情况下，解也并不唯一确定！但是如果是在因子分部可以明显知道的情况下，使用这个方法就比较好了！</p>

<hr />

<h3 id="section-3">总结</h3>
<hr />

<p>到这边，对于基础的因子模型就介绍结束了。回顾一下，主要就是介绍了正交因子模型以及它的参数估计问题。但是，到这里，我们还没有说到模型中的公共因子如何解释这个问题！对于这个问题的解释，通常结合实际的问题，需要一定的专业知识和经验，然后才能给每个公共因子给出一个实际意义。而且，公共因子的解释，在很大程度上也依赖于因子模型中因子载荷矩阵的元素结构！这个时候就会牵扯出因子分析中其它的一些问题：<strong>因子旋转</strong>和<strong>因子得分</strong>。因为这两个问题涉及到一些比较复杂的数学知识，我不能够在清减数学的情况下说好它，如果单说怎么用，我觉得没有必要，所以暂时并不打算介绍这两个问题。有兴趣的，可以翻阅一些多元统计的书籍，一般都会有讲。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[因子分析1]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/14/factor-analysis1/"/>
    <updated>2014-05-14T16:05:45+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/14/factor-analysis1</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article34_new.jpg" alt="article 34" />
<!-- more --></p>

<p>图片来源于<a href="http://software.ssri.co.jp/statweb2/column/column0811.html">网址</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p><em>updata:</em></p>

<p>感谢谢老大的提醒，让我发觉，在机器学习下放置因子分析并不合适。详细情况参见文章的回复板块。</p>

<hr />

<p>前一篇介绍的主成分分析(PCA)，是一种降维技术；这一篇介绍的因子分析也是一种降维的方法，不仅如此，还可以将因子分析看作是主成分分析的一种推广和发展。与之主成分分析相比较，因子分析更为灵活，对变量降维后的解释能够更加清楚。</p>

<p>但因子分析和主成分分析有非常多的不同点。</p>

<ol>
  <li>主成分分析不能作为一个模型来描述，主成分是观测变量的线性组合；</li>
  <li>因子分析需要构造因子模型，观测的原始变量是因子的线性组合。</li>
</ol>

<hr />

<h4 id="section">初窥</h4>
<hr />

<p>在介绍因子模型之前，可以先看看这个因子分析到底是要干什么，以及是怎么干的！</p>

<p>在二维空间中，主成分分析，它想做的是寻找一组新的变量<script type="math/tex">y_1,y_2</script>，用它去替代原来的变量<script type="math/tex">x_1,x_2</script>，并且满足<script type="math/tex">y_1</script>和<script type="math/tex">y_2</script>这两个变量都是<script type="math/tex">x_1,x_2</script>两个变量的线性组合！即：</p>

<p><img src="/images/a34/eq1.jpg" alt="" /></p>

<p>而在因子模型中，我们需要做的跟此不同。我们需要找到一组潜在变量(不可观测)，用这组潜在变量的线性组合去表示原始变量<script type="math/tex">x_1,x_2</script>。这里假设有1个潜在变量<script type="math/tex">f_1</script>，那么因子模型可以描述成：</p>

<p><img src="/images/a34/eq2.png" alt="" /></p>

<p>其中，<script type="math/tex">f_1</script>就是因子，称为<strong>公共因子</strong>；<script type="math/tex">a_{ij}</script>称之为变量<script type="math/tex">x_i</script>在因子<script type="math/tex">f_j</script>上的<strong>载荷</strong>；<script type="math/tex">\mu_i</script>是<script type="math/tex">x_i</script>的均值；<script type="math/tex">\varepsilon_i</script>为特殊因子，即不能被公共因子解释的部分。</p>

<hr />

<h4 id="section-1">正交因子模型</h4>
<hr />

<p>首先看看最基础的因子模型，就是正交假设下的因子模型：</p>

<p><img src="/images/a34/eq3.png" alt="" /></p>

<p>在给出假定之前，我们先将上面式子转换成矩阵形式：</p>

<script type="math/tex; mode=display"> x = \mu + Af + \varepsilon </script>

<p>其中，<script type="math/tex">x = (x_1, x_2, \dots, x_p)'</script>，<script type="math/tex">\mu = (\mu_1, \mu_2, \dots, \mu_p)'</script>为均值向量，<script type="math/tex">\varepsilon = (\varepsilon_1, \varepsilon_2, \dots, \varepsilon_p)'</script>为特殊因子向量, <script type="math/tex">f = (f_1, f_2, \dots, f_p)'</script>为公共因子向量，<script type="math/tex"> A = (a_{ij}):p \times m </script>为载荷矩阵。那么我们就可以给出如下的正交假设：</p>

<p><img src="/images/a34/eq4.png" alt="" /></p>

<p>在这样的假定下，我们首先来计算一下，原始变量<script type="math/tex">x</script>的协方差：</p>

<script type="math/tex; mode=display"> \Sigma = V(x) = V(Af+\varepsilon) = Cov(Af+\varepsilon,Af+\varepsilon) </script>

<p>又：<script type="math/tex">Cov(Af+\varepsilon,Af+\varepsilon)=AV(f)A'+ACov(f,\varepsilon)+Cov(\varepsilon,f)A'+V(\varepsilon)</script></p>

<p>由于<script type="math/tex">V(f) = I, Cov(f, \varepsilon) = Cov(\varepsilon, f) = 0</script>，所以：</p>

<script type="math/tex; mode=display"> \Sigma = AA' + V(\varepsilon) = AA' + D </script>

<p><strong>显然，我们要处理正交因子模型，最重要的就是求解<script type="math/tex">A,D</script>的估计值，那么这里就给出了这两个量与原始变量的协方差矩阵间的关系。</strong></p>

<p><strong>那么我们开始所说的，因子分析也是一种降维手段体现在哪里呢？</strong>这个就体现在，公共因子的数量上，当公共因子的数量少于原始变量的数量时，使用因子去解释原始变量就达到了一种降维的目的！</p>

<hr />

<p><strong><em>载荷矩阵</em></strong></p>

<p>显然，载荷矩阵<script type="math/tex">A</script>是我们关心的一个重点。首先，我们想弄明白<script type="math/tex">A</script>中的元素<script type="math/tex">a_{ij}</script>是否有什么具体的含义：</p>

<script type="math/tex; mode=display">Cov(x_i,f_j)=Cov(\sum_{k=1}^{m}a_{ik}f_k + \varepsilon_i, f_j) =a_{ij}Cov(f_j,f_j) = a_{ij} </script>

<p>那么可以看到，<script type="math/tex">a_{ij}</script>是<script type="math/tex">x_i</script>和<script type="math/tex">f_j</script>之间的协方差函数。</p>

<p>经过上面的计算，我们容易得到：</p>

<script type="math/tex; mode=display">V(x_i) = a_{i1}^2 + a_{i2}^2 + \dots + a_{1m}^2 + V(\varepsilon_i)</script>

<p>记<script type="math/tex">h_i^2 = \sum_{j=1}^{m}a_{ij}^2</script>，那么上式可转化为：</p>

<script type="math/tex; mode=display"> (V(x_i) =) \sigma_{ii} = h_i^2 + \sigma_i^2, i=1,2,\dots,p</script>

<p>这样就将<script type="math/tex">x_i</script>的方差进行了一个分解，一部分由公共因子解释，即<script type="math/tex">h_i^2</script>，称为<strong>共性方差</strong>；另一部分由特殊因子解释，即<script type="math/tex">\sigma_i^2</script>，称为<strong>特殊方差</strong>。</p>

<hr />

<p>至此，因子分析的基础模型就介绍完了，下面剩下的就是如何去进行参数的估计，这一般有三种方法：主成分法、主因子法以及极大似然法。下一篇，我们就来详细说说因子分析的参数估计问题。</p>
]]></content>
  </entry>
  
</feed>
