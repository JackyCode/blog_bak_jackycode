<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DataScience | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/datascience/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-05-07T19:11:25+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习14: 关联分析之apriori算法]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/07/apriori/"/>
    <updated>2014-05-07T17:00:48+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/07/apriori</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical31.jpg" alt="artical 31" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在上一篇中，我们介绍了关联分析相关的概念，这一节来看看如何使用Apriori算法去寻找满足条件的项集。</p>

<p>首先回顾一个概念，<strong>一个项集的支持度</strong>就是数据集中该项集所占的比例。Apriori算法就是用于寻找数据集中，支持度和可信度超过某一给定值的项集和关联规则。</p>

<hr />

<h4 id="section">一、原理</h4>
<hr />

<p>在介绍算法之前，首先了解一个集合论中的性质定理：集合的向下封闭性。</p>

<p>我们通过一个例子来看看这个定理，见下图：</p>

<p><img src="/images/a31/apriori_1.jpg" alt="" /></p>

<p>集合的向下封闭性，在这边解释的话，也就是说，<strong>如果一个项集的支持度低于某一个值，那么该项集超集的支持度也必定低于这个值。如果一个项集的支持度高于某一个值，那么该项集子集的支持度也必定高于某一个值。</strong></p>

<p><strong>超集</strong>：就是指包含这个集合中所以元素的集合（不包括自身），比如集合ABC就是集合AB的超集。</p>

<p>那么这个定理放在上一个图当中，就有这样的含义：</p>

<p><img src="/images/a31/apriori_2.jpg" alt="" /></p>

<hr />

<h4 id="section-1">二、算法构成</h4>
<hr />

<p>有了上面这个原理，那么就可以利用这个原理去减少我们寻找频繁集的计算量。因为，只要我们找到一个项集，其支持度低于给定的值，那么这个项集的所有超集就可以直接忽略不计了。如上图，项集A的支持度低于指定的值，那么其超集就都不用再考虑了。</p>

<p><strong>Apriori算法由两部分构成：</strong></p>

<ol>
  <li>找到满足最小支持度的项集；</li>
  <li>找到可信度超过最小可信度的关联规则。</li>
</ol>

<p>下面，我们一个一个地解决：</p>

<hr />

<h5 id="section-2">2.1. 寻找频繁项集</h5>
<hr />

<p>利用上面所讲的原理，我们来整理一下这个步骤的流程：</p>

<ol>
  <li>从数据集中构造集合C1，C1满足：大小为1的所有候选项集的集合，例如上图中的：C1 = {A, B, C};</li>
  <li>计算C1中所有项集（单元素项集）是否满足最小支持度，满足的项集构成集合L1，例如上图中的：L1 = {B, C};</li>
  <li>利用L1生成新的候选项集C2，C2满足：大小为2的所有候选项集的集合，例如上图中得：C2 = {BC};</li>
  <li>计算C2中所有项集（双元素项集）是否满足最小支持度，满足的项集构成集合L2；</li>
  <li>重复直到Lk中得元素个数为1。</li>
</ol>

<hr />

<h5 id="section-3">2.2. 寻找关联规则</h5>
<hr />

<p>在得到频繁项集之后，要寻找关联规则就容易多了。可以直接从频繁项集中构造初始的关联规则，计算该关联规则的可信度，然后与给定的最小可信度作比较，若值大于最小可信度，则记录该关联规则。</p>

<p>在实际编程时，需要注意使用<strong>集合的向下封闭性</strong>！！！想想看，在关联规则中，这个性质应该怎样去实现？(可以到Machine Learning in Action中找答案！)</p>

<h4 id="r">三、R语言实现</h4>

<h5 id="section-4">1. 使用自带的程序</h5>

<p>在R语言的<code>arules</code>这个包里面，提供了一个实现Apriori算法的函数：<code>apriori()</code>：</p>

<p><code>r
# 构造数据集
dataSet &lt;- matrix(0, 5, 3)
rownames(dataSet) &lt;- paste("item", 1:5, sep='')
colnames(dataSet) &lt;- c("A", "B", "C")
dataSet[1,] &lt;- c(1, 1, 0)
dataSet[2,] &lt;- c(1, 0, 1)
dataSet[3,] &lt;- c(1, 0, 1)
dataSet[4,] &lt;- c(1, 1, 1)
dataSet[5,] &lt;- c(0, 1, 1)
dataSet
# 转换数据格式(可以?apriori查看数据格式)
dataSet_class &lt;- as(dataSet,"transactions")
# 构造频繁项集
rules&lt;-apriori(dataSet_class,parameter=list(supp=0.5,conf=0.6,target="rules"))
# 查看结果
summary(rules)
# 构造关联规则
inspect(rules)
</code></p>

<h5 id="section-5">2. 自定义函数解决</h5>

<p>相对而言，Apriori算法的函数比较难以编写，原因可想而知，肯定是因为数据结构的问题！但是也只是比其他函数难编一点，毕竟其自带的数据结构功能还是非常强大的。我在<a href="/datascience">我的项目</a>中给出的一种编写方式，是利用R语言的list来实现的。不过，我想，利用Matrix或者data.frame，当然如果你还懂<code>data.table</code>的话，那么肯定也是可以编写的，而且我想应该会比用list简单！(没有亲手编写，只是猜想！)</p>

<p>详见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习13: 关联分析]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis/"/>
    <updated>2014-05-04T10:12:50+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical29.jpg" alt="artical 29" />
<!-- more --></p>

<p>标题图片出处：<a href="http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter002/section003/blue/page002.html">Rule Generation</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">一、简介</h4>
<hr />

<p>在统计学中，变量与变量之间的关系是统计结构的重要参数，统计的核心问题也就是研究变量与变量之间的关系。如果变量与变量之间不独立，那么这两个变量之间肯定存在一定的关联性，那么如何处理度量这个关联性，在统计中就称为关联分析。</p>

<p>列联表是传统统计学中度量两个分类变量之间关系强弱的方法，但是这个方法是对于两个固定的变量进行的一种测量。在机器学习中，常常会遇到大规模的变量，这时候更倾向于从众多的变量中最快地找到关联性最强的两组或多组变量。那么使用列联表就显得不太合适了，此时应该使用什么方法呢？</p>

<hr />

<h4 id="section-1">二、关联规则</h4>
<hr />

<p>首先介绍2个定义：</p>

<ol>
  <li><strong>k项集：</strong>设<script type="math/tex">I = \{ i_1, i_2, \dots, i_m \}</script>是$m$个待研究的项构成的有限项集，对于给定的事物数据表<script type="math/tex">T = \{ T_1, T_2, \dots, T_n \}</script>，其中任意的$T_i$是$I$中的$k$项组成的集合，称之为<strong>k项集</strong>。</li>
  <li><strong>关联规则</strong>：形如<script type="math/tex"> X \rightarrow Y</script>的形式，其中<script type="math/tex"> X \subseteq I </script>, <script type="math/tex">Y \subseteq I</script>，且有<script type="math/tex"> X \bigcap Y = \emptyset </script>。</li>
</ol>

<hr />

<h5 id="section-2">度量方式</h5>
<hr />

<p>对于一个项集，我们正常用支持度来度量它的频繁程度，其实就是其在数据集中出现的比例，这个很容易理解，就不多说了。</p>

<p>那么下面要讨论的就是：<strong>如何度量一个关联规则</strong>。一般使用下面两个概念：</p>

<ol>
  <li><strong>支持度S</strong>：定义为X和Y同时出现在一个事务中得可能性，即：
 <script type="math/tex"> S(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T \mid </script>
 其中，<script type="math/tex">\mid T(X \vee Y) \mid</script>表示同时包含X和Y的事务数，<script type="math/tex">\mid T \mid</script>表示总事务数。</li>
  <li><strong>支持度C</strong>：定义为出现在关联规则前项中得事务中出现关联规则后项的比例，即：
 <script type="math/tex"> C(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T(X) \mid </script>
 其中，<script type="math/tex"> \mid T(X) \mid</script>表示包含X的事务数。</li>
</ol>

<hr />

<h5 id="section-3">例子</h5>

<hr />

<p>这边举个简单地例子，方便理解上面的概念。比如设计一个购物数据：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">id</th>
      <th style="text-align: center">items</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">AB</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">AC</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">C</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">ABC</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">BC</td>
    </tr>
  </tbody>
</table>

<p>上面数据的意思，比如第一行就表示，id为1的人购买了A和B两个物品，其他的意思类似。那么这个AB就是一个2项集，因为存在ABC三种物品，这边就有6种关联规则：<script type="math/tex">A \Rightarrow B</script>, <script type="math/tex">A \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow A </script>, <script type="math/tex"> C \Rightarrow A</script>, <script type="math/tex"> C \Rightarrow B </script>。</p>

<p>那么如何计算一个关联规则的支持度与可信度呢？</p>

<p>试试关联规则<script type="math/tex">A \Rightarrow B</script>：</p>

<script type="math/tex; mode=display"> S(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T \mid = 2/5 </script>

<script type="math/tex; mode=display"> C(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T(A) \mid = 2/3 </script>

<p>是不是很简单呢。试试计算一下其他的关联规则，多算几次就能够很了解其中的含义了。</p>

<hr />

<h5 id="section-4">关联规则的作用</h5>
<hr />

<ol>
  <li>
    <p>关联规则的目的在于，找到变量之间支持度和可信度都比较高的那些关联规则。</p>
  </li>
  <li>
    <p>关联规则的支持度，用于度量关联规则在数据库中得普适程度，是对关联规则重要性(适用性)的一种度量。</p>
  </li>
  <li>
    <p>关联规则的可信度，这是一个相对指标，是对关联规则准确度的一个度量。值越高，代表这个关联规则的后项依赖前项的可能性比较高。</p>
  </li>
</ol>

<hr />

<h4 id="section-5">小节</h4>
<hr />

<p>这篇简单介绍了一下关联分析的一些概念问题，以及如何度量一个关联规则。但是，试想一想，要是$I$中包含的项很多，事物数据表$T$也很大，那么要计算所有关联规则的支持度和可信度，难度可想而知！这时候就需要使用一些算法去解决这个问题，现在比较流行的算法就是：处理静态关联规则的Apriori算法和处理动态关联规则的Carma算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习12: Logisic回归]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/27/logistic-regression/"/>
    <updated>2014-04-27T10:34:11+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/27/logistic-regression</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical27.jpg" alt="artical 27" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在数据科学系列的开头，花了三篇介绍了线性回归。线性回归模型应该是定量分析中最常用的一种统计分析方法。但是线性回归处理数据时，要求因变量是连续型变量。但是很多时候，需要处理的数据，其因变量并不是连续的。像性别、对错等等，这种离散的因变量，一般称为分类响应变量。</p>

<p>在机器学习的<a href="http://jackycode.github.io/blog/2014/03/30/data-science-an-introduction-to-machine-learning/">介绍篇</a>中，已经介绍了监督学习与非监督学习。在监督学习中，最主要的两类问题，一个就是回归，另一个就是分类。而Logistic回归就是处理二元分类的一种方法，当然其也存在自身的问题，这个后面再讲。</p>

<h4 id="sigmoid">Sigmoid函数</h4>
<hr />

<p>要了解Logistic回归，首先需要了解一下Sigmoid函数。为什么呢？</p>

<p>一般来说，我们会定义二元分类变量的输出为0和1，这种函数叫做单位阶跃函数，也称Heaviside step function。这个函数的特点就是其取值可以从0突变到1，反之也可。学过数分的话就知道，这种函数有时候会非常难以处理，因为带跳突变，导致了不可微不可导。在这里，就可以借助Sigmoid函数，因为这个函数可以近似地描述单位阶跃函数的特点。首先看看Sigmoid函数：</p>

<script type="math/tex; mode=display"> p = \frac{exp(y)}{1+exp(y)} = \frac{1}{1 + exp(-y)}</script>

<p>简单看一下这个函数，当$y=0$时，$p=0.5$；当$y$变大，趋近于无穷时，$y$趋近1；反之，$y$趋近0。而且，这种趋近的速度是非常快的。正是因为这个趋近速度非常快，我们可以使用Sigmoid函数来处理这边的单位阶跃函数。</p>

<h4 id="logistic">Logistic回归</h4>
<hr />

<p>利用Sigmoid函数可以将单位阶跃函数做个近似，而Sigmoid函数是连续的，那么就可以利用之前的线性回归来建立模型。</p>

<p>令$ y = X\beta $，又<script type="math/tex"> p = \frac{1}{1 + exp(-y)} </script>，变形可得Logistic回归模型：</p>

<script type="math/tex; mode=display"> logit(p) = ln(\frac{p}{1-p}) = X\beta </script>

<p>上述的$logit(p)$称为$logit$变换，此时$p$就是响应变量，$X$就是自变量。到这，我们就可以利用线性模型对参数进行估计了。</p>

<h5 id="section">统计中的解释</h5>
<hr />

<p>这里面的$p$除了利用Sigmoid函数来解释之外，还可以利用统计中的二项分布来解释，而且从某种角度来说，这个解释会更便于理解。试想，我们这边需要处理的二元分类变量就是0和1。我们考虑0就是“不发生”，1就是“发生”，那么我们可以将前面的$p$理解成发生的概率。通过对已知数据建立模型，估计出参数，我们就可以利用模型去预测在不同的自变量条件下，“发生”的概率是多大，从而达到一个分类的目的。</p>

<p>从这边的分析就可以看到，Logistic回归的缺点：那就是欠拟合，会导致分类的精度下降。</p>

<h5 id="r">R语言实现</h5>
<hr />

<p>因为这个属于线性回归的一种变形，所以求解的方式直接<strong>借用线性回归</strong>即可。</p>

<p>当然，也存在其它的方法。因为Logistic回归是属于广义线性回归模型的，在R中有专门处理广义线性模型的函数<code>glm</code>：</p>

<p><code>
model &lt;- glm(formula, family=binominal(link = logit), data=data.frame)
</code></p>

<p>这里处理的方式中，利用了连接函数(link=logit)，感兴趣的话可以找找广义线性模型的内容看看，当然，以后如果介绍统计模型的话，这个肯定也是必讲得内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习11: 聚类分析2]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/24/cluster-analysis2/"/>
    <updated>2014-04-24T09:39:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/24/cluster-analysis2</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical26.jpg" alt="artical 26" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇介绍了聚类分析的定义，给出了很多不同的相似性的度量方法。这一篇主要想介绍一下除了快速聚类之外的另外一种聚类方法：<strong>系统聚类法</strong>。</p>

<h3 id="section">二、系统聚类法</h3>
<hr />

<p>系统聚类法，hierarchical clustering method，是聚类分析方法中用的较多的一种。其具体过程如下：</p>

<ol>
  <li>对于n个样品，构造n个类，每个样品单独作为一类。计算每个类之间的距离；</li>
  <li>合并距离最近的两个类为一个新类；</li>
  <li>计算新类与其它类之间的距离，重复2直到所有类合并称为1个类为止。</li>
</ol>

<p>那么类与类之间的距离如何定义呢？</p>

<p>其实，类与类之间的距离有很多中定义方式常见的有：</p>

<ol>
  <li>最短距离法，single linkage method，<script type="math/tex"> D_{KL} = \min_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>最长距离法，complete linkage method，<script type="math/tex"> D_{KL} = \max_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>中间距离法，median linkage method，即取最远距离与最近距离两者的中间距离；</li>
  <li>类平均法，average linkage method，<script type="math/tex"> D_{KL} = \frac{1}{n_Kn_L} \sum_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>重心法，centroid hierarchical method，即取类重心之间的距离；</li>
  <li>离差平方和法，Ward’s minimum variance method, 定义较为繁琐，可以自行Google；</li>
</ol>

<p><strong>注</strong>：系统聚类的方法并不困难，但是实现时会存在计算量的问题。系统聚类法一般是在样品间距离矩阵的基础上进行的，它需要计算所有点到所有点之间的距离，当样品量很大时，这个计算量会变得非常的大。因而，很多时候人们会采用动态聚类的方法去处理数据，动态聚类法中一种最常用的方法就是之前已经介绍过的KMeans方法。</p>

<h4 id="r">R语言实现</h4>

<p>在R语言中，自带了一个函数可以实现系统聚类：<code>hclust</code>。可以自己查阅help。</p>

<h3 id="section-1">三、聚类的一些问题</h3>
<hr />

<ol>
  <li>量纲问题。实际问题中，由于数据采用的量纲不同，很多时候需要对数据进行一些变换，最常用的就是标准化。但也有一些其它方式：极差变换（数据除以极差）；主成分变换（用主成分代替本身数据）；对数变换等等。</li>
  <li>kmeans算法只有在类的平均值可以被定义的情况下使用，所以在一些特殊的场合，kmeans并不适用。比如分类数据等等!!</li>
  <li>kmeans算法使用平均值作为衡量，这就造成了一个新的问题。即kmeans不适用于含有异常值的数据，非凸面的数据以及大小值相差很大的数据。</li>
  <li>聚类的一个<strong>难点</strong>在于：确定类的个数。通过上面介绍的算法来看，所有的方法都需要自己去定义类的个数。那么如何去定义类的个数呢？这是一个到现在还没有满意解决的问题。常用的方式就是观察样品散点图，查看变化率，以及使用一些假设检验的方式（感兴趣可以翻阅专业的书籍材料，比如上海财经出版社的应用多元分析中就有讲到这部分内容）。</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习10: 聚类分析1]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/21/cluster-analysis/"/>
    <updated>2014-04-21T18:46:04+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/21/cluster-analysis</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical25.jpg" alt="artical 25" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇介绍了聚类分析中的KMeans算法，这一节就来具体地说说聚类分析。聚类分析，cluster analysis，是一种研究“物以类聚”现代统计学分析方法，其目的是要把分类对象按照一定的规则分成若干个类。这些类别并非事先给定的，而是根据数据的特征确定的。</p>

<hr />

<h5 id="note">NOTE：聚类的划分：</h5>

<ol>
  <li>
    <p>根据分类对象的不同，可以分为：<strong>Q型聚类分析</strong>和<strong>R型聚类分析</strong>。Q型是指对样品对象进行聚类；而R型则是对变量(属性)进行聚类。</p>
  </li>
  <li>
    <p>按照分析方法的不同，又可以分为：<strong>系统聚类法</strong>、<strong>快速聚类法</strong>和<strong>模糊聚类法</strong>。上一篇介绍的KMeans法就是快速聚类法中的一种。</p>
  </li>
</ol>

<hr />

<h4 id="section">一、相似性的度量</h4>
<hr />

<p>在上一篇中，我们已经介绍过，聚类其实就是将相似度高的样品啊属性啊合并成一个类别。但是，上一篇我们仅仅给出了一种也是最简单的一种相似性的度量方式——欧式距离。这里我们详细看看相似性有哪些度量方式：</p>

<p>除了使用<strong>有序尺度变量</strong>（将属性划分为一级、二级等等的有次序关系的量来表示）和<strong>名义尺度变量</strong>（使用既没有等级关系，又不存在数量关系的量来表示。比如男女）之外，一般采用的测量尺度的方式就是<strong>间隔尺度变量</strong>。</p>

<p><strong>间隔尺度变量</strong>即是使用连续的量来表示测量尺度，一般都是连续型的，比如欧式距离、重量等等。一般来讲，在应对Q型聚类时会使用<strong>距离</strong>去度量；而对R型聚类来说，则会使用<strong>相似系数</strong>这种方式去度量。下面来分别看一看：</p>

<hr />

<h5 id="a-">a. 距离</h5>
<hr />

<p>上一篇中使用的欧氏距离即是这里的一种，在介绍各种不同的距离定义之前，首先看看距离的定义需要满足哪些条件：</p>

<ol>
  <li>首先，距离必须是非负的。即：<script type="math/tex">d_{ij} \geqslant 0, \forall i,j</script>;</li>
  <li>对于相同取值的样品，之间的距离必须为0。即：<script type="math/tex">d_{ij} = 0</script>，当且仅当，第i个样品与第j个样品的各变量值相同；</li>
  <li>i样品到j样品的距离与j样品到i样品的距离相等。即：<script type="math/tex">d_{ij} = d_{ji}, \forall i, j</script>；</li>
  <li>满足：<script type="math/tex">d_{ij} \leqslant d_{ik} + d_{kj}, \forall i,j,k</script>。</li>
</ol>

<p>下面就来看看常用的距离定义，首先看看最常用的Minkowski距离：</p>

<hr />

<h6 id="minkowski">(1). Minkowski距离</h6>

<script type="math/tex; mode=display"> d_{ij}(q) = [\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid ^ q}]^{1/q} </script>

<p>观察这个距离可以看到，当$q=2$时，上面定义的距离就是常用的欧氏距离。另外：</p>

<ul>
  <li><script type="math/tex">q=1</script>时，<script type="math/tex">d_{ij}=\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid}</script>称为<strong>绝对值距离</strong>；</li>
  <li><script type="math/tex">q=\infty</script>时，<script type="math/tex">d_{ij}=\max_{1 \leqslant k \leqslant p} {\mid x_{ik} - x_{jk} \mid}</script>称为<strong>切比雪夫距离</strong>。</li>
</ul>

<p>Minkowski距离存在一个问题，就是当变量的单位不同或者测量值范围相差很大时，直接使用Minkowski距离效果不佳。这个时候，应该先对数据进行<strong>标准化</strong>（就是减去均值除上标准差）之后再计算距离(这个后面还会说到)。</p>

<hr />

<h6 id="lancelance-and-williams">(2). Lance距离(Lance and Williams)</h6>

<p>当$x_{ji} &gt; 0$时，定义第i个样品到第j个样品的距离为：</p>

<script type="math/tex; mode=display"> d_{ij} = \sum_{k=1}^{p} {\frac{\mid x_{ik} - x_{jk} \mid}{x_{ik} + x_{jk}}} </script>

<p>从公式就可以看出来，这个距离与变量之间的单位没有什么关系；而且其对异常值也不敏感，因而适用于一些高度偏斜的数据。</p>

<hr />

<h6 id="mahalanobis">(3). Mahalanobis距离(马氏距离)</h6>

<p>上面的两种距离都没有考虑变量之间的相关性问题，马氏距离就可以考虑到这个问题。但是由于马氏距离定义的问题，在聚类分析中使用马氏距离并不合适。但是这里也还是给出马氏距离的定义：</p>

<script type="math/tex; mode=display"> d_{ij} = \sqrt{(x_i - x_j)^TS^{-1}(x_i - x_j)} </script>

<p>其中<script type="math/tex">x_i = (x_{i1}, \dots, x_{ip})^T</script>，<script type="math/tex">x_j = (x_{j1}, \dots, x_{jp})^T</script>，<script type="math/tex">S</script>为样本协方差阵。</p>

<p><strong><em>注：</em></strong>为什么说马氏距离不适用与聚类分析呢？</p>

<p>聚类分析是无监督算法中的一种，无监督算法是什么？无监督算法是没有先验信息的，所有的数据拿过来是没有什么目标信息啊什么的。没有不同类之间的先验信息，那么协方差阵<script type="math/tex">S</script>就无法计算。因而，在实际聚类分析中，马氏距离并不适用。</p>

<hr />

<h6 id="section-1">(4). 斜交空间距离</h6>

<script type="math/tex; mode=display"> d_{ij} = [ \frac{1}{p^2} \sum_{k=1}^{p} \sum_{l=1}^{p} (x_{ik} -x_{jk})(x_{il} - x_{jl})r_{kl} ] ^ {1/2} </script>

<p>其中<script type="math/tex">r_{kl}</script>是变量<script type="math/tex">x_k</script>与变量<script type="math/tex">x_l</script>的相关系数。学过高等代数的应该可以很容易看明白这个定义。此外，当变量之间互不相关的时候，这里的<script type="math/tex">d_{ij} = [d_{ij}(2)/p]_{Minkowski}</script>，也就是退化到了欧氏距离（相差一个常数倍）。</p>

<hr />

<h5 id="b-">b. 相似系数</h5>
<hr />

<p>对变量进行聚类时，通常使用相似系数来考量其间的相似度。那么相似系数的定义有需要满足哪些条件呢？</p>

<ol>
  <li>完全相关。即：$c_{ij} = \pm 1$，当且仅当$x_i = ax_j + b;a(\neq 0),b$是常数；</li>
  <li><script type="math/tex">\mid c_{ij} \mid \leqslant 1, \forall i, j </script>；</li>
  <li><script type="math/tex">c_{ij} = c_{ji}, \forall i, j </script>。</li>
</ol>

<p>下面看看常用的两种相似系数：</p>

<hr />

<h6 id="section-2">(1). 夹角余弦</h6>
<hr />

<p>变量$x_i$和$x_j$的夹角余弦的定义为：</p>

<script type="math/tex; mode=display"> c_{ij} = \frac{\sum_{k=1}^{n} {x_{ki}x_{kj}} }{ [ (\sum_{k=1}^{n}{ x^2_{ki} })(\sum_{k=1}^{n} {x^2_{kj}} ) ]^{1/2} } </script>

<p>学过解析几何应该很容易看出这个定义的含义所在，其实<script type="math/tex">c_{ij} = \cos \theta_{ij}</script>。</p>

<hr />

<h6 id="section-3">(2). 相关系数</h6>
<hr />

<script type="math/tex; mode=display"> c_{ij} = \frac { \sum _{ k=1 }^{ n }{ ({ x }_{ ki }-\overline{x_i})({ x }_{ kj }-\overline{x_j}) }  }{ \{ [\sum_{k=1}^{n}({ x }_{ ki }-\overline{x_i})^2][\sum_{k=1}^{n}({ x }_{ kj }-\overline{x_j})^2] \}^{1/2} } </script>

<p>这里的相关系数其实就是统计里面通常所说的相关系数。其实，如果变量都是标准化了的，那么夹角余弦就是相关系数，看出来了吗？</p>

<hr />

<h4 id="section-4">小节</h4>
<hr />

<p>到这边，就把统计中常用的用于度量相似性的定义讲了一些。这些定义，大都有其自身的数学背景。有些来自于几何学，有些来自于线性空间理论。对于使用者来说，搞明白什么时候选择什么样的度量方式更加重要！下一篇，我们讲一讲聚类分析中的一个常用方法：<strong>系统聚类法</strong>。</p>
]]></content>
  </entry>
  
</feed>
