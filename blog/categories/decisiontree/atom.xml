<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: DecisionTree | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/decisiontree/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-05-09T17:32:13+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习8: 决策树之ID3]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/14/id3/"/>
    <updated>2014-04-14T16:02:45+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/14/id3</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical23.jpg" alt="artical 23" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>ID3算法的核心问题就在于：如何选取在决策树的每个节点处要测试的属性。那么如何去选择呢？当然，我们要选取<strong>分类能力最好的属性</strong>，那么怎么去确定哪个属性是分类能力最好的呢？ID3算法中，使用<strong>信息增益</strong>作为评判标准。在看信息增益之前，我们先看看这个决策树的构造过程：</p>

<h3 id="section">一、构造过程</h3>
<hr />

<ol>
  <li>选取<strong>分类能力最好的属性</strong>作为决策树根部节点的测试；</li>
  <li>为根节点属性的每一个可能值产生一个分支；</li>
  <li>以各个分支节点为根节点，重复上述过程。</li>
</ol>

<h3 id="section-1">二、信息增益</h3>
<hr />

<h4 id="section-2">1. 熵</h4>
<hr />

<p>在看信息增益之前，首先需要介绍一个概念，那就是<strong>香农熵</strong>，简称为<strong>熵</strong>。相信学过物理的应该大都听过这个名词，在热力学中不就有个熵增原理嘛。其实，<strong>熵是信息论中广泛使用的一个度量标准，刻画了任意样例集合的纯度。</strong></p>

<p><strong>熵是信息的期望值</strong>，所以可以用熵来刻画一个数据集的纯度。若用$x_i,i=1,2,\dots,n$来表示数据集所包含的属性，那么这个数据集的熵为：</p>

<script type="math/tex; mode=display"> H = - \sum_{i=1}^{n}{p(x_i)l(x_i)} </script>

<p>其中，$p(x_i)$表示选取$x_i$作为分类的最终类别的概率；$l(x_i)$为$x_i$的信息，定义为：<script type="math/tex"> l(x_i) = - \log_2p(x_i)</script>。</p>

<h4 id="section-3">2. 信息增益</h4>
<hr />

<p>有了熵之后就可以刻画一个数据集的纯度，也就是熵值。那么什么信息增益呢？</p>

<p>简单来说，<strong>一个属性的信息增益就是：使用这个属性分割样例集合而导致的熵值降低</strong>。那么要选取分类能力最好的属性，就是要选取使得信息增益最大的那个属性。</p>

<p>一个属性A对样例集合S的信息增益定义为：</p>

<script type="math/tex; mode=display"> Gain(S, A) = H(S) - \sum_{v \in A} { \frac{\# S_v}{\# S} H(S_v) } </script>

<p>其中，<script type="math/tex">S_v</script>表示集合S中，属性A取值为$v$的那部分数据；<script type="math/tex">\# S_v</script>表示，集合S中，属性A取值为$v$的个数；<script type="math/tex">\# S</script>表示集合S中观测的个数。</p>

<h4 id="section-4">3. 简单的例子</h4>
<hr />

<table>
  <thead>
    <tr>
      <th style="text-align: center">序号</th>
      <th style="text-align: center">age</th>
      <th style="text-align: center">income</th>
      <th style="text-align: center">buy_iphone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">low</td>
      <td style="text-align: center">no</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: center">youth</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">yes</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: center">senior</td>
      <td style="text-align: center">high</td>
      <td style="text-align: center">no</td>
    </tr>
  </tbody>
</table>

<p>考虑上面这个问题，我们来计算一下各个属性的信息增益。</p>

<p>首先，我们可以看到，这个数据集S最终分类buy_iphone有两种取值：$yes,no$。则数据集S的熵值为：</p>

<script type="math/tex; mode=display"> H(S) = -\frac{4}{7} \log_2{\frac{4}{7} } - \frac{3}{7} \log_2{\frac{3}{7} }  \approx 0.985</script>

<p>若按照age分类，age有两个属性：$senior, youth$，分别有4个和3个。age = senior时，yes有2个，no有2个则有：</p>

<script type="math/tex; mode=display"> H_{age}(S_{senior}) = -\frac{2}{4} \log_2{\frac{2}{4}} - \frac{2}{4} \log_2{\frac{2}{4}} \approx 1 </script>

<script type="math/tex; mode=display"> H_{age}(S_{youth}) = -\frac{2}{3} \log_2 {\frac{2}{3}} - \frac{1}{3} \log_2 {\frac{1}{3}} \approx 0.918 </script>

<p>则有：</p>

<script type="math/tex; mode=display"> H_{age}(S) = \frac{4}{7} \times 1 + \frac{3}{7} \times 0.918 = 0.965 </script>

<p>则age属性的信息增益为：</p>

<script type="math/tex; mode=display"> H(S) - H_{age}(S) = 0.985 - 0.965 = 0.020 </script>

<p>那么属性income的信息增益怎么去计算，可以动手试试。</p>

<h3 id="id3">三、ID3算法的伪代码</h3>
<hr />

<h5 id="section-5">定义：</h5>
<ul>
  <li>data：为训练样本集</li>
  <li>label：为目标属性 （比如例子中的属性buy_iphone）</li>
  <li>attrs：出目标属性外，供算法学习测试使用的其它属性 （比如例子中的age和income属性）</li>
</ul>

<h5 id="section-6">伪代码：</h5>
<p>ID3(data, label, attrs)：</p>

<ol>
  <li>创建决策树的Root节点；</li>
  <li>若lable中取值单一，则返回 <code>label=label</code> 的单节点树；</li>
  <li>若attrs为空，则返回 <code>label=（data中取值最多的那个label）</code> 的单节点树；</li>
  <li>否则：
    <ol>
      <li>选取attrs中分类能力最好的属性作为Root的决策属性，记为A；</li>
      <li>对A的每一个可能取值vi：
        <ol>
          <li>在Root添加一个分支对应 <code>A = vi </code>；</li>
          <li>data_vi = data中 <code>A = vi</code> 的子集，label_vi 表示 data_vi 所对应的目标属性取值；</li>
          <li>若 data_vi 为空集：
            <ol>
              <li>在新分支下加一个叶子节点，节点 <code>label =（data中取值最多的那个label）</code> ;</li>
              <li>否则，加一个子树：ID3(data_vi, label_vi, attrs);</li>
            </ol>
          </li>
        </ol>
      </li>
    </ol>
  </li>
  <li>结束</li>
  <li>返回Root</li>
</ol>

<h3 id="r">四、R语言实现</h3>
<hr />

<p>见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习7: 决策树]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/11/decision-trees/"/>
    <updated>2014-04-11T19:00:32+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/11/decision-trees</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical21.jpg" alt="artical 21" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h3 id="section">一、开始</h3>
<hr />

<p>在介绍决策树的概念内容之前，先来初步了解一下决策树的流程。这是一个很简单的概念，通过一张简单的流程图就可以大致了解决策树是干什么的，怎么干的。</p>

<p><img src="\images\a21\decisiontrees.jpg" alt="decision trees" /></p>

<h3 id="section-1">二、相关概念</h3>
<hr />

<h4 id="section-2">1. 一些概念</h4>
<hr />

<ul>
  <li>决策树学习是一种逼近离散值目标函数的方法。</li>
  <li>决策树通过把实例从根节点排列到某个叶子节点来分类实例，叶子的节点即为实例所属的分类。</li>
  <li>决策树上的每一个节点，指定了对实例的某一个属性的测试，并且，该节点的每一个后续分支对应该属性的一个可能值。</li>
</ul>

<h4 id="section-3">2. 分类方法</h4>
<hr />
<p>从树的根节点开始，测试这个节点指定的属性，然后按照给定实例的该属性值对应的分支向下移动。然后以新节点作为根节点重复上面的过程直至结束。</p>

<h3 id="section-4">三、 评价</h3>
<hr />

<p>通过决策树的流程，可以发现决策树的计算复杂度不高，而且其输出的结果易于理解，并且对缺失值不敏感。</p>

<p>但是，正是由于其划分过于细致，可能会导致过度匹配问题(与回归中的overfitting类似)。</p>

<h3 id="section-5">四、主要的决策树算法</h3>
<hr />

<p>从决策树的流程可以看出，<strong>如何选择属性作为节点以测试实例</strong>是最为关键的一步。不同的算法采取了不同的方法，主要的决策树算法有这样几个：</p>

<ul>
  <li>ID3</li>
  <li>C4.5 （数据挖掘十大算法之一，也是ID3算法的改进）</li>
  <li>C5.0 （C4.5的改进，适用于处理大数据集，采用Boosting方式提高模型准确率，因而又称BoostingTrees。）</li>
  <li>CART（数据挖掘十大算法之一）</li>
</ul>

<p>下一篇就开始讲讲一些决策树的算法。</p>
]]></content>
  </entry>
  
</feed>
