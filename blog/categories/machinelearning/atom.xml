<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MachineLearning | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-08-18T17:10:00+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习15: 主成分分析]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/11/principal-components/"/>
    <updated>2014-05-11T11:35:10+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/11/principal-components</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article33.jpg" alt="article 33" />
<!-- more --></p>

<p>图片为：本文实例数据得到的，前两个主成分的散点图！</p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在之前<a href="http://jackycode.github.io/blog/2014/04/02/linear-regression3/">线性回归3</a>提到多重共线性问题，当时说了一些解决这个问题的办法，其中一种就是今天要说的<strong>主成分分析</strong>。</p>

<p><strong>主成分分析</strong>，Principal Components Analysis，简称PCA，是变量选择的一种方法。其一般的目的就是：变量的降维和主成分的解释！当主成分用于聚类或者回归，这个时候就是在做变量的降维；而当用来分析变量，尤其是使用前两个主成分进行散点图的绘制时，此时就是在对变量利用主成分做出一些解释。在了解主成分分析的原理之后，相信对这两个目的就可以很容易直观的理解了！</p>

<hr />

<h4 id="section">一、原理初窥</h4>
<hr />

<p>在用数学公式和概率统计知识推导其原理之前，不妨先直观地看看主成分分析到底是要干嘛，以及大致是怎么干的！</p>

<p>我们就用两个变量来说这个问题，变量分别记作<script type="math/tex">x_1,x_2</script>。那么，我们先画个散点图吧，也许它们的散点图是这个样子的：</p>

<p><img src="/images/a33/images1.jpg" alt="" /></p>

<p>我们可以看到，这两个变量明显呈现一种线性关系，如果在做线性回归时，将这两个变量都用作自变量，然后对某一个因变量进行线性拟合，那必定会存在一些问题。那么主成分分析是要做什么呢？其实主成分分析就是要寻找变量<script type="math/tex">y_1,y_2</script>去替代<script type="math/tex">x_1,x_2</script>，而且满足<script type="math/tex">y_1,y_2</script>几乎不相关，同时<script type="math/tex">y_1,y_2</script>能够保留<script type="math/tex">x_1,x_2</script>所包含的信息。</p>

<p>那么主成分分析是如何做的呢？这个时候就需要考虑旋转坐标轴，当我们像下图那样旋转过坐标轴之后，上面提出的要求就得到了实现。</p>

<p><img src="/images/a33/images2.jpg" alt="" /></p>

<p>从这张图我们就可以看出，数据投影到<script type="math/tex">y_1,y_2</script>两轴后，数据基本不相关，而且在<script type="math/tex">y_1</script>轴就保留了原本数据的大部分信息，<script type="math/tex">y_2</script>保留了数据的另外一部分信息。由此可见，数据越是集中在<script type="math/tex">y_1</script>轴两侧，数据映射到<script type="math/tex">y_1</script>轴后保留的信息就越多，而<script type="math/tex">y_2</script>就越少。当<script type="math/tex">y_2</script>含有的信息非常少，少到接近于0时，那么此时就到达了变量选择的目的，因为此时只要保留<script type="math/tex">y_1</script>就可以了。</p>

<hr />

<h4 id="section-1">二、原理</h4>
<hr />

<p>我们将数据映射到<script type="math/tex">y_1,y_2</script>轴，其实就是将原数据做个线性变换。利用上面的内容举个简单地例子就是：</p>

<p><img align="center" src="http://jackycode.github.io/images/a33/eq1.jpg" /></p>

<p>其中的系数满足：<script type="math/tex">a_{11}^2 + a_{21}^2=1,a_{12}^2 + a_{22}^2=1</script>。这样就可以成功地将数据投影到<script type="math/tex">y_1,y_2</script>轴。这里我们考虑更一般的情况，考虑<script type="math/tex">p</script>维的情况：</p>

<p><img align="center" src="http://jackycode.github.io/images/a33/eq2.jpg" /></p>

<p>其中<script type="math/tex">a_i=(a_{1i},a_{2i},\dots,a_{pi})', i=1,2,\dots,p</script>，且满足<script type="math/tex">a_i'a_1=1</script>。</p>

<p>下面要考虑的就是，如何选择<script type="math/tex">a_1</script>，使得<script type="math/tex">V(y_1)</script>到达最大，找到之后，<script type="math/tex">y_1</script>就是<strong>第一主成分</strong>。</p>

<p>首先，<script type="math/tex">V(y_1)=a_1' \Sigma a_1</script>，其中<script type="math/tex">\Sigma=V(x)</script>为协方差矩阵。我们知道<script type="math/tex">\Sigma</script>是非负定的，那么其所有的特征值必定都是大于等于0的，我们可以排个序：<script type="math/tex">\lambda _{ 1 }\geqslant \lambda _{ 2 }\geqslant \dots \geqslant \lambda_p \geqslant 0</script>，其对应的特征向量记为：<script type="math/tex">t_1, t_2, \dots, t_p</script>，显然这些特征向量是相互正交的。记<script type="math/tex">T=(t_1, t_2, \dots, t_p)=(t_{ik}), \Lambda=diag(\lambda_1,\lambda_2,\dots,\lambda_p)</script>。</p>

<p>那么根据谱分解就有：<script type="math/tex">\Sigma=T \Lambda T'=\sum_{i=1}^{p}\lambda_it_it_i'</script>。那么带入到<script type="math/tex">V(y_1)</script>中就有：</p>

<script type="math/tex; mode=display">V(y_1)=\sum_{i=1}^{p} \lambda_i a_1' t_it_i' a_1 = \sum_{i=1}^{p} \lambda_i(a_1't_i)^2</script>

<p>由于特征值中<script type="math/tex">\lambda_1</script>是最大的，那么就有：</p>

<script type="math/tex; mode=display">V(y_1) \leqslant \lambda_1 \sum_{i=1}^{p} (a_1't_i)^2 = \lambda_1 \sum_{i=1}^{p} {a_1't_it_i'a_1} = \lambda_1 a_1' TT'a_1 = \lambda_1</script>

<p>可以看到最终的结果是：<script type="math/tex">V(y_1) \leqslant \lambda_1</script>。那么什么时候取等号呢？取<script type="math/tex">a_1 = t_1</script>，则有：</p>

<script type="math/tex; mode=display">t_1'\Sigma t_1 = t_1' (\lambda_1t_1) = \lambda_1</script>

<p>到止为止，就可以看到。当<script type="math/tex">y_1=t_1'x</script>时就有其<script type="math/tex">V(y_1)</script>达到最大，为<script type="math/tex">\lambda_1</script>。那么此时<script type="math/tex">y_1=t_1'x</script>就是该数据第一主成分。</p>

<p>同理可以求解第二主成分直至最后。<strong>但是，在求解第二第三主成分的时候，需要注意一个新的问题：主成分之间不相关，即<script type="math/tex">Cov(y_i,y_k)=0,i \neq k</script></strong>，如何证明非常简单，我就不说了，自己动动手吧。</p>

<hr />

<h4 id="section-2">三、具体实例</h4>
<hr />

<p>我们使用R语言来做个小例子：</p>

<p>``` r
# 构造一个数据集
set.seed(10)
x1 &lt;- seq(1, 50, 2) + rnorm(25, mean=1, sd=1)
x2 &lt;- 2*x1 + rnorm(25, mean=1, sd=1)
x3 &lt;- x1/2 + x2 + rnorm(25, mean=1, sd=1)
x &lt;- data.frame(x1=x1, x2=x2, x3=x3)</p>

<h1 id="section-3">计算协方差矩阵</h1>
<p>Sig &lt;- cov(x)</p>

<h1 id="section-4">计算特征值和特征向量</h1>
<p>eigen(Sig)
```
得到结果如下：</p>

<p>``` r
$values
[1] 2381.3724526    0.3681085    0.1165201</p>

<p>$vectors
           [,1]       [,2]        [,3]
[1,] -0.2982259 -0.4601189  0.83627265
[2,] -0.6002191 -0.5908325 -0.53912331
[3,] -0.7421579  0.6627273  0.09997061
```</p>

<p>得到了特征值为：<script type="math/tex">\lambda_1=238.372, \lambda_2=0.368, \lambda_3=0.11652</script>，可以看到第一主成分<script type="math/tex">y_1=t_1'x=-0.298x_1-0.600x_2-0.742x_3</script>的特征值<script type="math/tex">\lambda_1</script>的值远大于其余的(由于数据构造时就是以<script type="math/tex">x_1</script>为底的)。说明第一主成分能够解释数据的大部分信息，那么如何衡量呢？</p>

<p>这时候就需要使用到<strong>贡献率</strong>这个概念，某一个主成分<script type="math/tex">y_i</script>的贡献率定义为：<script type="math/tex">\frac{\lambda_i}{\sum_{i=1}^{p} \lambda_i}</script>。</p>

<p>贡献率越大说明这个主成分能够解释数据的信息就越多，在具体的问题中，还常常用到一个概念，就是<strong>累积贡献率</strong>。前<script type="math/tex">k</script>个主成分的累计贡献率就是：<script type="math/tex">\frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{p} \lambda_i}</script>。实际中，当前<script type="math/tex">k</script>个主成分的累积贡献率达到某个临界值，比如<script type="math/tex">80\%</script>，就选择前<script type="math/tex">k</script>个主成分进行下一步操作(比如说聚类，回归或者单纯地做分析等等)</p>

<h4 id="section-5">四、基于相关矩阵</h4>
<hr />

<p>考虑这样两种情况：各个变量的单位不全相同，也就是数据的量纲不同；各变量之间的单位相同，但是变量的方差较大，也就是数值大小相差较大。那么这个时候，如果从协方差矩阵出发求解主成分，就显得不大合适了。</p>

<p>在之前的文章中提到过，当所有的变量都进行了标准化之后，协方差矩阵<script type="math/tex">\Sigma</script>就转换成了相关矩阵<script type="math/tex">R</script>！</p>

<p>那么，剩下的求解过程就与上面相同了，这里不再叙述。需要指出的是，标准化与否，所得到的结果可能会有很大的不同，所以，判断一批数据是否需要标准化是很有必要的！</p>

<h4 id="r">五、R语言实现</h4>
<hr />

<p>主成分分析的R语言实现比较简单，可以直接使用<code>eigen()</code>函数求出特征值特征向量。当然也有自带的函数：<code>princomp()</code>以及<code>psych</code>包中的<code>principal()</code>函数，可以自己查找一下帮助文档，这里就不做介绍了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习14: 关联分析之apriori算法]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/07/apriori/"/>
    <updated>2014-05-07T17:00:48+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/07/apriori</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article31.jpg" alt="article 31" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在上一篇中，我们介绍了关联分析相关的概念，这一节来看看如何使用Apriori算法去寻找满足条件的项集。</p>

<p>首先回顾一个概念，<strong>一个项集的支持度</strong>就是数据集中该项集所占的比例。Apriori算法就是用于寻找数据集中，支持度和可信度超过某一给定值的项集和关联规则。</p>

<hr />

<h4 id="section">一、原理</h4>
<hr />

<p>在介绍算法之前，首先了解一个集合论中的性质定理：集合的向下封闭性。</p>

<p>我们通过一个例子来看看这个定理，见下图：</p>

<p><img src="/images/a31/apriori_1.jpg" alt="" /></p>

<p>集合的向下封闭性，在这边解释的话，也就是说，<strong>如果一个项集的支持度低于某一个值，那么该项集超集的支持度也必定低于这个值。如果一个项集的支持度高于某一个值，那么该项集子集的支持度也必定高于某一个值。</strong></p>

<p><strong>超集</strong>：就是指包含这个集合中所以元素的集合（不包括自身），比如集合ABC就是集合AB的超集。</p>

<p>那么这个定理放在上一个图当中，就有这样的含义：</p>

<p><img src="/images/a31/apriori_2.jpg" alt="" /></p>

<hr />

<h4 id="section-1">二、算法构成</h4>
<hr />

<p>有了上面这个原理，那么就可以利用这个原理去减少我们寻找频繁集的计算量。因为，只要我们找到一个项集，其支持度低于给定的值，那么这个项集的所有超集就可以直接忽略不计了。如上图，项集A的支持度低于指定的值，那么其超集就都不用再考虑了。</p>

<p><strong>Apriori算法由两部分构成：</strong></p>

<ol>
  <li>找到满足最小支持度的项集；</li>
  <li>找到可信度超过最小可信度的关联规则。</li>
</ol>

<p>下面，我们一个一个地解决：</p>

<hr />

<h5 id="section-2">2.1. 寻找频繁项集</h5>
<hr />

<p>利用上面所讲的原理，我们来整理一下这个步骤的流程：</p>

<ol>
  <li>从数据集中构造集合C1，C1满足：大小为1的所有候选项集的集合，例如上图中的：C1 = {A, B, C};</li>
  <li>计算C1中所有项集（单元素项集）是否满足最小支持度，满足的项集构成集合L1，例如上图中的：L1 = {B, C};</li>
  <li>利用L1生成新的候选项集C2，C2满足：大小为2的所有候选项集的集合，例如上图中得：C2 = {BC};</li>
  <li>计算C2中所有项集（双元素项集）是否满足最小支持度，满足的项集构成集合L2；</li>
  <li>重复直到Lk中得元素个数为1。</li>
</ol>

<hr />

<h5 id="section-3">2.2. 寻找关联规则</h5>
<hr />

<p>在得到频繁项集之后，要寻找关联规则就容易多了。可以直接从频繁项集中构造初始的关联规则，计算该关联规则的可信度，然后与给定的最小可信度作比较，若值大于最小可信度，则记录该关联规则。</p>

<p>在实际编程时，需要注意使用<strong>集合的向下封闭性</strong>！！！想想看，在关联规则中，这个性质应该怎样去实现？(可以到Machine Learning in Action中找答案！)</p>

<h4 id="r">三、R语言实现</h4>

<h5 id="section-4">1. 使用自带的程序</h5>

<p>在R语言的<code>arules</code>这个包里面，提供了一个实现Apriori算法的函数：<code>apriori()</code>：</p>

<p><code>r
# 构造数据集
dataSet &lt;- matrix(0, 5, 3)
rownames(dataSet) &lt;- paste("item", 1:5, sep='')
colnames(dataSet) &lt;- c("A", "B", "C")
dataSet[1,] &lt;- c(1, 1, 0)
dataSet[2,] &lt;- c(1, 0, 1)
dataSet[3,] &lt;- c(1, 0, 1)
dataSet[4,] &lt;- c(1, 1, 1)
dataSet[5,] &lt;- c(0, 1, 1)
dataSet
# 转换数据格式(可以?apriori查看数据格式)
dataSet_class &lt;- as(dataSet,"transactions")
# 构造频繁项集
rules&lt;-apriori(dataSet_class,parameter=list(supp=0.5,conf=0.6,target="rules"))
# 查看结果
summary(rules)
# 构造关联规则
inspect(rules)
</code></p>

<h5 id="section-5">2. 自定义函数解决</h5>

<p>相对而言，Apriori算法的函数比较难以编写，原因可想而知，肯定是因为数据结构的问题！但是也只是比其他函数难编一点，毕竟其自带的数据结构功能还是非常强大的。我在<a href="/datascience">我的项目</a>中给出的一种编写方式，是利用R语言的list来实现的。不过，我想，利用Matrix或者data.frame，当然如果你还懂<code>data.table</code>的话，那么肯定也是可以编写的，而且我想应该会比用list简单！(没有亲手编写，只是猜想！)</p>

<p>详见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习13: 关联分析]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis/"/>
    <updated>2014-05-04T10:12:50+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article29.jpg" alt="article 29" />
<!-- more --></p>

<p>标题图片出处：<a href="http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter002/section003/blue/page002.html">Rule Generation</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">一、简介</h4>
<hr />

<p>在统计学中，变量与变量之间的关系是统计结构的重要参数，统计的核心问题也就是研究变量与变量之间的关系。如果变量与变量之间不独立，那么这两个变量之间肯定存在一定的关联性，那么如何处理度量这个关联性，在统计中就称为关联分析。</p>

<p>列联表是传统统计学中度量两个分类变量之间关系强弱的方法，但是这个方法是对于两个固定的变量进行的一种测量。在机器学习中，常常会遇到大规模的变量，这时候更倾向于从众多的变量中最快地找到关联性最强的两组或多组变量。那么使用列联表就显得不太合适了，此时应该使用什么方法呢？</p>

<hr />

<h4 id="section-1">二、关联规则</h4>
<hr />

<p>首先介绍2个定义：</p>

<ol>
  <li><strong>k项集：</strong>设<script type="math/tex">I = \{ i_1, i_2, \dots, i_m \}</script>是$m$个待研究的项构成的有限项集，对于给定的事物数据表<script type="math/tex">T = \{ T_1, T_2, \dots, T_n \}</script>，其中任意的$T_i$是$I$中的$k$项组成的集合，称之为<strong>k项集</strong>。</li>
  <li><strong>关联规则</strong>：形如<script type="math/tex"> X \rightarrow Y</script>的形式，其中<script type="math/tex"> X \subseteq I </script>, <script type="math/tex">Y \subseteq I</script>，且有<script type="math/tex"> X \bigcap Y = \emptyset </script>。</li>
</ol>

<hr />

<h5 id="section-2">度量方式</h5>
<hr />

<p>对于一个项集，我们正常用支持度来度量它的频繁程度，其实就是其在数据集中出现的比例，这个很容易理解，就不多说了。</p>

<p>那么下面要讨论的就是：<strong>如何度量一个关联规则</strong>。一般使用下面两个概念：</p>

<ol>
  <li><strong>支持度S</strong>：定义为X和Y同时出现在一个事务中得可能性，即：
 <script type="math/tex"> S(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T \mid </script>
 其中，<script type="math/tex">\mid T(X \vee Y) \mid</script>表示同时包含X和Y的事务数，<script type="math/tex">\mid T \mid</script>表示总事务数。</li>
  <li><strong>支持度C</strong>：定义为出现在关联规则前项中得事务中出现关联规则后项的比例，即：
 <script type="math/tex"> C(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T(X) \mid </script>
 其中，<script type="math/tex"> \mid T(X) \mid</script>表示包含X的事务数。</li>
</ol>

<hr />

<h5 id="section-3">例子</h5>

<hr />

<p>这边举个简单地例子，方便理解上面的概念。比如设计一个购物数据：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">id</th>
      <th style="text-align: center">items</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">AB</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">AC</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">C</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">ABC</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">BC</td>
    </tr>
  </tbody>
</table>

<p>上面数据的意思，比如第一行就表示，id为1的人购买了A和B两个物品，其他的意思类似。那么这个AB就是一个2项集，因为存在ABC三种物品，这边就有6种关联规则：<script type="math/tex">A \Rightarrow B</script>, <script type="math/tex">A \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow A </script>, <script type="math/tex"> C \Rightarrow A</script>, <script type="math/tex"> C \Rightarrow B </script>。</p>

<p>那么如何计算一个关联规则的支持度与可信度呢？</p>

<p>试试关联规则<script type="math/tex">A \Rightarrow B</script>：</p>

<script type="math/tex; mode=display"> S(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T \mid = 2/5 </script>

<script type="math/tex; mode=display"> C(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T(A) \mid = 2/3 </script>

<p>是不是很简单呢。试试计算一下其他的关联规则，多算几次就能够很了解其中的含义了。</p>

<hr />

<h5 id="section-4">关联规则的作用</h5>
<hr />

<ol>
  <li>
    <p>关联规则的目的在于，找到变量之间支持度和可信度都比较高的那些关联规则。</p>
  </li>
  <li>
    <p>关联规则的支持度，用于度量关联规则在数据库中得普适程度，是对关联规则重要性(适用性)的一种度量。</p>
  </li>
  <li>
    <p>关联规则的可信度，这是一个相对指标，是对关联规则准确度的一个度量。值越高，代表这个关联规则的后项依赖前项的可能性比较高。</p>
  </li>
</ol>

<hr />

<h4 id="section-5">小节</h4>
<hr />

<p>这篇简单介绍了一下关联分析的一些概念问题，以及如何度量一个关联规则。但是，试想一想，要是$I$中包含的项很多，事物数据表$T$也很大，那么要计算所有关联规则的支持度和可信度，难度可想而知！这时候就需要使用一些算法去解决这个问题，现在比较流行的算法就是：处理静态关联规则的Apriori算法和处理动态关联规则的Carma算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习12: Logisic回归]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/27/logistic-regression/"/>
    <updated>2014-04-27T10:34:11+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/27/logistic-regression</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article27.jpg" alt="article 27" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在数据科学系列的开头，花了三篇介绍了线性回归。线性回归模型应该是定量分析中最常用的一种统计分析方法。但是线性回归处理数据时，要求因变量是连续型变量。但是很多时候，需要处理的数据，其因变量并不是连续的。像性别、对错等等，这种离散的因变量，一般称为分类响应变量。</p>

<p>在机器学习的<a href="http://jackycode.github.io/blog/2014/03/30/data-science-an-introduction-to-machine-learning/">介绍篇</a>中，已经介绍了监督学习与非监督学习。在监督学习中，最主要的两类问题，一个就是回归，另一个就是分类。而Logistic回归就是处理二元分类的一种方法，当然其也存在自身的问题，这个后面再讲。</p>

<h4 id="sigmoid">Sigmoid函数</h4>
<hr />

<p>要了解Logistic回归，首先需要了解一下Sigmoid函数。为什么呢？</p>

<p>一般来说，我们会定义二元分类变量的输出为0和1，这种函数叫做单位阶跃函数，也称Heaviside step function。这个函数的特点就是其取值可以从0突变到1，反之也可。学过数分的话就知道，这种函数有时候会非常难以处理，因为带跳突变，导致了不可微不可导。在这里，就可以借助Sigmoid函数，因为这个函数可以近似地描述单位阶跃函数的特点。首先看看Sigmoid函数：</p>

<script type="math/tex; mode=display"> p = \frac{exp(y)}{1+exp(y)} = \frac{1}{1 + exp(-y)}</script>

<p>简单看一下这个函数，当$y=0$时，$p=0.5$；当$y$变大，趋近于无穷时，$y$趋近1；反之，$y$趋近0。而且，这种趋近的速度是非常快的。正是因为这个趋近速度非常快，我们可以使用Sigmoid函数来处理这边的单位阶跃函数。</p>

<h4 id="logistic">Logistic回归</h4>
<hr />

<h5 id="section">模型建立</h5>

<p>利用Sigmoid函数可以将单位阶跃函数做个近似，而Sigmoid函数是连续的，那么就可以利用之前的线性回归来建立模型。</p>

<p>令$ y = X\beta $，又<script type="math/tex"> p = \frac{1}{1 + exp(-y)} </script>，变形可得Logistic回归模型：</p>

<script type="math/tex; mode=display"> logit(p) = ln(\frac{p}{1-p}) = X\beta </script>

<p>上述的$logit(p)$称为$logit$变换，此时$p$就是响应变量，$X$就是自变量。到这，模型就建立好了，剩下就是如何对参数进行估计了。</p>

<h5 id="section-1">参数估计</h5>

<p>参数估计可以从两个方面来考虑，一个从最优化的方式，一个从统计角度！</p>

<p><strong>最优化的方式</strong>，是将模型转换成：<script type="math/tex"> p = sigmod(X\beta) </script>来考虑，对于估计值<script type="math/tex">\hat{\beta}</script>，应该使得其预测值<script type="math/tex">\hat{p}</script>与实际值<script type="math/tex">p</script>之间的差达到最小！即：</p>

<script type="math/tex; mode=display"> \hat{\beta} = \min_{\beta} \mid p - sigmod(X\beta) \mid</script>

<p>有了这个，我们就可以使用最速下降法等等最优化的方法去求解参数的估计值了。</p>

<p><strong>统计角度</strong>，那就需要考虑模型中<script type="math/tex">p</script>的统计意义了，这个属于广义线性模型的范畴，现在不想多说，有兴趣的话可以翻翻资料！也可以根据下面说的<strong>统计中的解释</strong>去试试，看看如何使用极大似然估计去估计参数。</p>

<h5 id="section-2">统计中的解释</h5>
<hr />

<p>这里面的$p$除了利用Sigmoid函数来解释之外，还可以利用统计中的二项分布来解释，而且从某种角度来说，这个解释会更便于理解。试想，我们这边需要处理的二元分类变量就是0和1。我们考虑0就是“不发生”，1就是“发生”，那么我们可以将前面的$p$理解成发生的概率。通过对已知数据建立模型，估计出参数，我们就可以利用模型去预测在不同的自变量条件下，“发生”的概率是多大，从而达到一个分类的目的。</p>

<p>从这边的分析就可以看到，Logistic回归的缺点：那就是欠拟合，会导致分类的精度下降。</p>

<h5 id="r">R语言实现</h5>
<hr />

<p>因为Logistic回归是属于广义线性回归模型的，在R中有专门处理广义线性模型的函数<code>glm</code>：</p>

<p><code>
model &lt;- glm(formula, family=binominal(link = "logit"), data=data.frame)
</code></p>

<p>这里处理的方式中，利用了连接函数(link=logit)，感兴趣的话可以找找广义线性模型的内容看看，当然，以后如果介绍统计模型的话，这个肯定也是必讲得内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习11: 聚类分析2]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/24/cluster-analysis2/"/>
    <updated>2014-04-24T09:39:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/24/cluster-analysis2</id>
    <content type="html"><![CDATA[<p><img src="/images/article/article26.jpg" alt="article 26" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇介绍了聚类分析的定义，给出了很多不同的相似性的度量方法。这一篇主要想介绍一下除了快速聚类之外的另外一种聚类方法：<strong>系统聚类法</strong>。</p>

<h3 id="section">二、系统聚类法</h3>
<hr />

<p>系统聚类法，hierarchical clustering method，是聚类分析方法中用的较多的一种。其具体过程如下：</p>

<ol>
  <li>对于n个样品，构造n个类，每个样品单独作为一类。计算每个类之间的距离；</li>
  <li>合并距离最近的两个类为一个新类；</li>
  <li>计算新类与其它类之间的距离，重复2直到所有类合并称为1个类为止。</li>
</ol>

<p>那么类与类之间的距离如何定义呢？</p>

<p>其实，类与类之间的距离有很多中定义方式常见的有：</p>

<ol>
  <li>最短距离法，single linkage method，<script type="math/tex"> D_{KL} = \min_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>最长距离法，complete linkage method，<script type="math/tex"> D_{KL} = \max_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>中间距离法，median linkage method，即取最远距离与最近距离两者的中间距离；</li>
  <li>类平均法，average linkage method，<script type="math/tex"> D_{KL} = \frac{1}{n_Kn_L} \sum_{i \in G_K,j \in G_L} d_{ij} </script>；</li>
  <li>重心法，centroid hierarchical method，即取类重心之间的距离；</li>
  <li>离差平方和法，Ward’s minimum variance method, 定义较为繁琐，可以自行Google；</li>
</ol>

<p><strong>注</strong>：系统聚类的方法并不困难，但是实现时会存在计算量的问题。系统聚类法一般是在样品间距离矩阵的基础上进行的，它需要计算所有点到所有点之间的距离，当样品量很大时，这个计算量会变得非常的大。因而，很多时候人们会采用动态聚类的方法去处理数据，动态聚类法中一种最常用的方法就是之前已经介绍过的KMeans方法。</p>

<h4 id="r">R语言实现</h4>

<p>在R语言中，自带了一个函数可以实现系统聚类：<code>hclust</code>。可以自己查阅help。</p>

<h3 id="section-1">三、聚类的一些问题</h3>
<hr />

<ol>
  <li>量纲问题。实际问题中，由于数据采用的量纲不同，很多时候需要对数据进行一些变换，最常用的就是标准化。但也有一些其它方式：极差变换（数据除以极差）；主成分变换（用主成分代替本身数据）；对数变换等等。</li>
  <li>kmeans算法只有在类的平均值可以被定义的情况下使用，所以在一些特殊的场合，kmeans并不适用。比如分类数据等等!!</li>
  <li>kmeans算法使用平均值作为衡量，这就造成了一个新的问题。即kmeans不适用于含有异常值的数据，非凸面的数据以及大小值相差很大的数据。</li>
  <li>聚类的一个<strong>难点</strong>在于：确定类的个数。通过上面介绍的算法来看，所有的方法都需要自己去定义类的个数。那么如何去定义类的个数呢？这是一个到现在还没有满意解决的问题。常用的方式就是观察样品散点图，查看变化率，以及使用一些假设检验的方式（感兴趣可以翻阅专业的书籍材料，比如上海财经出版社的应用多元分析中就有讲到这部分内容）。</li>
</ol>

]]></content>
  </entry>
  
</feed>
