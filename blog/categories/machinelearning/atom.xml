<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MachineLearning | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-04-05T20:47:26+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习5：分类之k-近邻算法]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/05/knn/"/>
    <updated>2014-04-05T19:59:29+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/05/knn</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical19.jpg" alt="artical 19" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">基本思想</h4>
<hr />

<p>kNN，k-Nearest Neighbor algorithm，也就这边的k-近邻算法，是数据挖掘十大算法之一，是一个比较简单的分类方法。</p>

<p>其基本的思想是：对于一个输入样本（未知分类的样本），考虑其与测试样本中与之距离最近（特征最相似）的k个样本，用这k个样本中出现最多的分类作为输入样本的分类。</p>

<h4 id="section-1">具体流程</h4>
<hr />

<p>对于输入样本中的每一个点，进行以下操作：</p>

<ol>
  <li>计算点与测试样本中点的距离；</li>
  <li>取出与当前点距离最小的k个点；</li>
  <li>确定k个点的分类，计算各个分类的频数；</li>
  <li>返回频数最高的类别，作为该输入点的预测分类。</li>
</ol>

<h4 id="section-2">距离的计算</h4>
<hr />

<p>上面一直在说，计算输入样本中点与测试样本中点之间的距离，那么这个距离应该怎么计算呢？这个距离一般就是使用欧式距离：</p>

<script type="math/tex; mode=display"> d = \sqrt{(x - y)^T(x - y)} </script>

<p>其中<script type="math/tex">x^T=[x_1, x_2,\dots,x_n], y^T=[y_1,y_2,\dots,y_n]</script>。二维的表示就是：</p>

<script type="math/tex; mode=display"> d = \sqrt{(x - y)^T(x - y)} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} </script>

<h4 id="r">R语言实现</h4>
<hr />

<p>见<a href="https://github.com/JackyCode/Data_Science/tree/master/kNN">我的github</a>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习4：线性回归3]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/02/linear-regression3/"/>
    <updated>2014-04-02T18:51:29+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/02/linear-regression3</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical18.jpg" alt="artical 18" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>这是介绍线性回归的最后一篇，首先回顾一下之前的两篇。第一篇主要就是介绍了如何去估计回归系数得到回归方程，以及在R语言中如何使用自带的函数去实现。第二篇主要介绍了对于回归方程和回归系数的显著性检验，以及给出了我自己写的一个处理线性回归的函数。</p>

<p>这一篇介绍线性回归中回归诊断的一些问题，也就是估计出回归方程，检验了回归方程的显著性以及回归系数的显著性后，对这个模型所做的进一步的诊断分析。然后对存在的问题进行探讨，选择不同的方式去解决这些问题。这大致可以分成三块：残差分析，影响分析以及共线性问题。</p>

<hr />

<h4 id="section">一、残差分析</h4>

<hr />

<h5 id="section-1">1. 残差</h5>

<hr />

<p>首先看一看残差的定义，常用的残差大致有三种：残差，标准化残差以及学生化残差：</p>

<ul>
  <li>残差：<script type="math/tex">\hat{\varepsilon} = y - \hat{y} = (1-H)y</script>,其中<script type="math/tex">H=X(X^TX)^{-1}X</script>称作帽子矩阵；</li>
  <li>标准化残差：<script type="math/tex">ZRE = \hat{\varepsilon} / \hat{\sigma}</script></li>
  <li>学生化残差：<script type="math/tex">SRE_i = \hat{\varepsilon}_i / (\hat{\sigma}\sqrt{1-h_{ii}})</script>，其中<script type="math/tex">h_{ii}</script>为帽子矩阵对角线上第$i$个元素。</li>
</ul>

<hr />

<p><strong>R语言中</strong></p>

<p>使用<code>residuals(),rstandard(),rstudent()</code>函数计算残差，标准化残差以及学生化残差。具体用法，请使用<code>help</code>函数查看。</p>

<hr />

<h5 id="section-2">2. 残差图</h5>

<hr />

<p>以残差为纵坐标，观测值、预测值活则观测时间等等作为横坐标的散点图，称为<strong>残差图</strong>。</p>

<hr />

<p><strong>R语言中</strong></p>

<p><code>r
model = lm(y~x1+x2)
y_pred = predict(model)
y_res = residuals(model)
plot(y_res ~ y_pred)
</code>
***</p>

<h5 id="section-3">3. 异方差问题</h5>

<hr />

<h6 id="a-">a. 问题的提出</h6>

<hr />

<p>在进行回归方程估计之前，一般都会假设误差的方差是齐性的。如果残差图出现类似下面的情况，则这批数据可能存在异方差问题，即方差非齐性。</p>

<ol>
  <li>随着横坐标的增大，纵坐标的值波动越来越大；</li>
  <li>随着横坐标的增大，纵坐标的值波动越来越小；</li>
  <li>随着横坐标的增大，纵坐标的值波动复杂多变，没有系统关系；</li>
</ol>

<p>大部分时候，考虑前两种就可以了。那么具体如何数值化地检验异方差呢，一般使用的是等级相关系数法，这里不做介绍(可到线性回归的书籍中寻找)。</p>

<hr />

<h6 id="b-">b. 问题的解决</h6>

<hr />

<p>一般有两种方式解决异方差问题，一种是加权最小二乘；另一种是对因变量作适当的变化。</p>

<ol>
  <li>
    <p>加权最小二乘</p>

    <p>即是将回归系数的估计转化成：<script type="math/tex">\hat{\beta}=(X^TWX)^{-1}X^TWy</script>，其中<script type="math/tex">W</script>是一个对角矩阵，用于给每一个数据点加上一个权重。一般使用“核”来对附近的点赋予较高的权重，常用的核就是高斯核，其对应的权重为：</p>

<script type="math/tex; mode=display">w_{ii}=exp(\frac{\|x^{(i)}-x\|}{-2k^2})</script>

    <p>从上式可以看出，点$x$离$x^{(i)}$越近，所得到的权重越高。</p>
  </li>
  <li>
    <p>对因变量作适当变化</p>

    <p>常用的变换有：</p>

    <ul>
      <li>$z = \sqrt{y}$</li>
      <li>$z = ln(y)$（对数变换）</li>
      <li>$z = 1/y$</li>
      <li>$z = \frac{x^{\lambda}-1}{\lambda}$(Box-Cox变换)，其中$\lambda=0$时，即是对数变换</li>
    </ul>
  </li>
</ol>

<hr />

<h5 id="section-4">4. 异常点</h5>

<hr />

<h6 id="a--1">a. 问题的提出</h6>

<hr />

<p>一般将标准化残差的绝对值大于等于2的称为可疑点；将标准化残差的绝对值大于等于3的称为异常点。</p>

<hr />

<h6 id="b--1">b. 问题的解决</h6>

<hr />

<p>一般来说，剔除异常数据即可。</p>

<hr />

<h5 id="section-5">5. 自相关问题</h5>

<hr />

<p>在作回归之前，总是会假设<script type="math/tex">cov(\varepsilon_i, \varepsilon_j)=0,\forall i \neq j</script>。但是实际情况下，可能并不满足这个假设，这就是存在了自相关问题。对于自相关问题，一般使用残差图，自相关系数以及DW检验去进行检验；而处理的方式一般是：迭代法和差分法。这里不做详细介绍，感兴趣的可以去找找相关材料。</p>

<hr />

<h4 id="section-6">二、影响分析</h4>

<hr />

<p>分析观测值对回归结果的影响，从而找出对回归结果影响较大的观测点的分析方法叫做影响分析。一般使用Cook距离去度量第$i$个观测值对回归影响大小，Cook距离的定义如下：</p>

<script type="math/tex; mode=display">D_i(M,MSE) = \frac{(\hat{\beta}_{(i)}-\hat{\beta})^TM(\hat{\beta}_{(i)}-\hat{\beta})}{MSE}</script>

<p>其中，$M$为观测数据的离差阵，$MSE$为回归模型的均方误差。一般<script type="math/tex">\|D_i\| \geqslant 4/n</script>时，称其为强影响点。</p>

<hr />

<p><strong>R语言中</strong></p>

<p>使用<code>cooks.distance()</code>函数计算Cook距离。</p>

<hr />

<h4 id="section-7">三、共线性诊断</h4>

<hr />

<p>共线性是指，在多元线性回归中，自变量之间存在线性关系或者近似线性关系。如果出现这种情况，那么在模型内部就会隐藏部分变量的显著性，也会导致参数估计的误差增大，影响模型的稳定性。</p>

<hr />

<h5 id="a--2">a. 检验方法</h5>

<hr />

<p>常用的检验方法有特征值法，条件数和方差膨胀因子（VIF）。</p>

<hr />

<h6 id="section-8">特征值法</h6>

<hr />

<p>首先介绍一个结论：当矩阵$X^TX$至少有一个特征根为0时，$X$的列向量间必存在多重共线性。</p>

<p>即可证：$X^TX$有多少个特征根接近于零，设计阵$X$就有多少个多重共线性。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>eigen()</code>函数去计算特征值和特征向量。</p>

<hr />

<h6 id="section-9">条件数</h6>

<hr />

<p>上述的特征值法中，特征根近似为0，这个标准好想并不明确。那么这边就给出一个条件数的定义：</p>

<script type="math/tex; mode=display">k_i = \frac{\lambda_m}{\lambda_i}</script>

<p>其中，<script type="math/tex">\lambda_m</script>为最大的那个特征根。一般认为，若$k_i$介于10到30之间为弱相关；在30到100之间为中等相关；超过100为强相关。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>kappa()</code>函数计算条件数。</p>

<hr />

<h6 id="vif">VIF</h6>

<hr />

<p>定义VIF为：</p>

<script type="math/tex; mode=display">VIF_i = \frac{\text{第i个回归系数的方差}}{\text{自变量不相关时第i个回归系数的方差}} = \frac{1}{1-R^2_i} = \frac{1}{TOL_i}</script>

<p>其中$TOL_i$称为容忍度；$R^2_i$为自变量$x_i$对其余自变量的复决定系数。一般认为，VIF超过10，模型就存在共线性问题。</p>

<hr />

<p><strong>在R语言中</strong></p>

<p>可以使用<code>vif()</code>函数计算VIF的值。</p>

<hr />

<h5 id="b--2">b. 多重共线性的处理</h5>

<hr />

<p>一般有这样几种处理方式：</p>

<ol>
  <li>
    <p>剔除一些不重要的解释变量</p>

    <ol>
      <li>使用变量选择的方式剔除部分变量，作回归；</li>
      <li>检验VIF，若存在共线性，删除VIF值最大的变量，作回归；</li>
      <li>再次检验VIF，若还存在共线性，再删除其中VIF值最大的那个；</li>
      <li>重复直至消除共线性。</li>
    </ol>
  </li>
  <li>
    <p>增大样本容量</p>

    <p>当变量的个数接近样本容量的数值时，自变量间容易产生多重共线性。所以增大样本容量是解决多重共线性的一种方式，但是在现实中，这种做法基本不可能。</p>
  </li>
  <li>
    <p>主成分回归</p>

    <p>这是一个比较大的主题，这里不做介绍。</p>
  </li>
  <li>
    <p>有偏估计等等。</p>
  </li>
</ol>

<hr />

<h3 id="section-10">最后</h3>

<hr />

<p>到这里，除了变量选择问题，线性回归的内容基本上就已经梳理了一遍。变量选择问题，方法简单的非常简单，难的非常难（像lasso），所以暂时还不想写这些内容。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习3：线性回归2]]></title>
    <link href="http://jackycode.github.io/blog/2014/04/01/linear-regression2/"/>
    <updated>2014-04-01T16:56:57+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/04/01/linear-regression2</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical17.jpg" alt="artical 17" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇简单介绍了线性回归中系数估计的问题，给出了一元以及多元情况下，系数估计值的表达式！此外，还给出了在R语言中如何使用自带的函数计算系数估计值。</p>

<p>这一篇，打算介绍一下线性回归中的一些显著性检验问题。这个就是完全属于统计学中的理论内容，不过依旧有着很强的实际意义。简单来说，显著性检验不能通过，说明你的线性回归的效果不好，可能你就需要选择其它的方式去处理你手头的数据，而不是选择线性回归了。</p>

<hr />

<h4 id="section">一、回归方程的显著性检验</h4>

<hr />

<p>考虑回归方程是否显著，意思就是查看自变量<script type="math/tex">x_1,x_2,\dots,x_p</script>从整体上是否对因变量<script type="math/tex">y</script>有显著的影响。则，我们可以考虑这样的假设检验问题：</p>

<script type="math/tex; mode=display"> H_0:\beta_0=\beta_1=\dots=\beta_p=0;~H_1:\beta_0,\beta_1,\dots,\beta_p\text{不全为0}</script>

<p>显然，如果原假设成立的话，自变量对因变量的影响不大，也就是用线性回归模型来解释就显得不合适了。</p>

<p>在正态假设下，原假设<script type="math/tex">H_0</script>成立时，有<script type="math/tex">F</script>检验统计量：</p>

<script type="math/tex; mode=display"> F = \frac{SSR/p}{SSE/(n-p-1)} \sim F(p,n-p-1) </script>

<p>其中<script type="math/tex">SSR=\sum_{i=1}^{n}{(\hat{y}_i-\overline{y})^2}</script>为回归平方和，<script type="math/tex">SSE=\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}</script>为残差平方和。对于给定的显著性水平$\alpha$，拒绝域为：<script type="math/tex">\{F \geqslant F_{1-\alpha}(p,n-p-1)\}</script>。</p>

<hr />

<h4 id="section-1">二、回归系数的显著性检验</h4>

<hr />

<p>显然，线性回归中很有可能就存在某个自变量对因变量的影响很小，那么它的回归系数就会接近0.因此有如下的假设检验问题：</p>

<script type="math/tex; mode=display">H_{0j}:\beta_j = 0; ~ H_{1j}:\beta_j \neq 0, ~ j=0,1,\dots,p</script>

<p>在原假设成立的条件下，$t$统计量有：</p>

<script type="math/tex; mode=display">t_j = \frac{\hat{\beta}_j}{\sqrt{c_{jj}}\hat{\sigma}} \sim t(n-p-1)</script>

<p>其中，<script type="math/tex">(c_{ij})=(X^TX)^{-1},i,j=0,1,\dots,p;~\hat{\sigma}=\sqrt{\frac{1}{n-p-1}\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}}</script>。对于给定的显著性水平$\alpha$，拒绝域为：<script type="math/tex">\{\|t_j\| \geqslant t_{\alpha/2}\}</script>。</p>

<hr />

<h4 id="r">三、R语言中的实现</h4>

<hr />

<p>在上一篇中可以看到，<code>lm</code>函数加上<code>summary</code>函数会有很多的输出内容。其实，那些输出中就含有上述的假设检验的结果，很容易就可以找到，这里不做阐述！</p>

<p>我自己也写了一个关于线性回归的R语言函数，托管在<a href="https://github.com/JackyCode/Data_Science/tree/master/Linear_Regression">我的github</a>上面，函数内部对于线性回归的过程大都涉及到了，有兴趣可以看看。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习2：线性回归1]]></title>
    <link href="http://jackycode.github.io/blog/2014/03/30/machine-learning1/"/>
    <updated>2014-03-30T19:02:30+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/03/30/machine-learning1</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical16.jpg" alt="artical 16" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">一、回归分析</h4>

<hr />

<p>在统计分析中，最大的两支应该算是相关分析和回归分析。而回归分析应该是统计学的核心。回归分析，就是研究因变量$y$与自变量$x$之间的关系，存在条件数学期望：$f(x)=E(y|x)$。此时有：$y=f(x)+\varepsilon$，一般假设$\varepsilon \sim N(0,\sigma^2)$。</p>

<p>回归分析有很多变种：简单线性回归；多项式回归；Logistic回归；非参数回归；非线性回归等等。本篇就介绍最简单的线性回归，首先来看看一元线性回归。</p>

<hr />

<h4 id="section-1">二、一元线性回归</h4>

<hr />

<p>对于一元线性回归来说，$f(x)$就是线性的，则有：$f(x)=E(y|x)=\beta_0 + \beta_1 x$。通过已知的数据，可以估计出$\beta_0,\beta_1$的估计值：$\hat{\beta}_0,\hat{\beta}_1$。那么就有$y$的预测值：$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$。</p>

<hr />

<h5 id="beta0beta1hatbeta0hatbeta1">1. 如何计算$\beta_0,\beta_1$的估计值$\hat{\beta}_0,\hat{\beta}_1$呢？</h5>

<hr />

<p>定义离差平方和：</p>

<script type="math/tex; mode=display">Q(\beta_0,\beta_1) = \sum_{i=1}^{n}(y_i-f(x_i))^2</script>

<p>显然，我们希望$f(x_i)$的值与真实值$y_i$越接近越好。那么就是需要离差平方和越小越好。则得到目标：</p>

<script type="math/tex; mode=display"> \min_{\beta_0,\beta_1}{\sum_{i=1}^{n}(y_i-f(x_i))^2} </script>

<p>如何寻找$\hat{\beta}_0,\hat{\beta}_1$使得上面方程达到最小呢？这个就需要对其对$\hat{\beta}_0,\hat{\beta}_1$求偏导，得到：</p>

<script type="math/tex; mode=display">\frac{\partial Q}{\partial \beta_0}=-2\sum_{i=1}^{n}{(y_i-\beta_0-\beta_1x_i)}</script>

<script type="math/tex; mode=display">\frac{\partial Q}{\partial \beta_1}=-2\sum_{i=1}^{n}{(y_i-\beta_0-\beta_1x_i)x_i}</script>

<p>令上述两式都等于0，计算得到：</p>

<script type="math/tex; mode=display">\hat{\beta}_0=\overline{y}-\hat{\beta}_1\overline{x}</script>

<script type="math/tex; mode=display">\hat{\beta}_1=\frac{\sum_{i=1}^{n}{(x_i-\overline{x})(y_i-\overline{x})}}{\sum_{i=1}^{n}{(x-\overline{x})^2}}</script>

<p>这样就得到$\beta_0,\beta_1$的估计值$\hat{\beta}_0,\hat{\beta}_1$。这个方法就叫做OLS，即普通最小二乘(ordinary least squares)。</p>

<hr />

<h5 id="r">2. R语言实现</h5>

<hr />

<p>在R语言中有自带的函数可以处理线性回归，那就是<code>lm</code>函数。这里使用自带的数据<code>cars</code>做演示：</p>

<p>``` r
&gt; attach(cars) # 使用数据集cars，与with函数类似
&gt; lingre &lt;- lm(dist ~ speed)
&gt; summary(lingre)</p>

<p>Call:
lm(formula = dist ~ speed)</p>

<p>Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 </p>

<p>Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  <br />
(Intercept) -17.5791     6.7584  -2.601   0.0123 *<br />
speed         3.9324     0.4155   9.464 1.49e-12 <strong>*
—
Signif. codes:  0 ‘</strong><em>’ 0.001 ‘**’ 0.01 ‘</em>’ 0.05 ‘.’ 0.1 ‘ ’ 1</p>

<p>Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12</p>

<blockquote>
  <p>plot(dist ~ speed, pch=4) # 画出散点图
abline(lingre, col=’red’) # 添加拟合直线
detach(cars) # 使用完记得释放
```</p>
</blockquote>

<p>从这里可以得到回归方程：$\hat{dist} = -17.5791 + 3.9324 \times speed$。（对于其它的结果是什么意思，可以去查看线性回归的相关书籍）</p>

<p>另外，得到拟合直线的图像：</p>

<p><img src="\images\a16\lingre_one.jpg" alt="lingre_one" /></p>

<hr />

<h4 id="section-2">三、多元线性回归</h4>

<hr />

<p>对于多元线性回归来说，其计算方式与一元线性回归类似，区别在于，多元的时候需要利用矩阵来处理。首先看一下回归模型：</p>

<script type="math/tex; mode=display"> y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon </script>

<p>其中$p$代表自变量的个数。</p>

<p>若取$x^T_0=[1, 1, \dots, 1]_{1 \times n}$，则可将上述模型改写成：<script type="math/tex">y=X\beta+\varepsilon</script>。其中：</p>

<script type="math/tex; mode=display">y^T=[y_1,y_2,\dots,y_n], X=[x_0,x_1,\dots,x_p], \beta^T=[\beta_0,\beta_1,\dots,\beta_p], \varepsilon^T=[\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n]</script>

<p>其中<script type="math/tex">x^T_i=[x_{1i},x_{2i},\dots,x_{ni}]</script>。</p>

<p>这样我们就可以将离差平方和<script type="math/tex">\sum_{i=1}^{n}{(y_i-\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p)^2}</script>写成矩阵形式：</p>

<script type="math/tex; mode=display">(y-X\beta)^T(y-X\beta)</script>

<p>求导可得：<script type="math/tex">-2X^T(y-X\beta)</script>(这里用到矩阵求导的知识，一般介绍<strong>线性模型</strong>的书籍中会讲到；当然也可以直接对上面不是矩阵形式的离差平和求导)。令其等于0，可得：</p>

<script type="math/tex; mode=display">\hat{\beta} = (X^TX)^{-1}X^Ty</script>

<hr />

<h5 id="r-1">R语言实现</h5>

<hr />

<p>对于R语言的实现，依旧使用<code>lm</code>函数：</p>

<p><code>r
lingre_mul &lt;- lm(y ~ x1 + x2, data=datasets)
summary(lingre_mul)
</code></p>

<p>这里就不再用实际数据去演示了。</p>

<hr />

<h4 id="section-3">四、最后</h4>

<hr />

<p>至此，就把线性回归的基础内容介绍完了。但其实线性回归还存在很多其它的问题。比如说回归诊断（就是检查回归的效果），变量选择等等等等。感兴趣的话，可以找本讲线性回归的书看看，有很多！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习1：简介]]></title>
    <link href="http://jackycode.github.io/blog/2014/03/30/data-science-an-introduction-to-machine-learning/"/>
    <updated>2014-03-30T16:39:09+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/03/30/data-science-an-introduction-to-machine-learning</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical15.jpg" alt="artical 15" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>上一篇“<a href="http://jackycode.github.io/blog/2014/03/27/an-introduction-to-data-science/">数据科学简介</a>”简单地说了一下数据科学是什么，以及它包罗的东西。这一篇打算简单介绍一下<strong>机器学习</strong>，并理一理机器学习中涉及的内容。</p>

<hr />

<h4 id="section">机器学习的定义</h4>

<hr />

<p>一般来说，教科书介绍一样东西，首先会给它下一个确切的定义。不过，对于机器学习的定义，我还真不知道该怎么去下。有太多的版本，太多的述说方式，不知道用哪个好。这里就列举一些我觉得有代表性的，讲的容易懂的那些定义。对于机器学习是什么，看看这些定义，应该就能够有个大致的了解了。</p>

<p>首先，在“<a href="http://www.amazon.cn/Machine-Learning-The-Art-and-Science-of-Algorithms-That-Make-Sense-of-Data-Flach-Peter/dp/1107422221/ref=tmm_pap_title_0">Machine Learning: the art and science of algorithms that make sense of data</a>”一书中，有这样一个定义：</p>

<blockquote>
  <p>Machine Learning is the systematic study of algorithms and systems that improve their knowledge or performance with experience.</p>
</blockquote>

<p>这个定义说的比较简单直白，就是说，机器学习就是研究如何通过经验(其实就是数据)去改进性能的算法(这个翻译学得不好，见谅！)。我们可以简单这样理解，机器学习就是研究这么一类算法，通过这类算法呢，系统可以从数据中获取信息来提升自己的性能。最简单的说法就是将数据转换成有用的信息。</p>

<p>在“<a href="http://www.amazon.cn/Machine-Learning-The-Art-and-Science-of-Algorithms-That-Make-Sense-of-Data-Flach-Peter/dp/1107422221/ref=tmm_pap_title_0">Machine Learning: the art and science of algorithms that make sense of data</a>”书中，还有这么一句话，将机器学习的流程说的很清楚：</p>

<blockquote>
  <p>Machine learning is concerned with using the right features to build the right models that achieve the right task.</p>
</blockquote>

<p>这句话就是说，机器学习关心的就是：如何通过对特征(数据)建立模型去完成一些任务。在Andrew Ng的机器学习视频中，还列出了Tom Mitchell给出的定义，与上面的说法有些相近：</p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
</blockquote>

<p><strong><em>上一篇博文里曾说，数据科学是统计、计算机科学以及专业知识的结合。而其实，机器学习就是统计与计算机科学的结合。正因如此，机器学习是数据科学的核心。</em></strong>简单来说，机器学习就是：研究让机器通过学习过去的经验，达到提高处理相同任务能力的算法。</p>

<hr />

<h4 id="section-1">机器学习的类别</h4>

<hr />

<p>机器学习大致可以分为两类：监督学习(supervised learning)和无监督学习(unsupervised learning)。</p>

<p>如何区分呢？<strong>区别就在于，测试数据有没有给出确切的类别或者结果。若给出了就是监督学习，若没有则是无监督学习</strong>。</p>

<p>比如说回归：其给定的测试样本中，一组自变量的值肯定对应一个确定的因变量的值；再说分类：对于给出的测试样本中的每一个数据，其都会有自己所属的类别。所以回归与分类就属于监督学习的范畴，也就是可以通过既定的数据去做预测。</p>

<p>而无监督学习不同，其给出的数据不存在任何其它信息，比方说聚类，给你一批数据，没有任何信息，让你把数据分成几个类别。一般来说，这时候就需要计算数据之间的相似度(距离)，然后把距离近的归位一类。</p>

<hr />

<h4 id="section-2">最后</h4>

<hr />

<p>到这里，基本上来说，机器学习是干什么的应该有了一些了解了。至于机器学习的分类，监督学习和无监督学习还不明白也不要紧。等弄明白了回归，分类以及聚类等等的是干什么的，自然就明白这两者之间的区别了。</p>
]]></content>
  </entry>
  
</feed>
