<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: MachineLearning | Jacky and MSC]]></title>
  <link href="http://jackycode.github.io/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://jackycode.github.io/"/>
  <updated>2014-06-04T17:06:03+08:00</updated>
  <id>http://jackycode.github.io/</id>
  <author>
    <name><![CDATA[Jacky Code]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习17:因子分析2]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/19/factor-analysis2/"/>
    <updated>2014-05-19T15:00:02+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/19/factor-analysis2</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical35.jpg" alt="artical 35" />
<!-- more --></p>

<p>图片来源于<a href="http://software.ssri.co.jp/statweb2/column/column0811.html">网址</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>这两天来了个同学，大家聚了聚，我也乘机休息了两天（好奢侈！）。这两天属于什么都没有写，就翻看了两本书。一本是二月河的<a href="http://www.amazon.cn/gp/product/B0032K0X6Q/ref=olp_product_details?ie=UTF8&amp;me=&amp;seller=">康熙大帝</a>，另外一本是<a href="http://www.ituring.com.cn/book/894">推荐系统实践</a>，这本书的电子版，图灵正在打折，有兴趣可以买本看看。</p>

<hr />

<p>好了，不废话了，下面就接着上一篇讲的继续！上一篇简单介绍了因子分析的一些概念，以及最基础的因子模型：<strong>正交因子模型</strong>。那么这一篇，就来说说正交因子模型的<strong>参数估计问题</strong>。</p>

<p>对于一组p维样本，有n个观测值：<script type="math/tex">x_1, x_2, \dots, x_n</script>，则其均值和协方差矩阵可以使用样本均值和样本协方差矩阵来估计：</p>

<script type="math/tex; mode=display"> \hat{\mu} = \overline{x} = \frac{1}{n} {\sum_{i=1}^{n} x_i} </script>

<script type="math/tex; mode=display"> \hat{\Sigma} = S = \frac{1}{n-1} {\sum_{i=1}^{n} (x_i-\overline{x})(x_i-\overline{x})'} </script>

<p>在因子模型中，需要估计的有两个参数：<strong>因子载荷矩阵</strong><script type="math/tex">A=(a_{ij}:p \times m)</script>以及<strong>特殊方差矩阵</strong><script type="math/tex">D = diag(\sigma_1^2, \sigma_2^2, \dots, \sigma_p^2)</script>。</p>

<hr />

<h4 id="section">1. 主成分法</h4>
<hr />

<p>主成分法的思想取自于主成分分析，即是取出前m个成分作为主成分，然后以此来得到因子载荷矩阵的估计；然后再以协方差阵和因子载荷矩阵为条件，直接推出特殊方差矩阵。具体如下：</p>

<ol>
  <li>求出协方差矩阵<script type="math/tex">S</script>的特征值：<script type="math/tex">\hat{\lambda}_1 \geqslant \hat{\lambda}_2 \geqslant \dots \geqslant \hat{\lambda}_p \geqslant 0</script>，其对应的特征向量为：<script type="math/tex">\hat{t}_1, \hat{t}_2, \dots, \hat{t}_p</script>。</li>
  <li>选一个较小因子数<script type="math/tex">m</script>，并且使得前m项累计贡献率<script type="math/tex">\frac{\sum_{i=1}^{m} \hat{\lambda}_i}{\sum_{i=1}^{p} \hat{\lambda}_i} </script>高于设定值。</li>
  <li>将协方差矩阵<script type="math/tex">S</script>做这样的近似：<script type="math/tex">S = \sum_{i=1}^{m} \hat{\lambda}_i + \sum_{i=m+1}^{p} \hat{\lambda}_i \approx \sum_{i=1}^{m} \hat{\lambda}_i + \hat{D} := \hat{A}\hat{A}' + \hat{D} </script></li>
</ol>

<p>其中，<script type="math/tex">A = (\sqrt{\hat{\lambda}_1}t_1, \sqrt{\hat{\lambda}_2}t_2, \dots, \sqrt{\hat{\lambda}_m}t_m) </script>,<script type="math/tex">\hat{D} = diag(\hat{\sigma}_1^2, \hat{\sigma}_2^2, \dots, \hat{\sigma}_p^2) </script>,<script type="math/tex">\hat{\sigma}_i^2 = s_{ii} - \sum_{j=1}^{m} \hat{a}_{ij}^2 </script></p>

<p>从上述的过程来看，我们是使用了一种近似的方法估计出了<script type="math/tex">A</script>和<script type="math/tex">D</script>，那么这就有一个<strong>残差矩阵</strong><script type="math/tex">S - (\hat{A}\hat{A}' + \hat{D}) </script>，显然这个矩阵的对角线元素为0。既然是一种近似，那么，如果这个残差矩阵的非对角线元素都非常小的时候，我们就可以认为取<script type="math/tex">m</script>个因子的模型就可以很好地解释或者是拟合原始的数据了。</p>

<hr />

<h4 id="section-1">2. 主因子法</h4>
<hr />

<p>对于因子模型，我们先对原始向量进行标准化，则有：<script type="math/tex"> R = AA' + D</script>。取<strong>约相关矩阵</strong><script type="math/tex">R^* = R - D = AA'</script>，假设特殊方差<script type="math/tex">\sigma_i^2</script>的一个估计值<script type="math/tex">\hat{\sigma}^2</script>为初始估计，则有约相关矩阵的估计值为：</p>

<p><img src="/images/a35/eq2_1.png" alt="" /></p>

<p>其中<script type="math/tex">\hat{R}</script>为样本相关矩阵，<script type="math/tex">\hat{D} = diag(\hat{\sigma^2_1},\hat{\sigma_2^2}, \dots, \hat{\sigma_p^2}) </script>, <script type="math/tex">\hat{h_i^2} = 1 - \hat{\sigma_i^2} </script>为<script type="math/tex">h_i^2</script>的初始估计。</p>

<p>计算<script type="math/tex">\hat{R}^*</script>的特征值，取足够小，但累计贡献率达到要求的m：<script type="math/tex">\hat{\lambda^*_1} \geqslant \hat{\lambda^*_2} \geqslant \dots \geqslant \hat{\lambda^*_m} > 0 </script>，其对应的特征向量为：<script type="math/tex">\hat{t^*_1}, \hat{t^*_2}, \dots, \hat{t^*_m}</script>，则得到 <script type="math/tex">A</script> 的估计值：</p>

<script type="math/tex; mode=display"> \hat{A} = (\sqrt{\hat{\lambda^*_1}}t^*_1, \sqrt{\hat{\lambda^*_2}}t^*_2, \dots, \sqrt{\hat{\lambda^*_m}}t^*_m ) </script>

<p>那么<script type="math/tex">\sigma^2_i</script>的最终估计为： <script type="math/tex"> \hat{\sigma_i^2} = 1-\hat{h_i^2} = 1 - \sum_{j=1}^{m} \hat{a_{ij}^2} </script>。</p>

<p>可以看到，这是一个可以迭代的过程，我们可以一直迭代，直到结果达到稳定为止！从过程来看，这里其实也是利用了主成分，因而，主因子法也是主成分法的一种修正！</p>

<p>那么接下来的问题就是，这个特殊方差<script type="math/tex">\sigma^2_i</script>的初始估计值应该如何取呢？最常用的取法：<script type="math/tex"> \hat{\sigma_i^2} = 1/r^{ii} </script>, 其中<script type="math/tex"> r^{ii} </script>为<script type="math/tex">\hat{R}^{-1}</script>对角线元素的第<script type="math/tex">i</script>个。</p>

<hr />

<h4 id="section-2">3. 极大似然法</h4>
<hr />

<p>使用极大似然估计，那么就肯定需要使用样本的分布，这里我们假定公共因子<script type="math/tex">f \sim N_m(0, I) </script>，特殊因子<script type="math/tex"> \varepsilon \sim N_p(0, D) </script>，并且相互独立！这里的假设其实就是来源于模型的正交性假设，只不过是将正交性假设进一步限定，假设它们都是属于多元正态分布！</p>

<p>有了上述假设，通过模型就可以知道<script type="math/tex">x \sim N_p(\mu, \Sigma) </script>，有了这个就可以计算样本的似然函数了，这里涉及到较为复杂的矩阵计算，不想多说，有兴趣的话可以查找一些资料；或者学习一下线性模型中关于矩阵求导的知识，然后自己推导一下。</p>

<p>一般情况下，极大似然法使用得并不太多，因为这个方法是算不出显式解的，在没有限制条件的情况下，解也并不唯一确定！但是如果是在因子分部可以明显知道的情况下，使用这个方法就比较好了！</p>

<hr />

<h3 id="section-3">总结</h3>
<hr />

<p>到这边，对于基础的因子模型就介绍结束了。回顾一下，主要就是介绍了正交因子模型以及它的参数估计问题。但是，到这里，我们还没有说到模型中的公共因子如何解释这个问题！对于这个问题的解释，通常结合实际的问题，需要一定的专业知识和经验，然后才能给每个公共因子给出一个实际意义。而且，公共因子的解释，在很大程度上也依赖于因子模型中因子载荷矩阵的元素结构！这个时候就会牵扯出因子分析中其它的一些问题：<strong>因子旋转</strong>和<strong>因子得分</strong>。因为这两个问题涉及到一些比较复杂的数学知识，我不能够在清减数学的情况下说好它，如果单说怎么用，我觉得没有必要，所以暂时并不打算介绍这两个问题。有兴趣的，可以翻阅一些多元统计的书籍，一般都会有讲。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习16:因子分析1]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/14/factor-analysis1/"/>
    <updated>2014-05-14T16:05:45+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/14/factor-analysis1</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical34.jpg" alt="artical 34" />
<!-- more --></p>

<p>图片来源于<a href="http://software.ssri.co.jp/statweb2/column/column0811.html">网址</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>前一篇介绍的主成分分析(PCA)，是一种降维技术；这一篇介绍的因子分析也是一种降维的方法，不仅如此，还可以将因子分析看作是主成分分析的一种推广和发展。与之主成分分析相比较，因子分析更为灵活，对变量降维后的解释能够更加清楚。</p>

<p>但因子分析和主成分分析有非常多的不同点。</p>

<ol>
  <li>主成分分析不能作为一个模型来描述，主成分是观测变量的线性组合；</li>
  <li>因子分析需要构造因子模型，观测的原始变量是因子的线性组合。</li>
</ol>

<hr />

<h4 id="section">初窥</h4>
<hr />

<p>在介绍因子模型之前，可以先看看这个因子分析到底是要干什么，以及是怎么干的！</p>

<p>在二维空间中，主成分分析，它想做的是寻找一组新的变量<script type="math/tex">y_1,y_2</script>，用它去替代原来的变量<script type="math/tex">x_1,x_2</script>，并且满足<script type="math/tex">y_1</script>和<script type="math/tex">y_2</script>这两个变量都是<script type="math/tex">x_1,x_2</script>两个变量的线性组合！即：</p>

<p><img src="/images/a34/eq1.jpg" alt="" /></p>

<p>而在因子模型中，我们需要做的跟此不同。我们需要找到一组潜在变量(不可观测)，用这组潜在变量的线性组合去表示原始变量<script type="math/tex">x_1,x_2</script>。这里假设有1个潜在变量<script type="math/tex">f_1</script>，那么因子模型可以描述成：</p>

<p><img src="/images/a34/eq2.png" alt="" /></p>

<p>其中，<script type="math/tex">f_1</script>就是因子，称为<strong>公共因子</strong>；<script type="math/tex">a_{ij}</script>称之为变量<script type="math/tex">x_i</script>在因子<script type="math/tex">f_j</script>上的<strong>载荷</strong>；<script type="math/tex">\mu_i</script>是<script type="math/tex">x_i</script>的均值；<script type="math/tex">\varepsilon_i</script>为特殊因子，即不能被公共因子解释的部分。</p>

<hr />

<h4 id="section-1">正交因子模型</h4>
<hr />

<p>首先看看最基础的因子模型，就是正交假设下的因子模型：</p>

<p><img src="/images/a34/eq3.png" alt="" /></p>

<p>在给出假定之前，我们先将上面式子转换成矩阵形式：</p>

<script type="math/tex; mode=display"> x = \mu + Af + \varepsilon </script>

<p>其中，<script type="math/tex">x = (x_1, x_2, \dots, x_p)'</script>，<script type="math/tex">\mu = (\mu_1, \mu_2, \dots, \mu_p)'</script>为均值向量，<script type="math/tex">\varepsilon = (\varepsilon_1, \varepsilon_2, \dots, \varepsilon_p)'</script>为特殊因子向量, <script type="math/tex">f = (f_1, f_2, \dots, f_p)'</script>为公共因子向量，<script type="math/tex"> A = (a_{ij}):p \times m </script>为载荷矩阵。那么我们就可以给出如下的正交假设：</p>

<p><img src="/images/a34/eq4.png" alt="" /></p>

<p>在这样的假定下，我们首先来计算一下，原始变量<script type="math/tex">x</script>的协方差：</p>

<script type="math/tex; mode=display"> \Sigma = V(x) = V(Af+\varepsilon) = Cov(Af+\varepsilon,Af+\varepsilon) </script>

<p>又：<script type="math/tex">Cov(Af+\varepsilon,Af+\varepsilon)=AV(f)A'+ACov(f,\varepsilon)+Cov(\varepsilon,f)A'+V(\varepsilon)</script></p>

<p>由于<script type="math/tex">V(f) = I, Cov(f, \varepsilon) = Cov(\varepsilon, f) = 0</script>，所以：</p>

<script type="math/tex; mode=display"> \Sigma = AA' + V(\varepsilon) = AA' + D </script>

<p><strong>显然，我们要处理正交因子模型，最重要的就是求解<script type="math/tex">A,D</script>的估计值，那么这里就给出了这两个量与原始变量的协方差矩阵间的关系。</strong></p>

<p><strong>那么我们开始所说的，因子分析也是一种降维手段体现在哪里呢？</strong>这个就体现在，公共因子的数量上，当公共因子的数量少于原始变量的数量时，使用因子去解释原始变量就达到了一种降维的目的！</p>

<hr />

<p><strong><em>载荷矩阵</em></strong></p>

<p>显然，载荷矩阵<script type="math/tex">A</script>是我们关心的一个重点。首先，我们想弄明白<script type="math/tex">A</script>中的元素<script type="math/tex">a_{ij}</script>是否有什么具体的含义：</p>

<script type="math/tex; mode=display">Cov(x_i,f_j)=Cov(\sum_{k=1}^{m}a_{ik}f_k + \varepsilon_i, f_j) =a_{ij}Cov(f_j,f_j) = a_{ij} </script>

<p>那么可以看到，<script type="math/tex">a_{ij}</script>是<script type="math/tex">x_i</script>和<script type="math/tex">f_j</script>之间的协方差函数。</p>

<p>经过上面的计算，我们容易得到：</p>

<script type="math/tex; mode=display">V(x_i) = a_{i1}^2 + a_{i2}^2 + \dots + a_{1m}^2 + V(\varepsilon_i)</script>

<p>记<script type="math/tex">h_i^2 = \sum_{j=1}^{m}a_{ij}^2</script>，那么上式可转化为：</p>

<script type="math/tex; mode=display"> (V(x_i) =) \sigma_{ii} = h_i^2 + \sigma_i^2, i=1,2,\dots,p</script>

<p>这样就将<script type="math/tex">x_i</script>的方差进行了一个分解，一部分由公共因子解释，即<script type="math/tex">h_i^2</script>，称为<strong>共性方差</strong>；另一部分由特殊因子解释，即<script type="math/tex">\sigma_i^2</script>，称为<strong>特殊方差</strong>。</p>

<hr />

<p>至此，因子分析的基础模型就介绍完了，下面剩下的就是如何去进行参数的估计，这一般有三种方法：主成分法、主因子法以及极大似然法。下一篇，我们就来详细说说因子分析的参数估计问题。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习15: 主成分分析]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/11/principal-components/"/>
    <updated>2014-05-11T11:35:10+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/11/principal-components</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical33.jpg" alt="artical 33" />
<!-- more --></p>

<p>图片为：本文实例数据得到的，前两个主成分的散点图！</p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在之前<a href="http://jackycode.github.io/blog/2014/04/02/linear-regression3/">线性回归3</a>提到多重共线性问题，当时说了一些解决这个问题的办法，其中一种就是今天要说的<strong>主成分分析</strong>。</p>

<p><strong>主成分分析</strong>，Principal Components Analysis，简称PCA，是变量选择的一种方法。其一般的目的就是：变量的降维和主成分的解释！当主成分用于聚类或者回归，这个时候就是在做变量的降维；而当用来分析变量，尤其是使用前两个主成分进行散点图的绘制时，此时就是在对变量利用主成分做出一些解释。在了解主成分分析的原理之后，相信对这两个目的就可以很容易直观的理解了！</p>

<hr />

<h4 id="section">一、原理初窥</h4>
<hr />

<p>在用数学公式和概率统计知识推导其原理之前，不妨先直观地看看主成分分析到底是要干嘛，以及大致是怎么干的！</p>

<p>我们就用两个变量来说这个问题，变量分别记作<script type="math/tex">x_1,x_2</script>。那么，我们先画个散点图吧，也许它们的散点图是这个样子的：</p>

<p><img src="/images/a33/images1.jpg" alt="" /></p>

<p>我们可以看到，这两个变量明显呈现一种线性关系，如果在做线性回归时，将这两个变量都用作自变量，然后对某一个因变量进行线性拟合，那必定会存在一些问题。那么主成分分析是要做什么呢？其实主成分分析就是要寻找变量<script type="math/tex">y_1,y_2</script>去替代<script type="math/tex">x_1,x_2</script>，而且满足<script type="math/tex">y_1,y_2</script>几乎不相关，同时<script type="math/tex">y_1,y_2</script>能够保留<script type="math/tex">x_1,x_2</script>所包含的信息。</p>

<p>那么主成分分析是如何做的呢？这个时候就需要考虑旋转坐标轴，当我们像下图那样旋转过坐标轴之后，上面提出的要求就得到了实现。</p>

<p><img src="/images/a33/images2.jpg" alt="" /></p>

<p>从这张图我们就可以看出，数据投影到<script type="math/tex">y_1,y_2</script>两轴后，数据基本不相关，而且在<script type="math/tex">y_1</script>轴就保留了原本数据的大部分信息，<script type="math/tex">y_2</script>保留了数据的另外一部分信息。由此可见，数据越是集中在<script type="math/tex">y_1</script>轴两侧，数据映射到<script type="math/tex">y_1</script>轴后保留的信息就越多，而<script type="math/tex">y_2</script>就越少。当<script type="math/tex">y_2</script>含有的信息非常少，少到接近于0时，那么此时就到达了变量选择的目的，因为此时只要保留<script type="math/tex">y_1</script>就可以了。</p>

<hr />

<h4 id="section-1">二、原理</h4>
<hr />

<p>我们将数据映射到<script type="math/tex">y_1,y_2</script>轴，其实就是将原数据做个线性变换。利用上面的内容举个简单地例子就是：</p>

<p><img align="center" src="http://jackycode.github.io/images/a33/eq1.jpg" /></p>

<p>其中的系数满足：<script type="math/tex">a_{11}^2 + a_{21}^2=1,a_{12}^2 + a_{22}^2=1</script>。这样就可以成功地将数据投影到<script type="math/tex">y_1,y_2</script>轴。这里我们考虑更一般的情况，考虑<script type="math/tex">p</script>维的情况：</p>

<p><img align="center" src="http://jackycode.github.io/images/a33/eq2.jpg" /></p>

<p>其中<script type="math/tex">a_i=(a_{1i},a_{2i},\dots,a_{pi})', i=1,2,\dots,p</script>，且满足<script type="math/tex">a_i'a_1=1</script>。</p>

<p>下面要考虑的就是，如何选择<script type="math/tex">a_1</script>，使得<script type="math/tex">V(y_1)</script>到达最大，找到之后，<script type="math/tex">y_1</script>就是<strong>第一主成分</strong>。</p>

<p>首先，<script type="math/tex">V(y_1)=a_1' \Sigma a_1</script>，其中<script type="math/tex">\Sigma=V(x)</script>为协方差矩阵。我们知道<script type="math/tex">\Sigma</script>是非负定的，那么其所有的特征值必定都是大于等于0的，我们可以排个序：<script type="math/tex">\lambda _{ 1 }\geqslant \lambda _{ 2 }\geqslant \dots \geqslant \lambda_p \geqslant 0</script>，其对应的特征向量记为：<script type="math/tex">t_1, t_2, \dots, t_p</script>，显然这些特征向量是相互正交的。记<script type="math/tex">T=(t_1, t_2, \dots, t_p)=(t_{ik}), \Lambda=diag(\lambda_1,\lambda_2,\dots,\lambda_p)</script>。</p>

<p>那么根据谱分解就有：<script type="math/tex">\Sigma=T \Lambda T'=\sum_{i=1}^{p}\lambda_it_it_i'</script>。那么带入到<script type="math/tex">V(y_1)</script>中就有：</p>

<script type="math/tex; mode=display">V(y_1)=\sum_{i=1}^{p} \lambda_i a_1' t_it_i' a_1 = \sum_{i=1}^{p} \lambda_i(a_1't_i)^2</script>

<p>由于特征值中<script type="math/tex">\lambda_1</script>是最大的，那么就有：</p>

<script type="math/tex; mode=display">V(y_1) \leqslant \lambda_1 \sum_{i=1}^{p} (a_1't_i)^2 = \lambda_1 \sum_{i=1}^{p} {a_1't_it_i'a_1} = \lambda_1 a_1' TT'a_1 = \lambda_1</script>

<p>可以看到最终的结果是：<script type="math/tex">V(y_1) \leqslant \lambda_1</script>。那么什么时候取等号呢？取<script type="math/tex">a_1 = t_1</script>，则有：</p>

<script type="math/tex; mode=display">t_1'\Sigma t_1 = t_1' (\lambda_1t_1) = \lambda_1</script>

<p>到止为止，就可以看到。当<script type="math/tex">y_1=t_1'x</script>时就有其<script type="math/tex">V(y_1)</script>达到最大，为<script type="math/tex">\lambda_1</script>。那么此时<script type="math/tex">y_1=t_1'x</script>就是该数据第一主成分。</p>

<p>同理可以求解第二主成分直至最后。<strong>但是，在求解第二第三主成分的时候，需要注意一个新的问题：主成分之间不相关，即<script type="math/tex">Cov(y_i,y_k)=0,i \neq k</script></strong>，如何证明非常简单，我就不说了，自己动动手吧。</p>

<hr />

<h4 id="section-2">三、具体实例</h4>
<hr />

<p>我们使用R语言来做个小例子：</p>

<p>``` r
# 构造一个数据集
set.seed(10)
x1 &lt;- seq(1, 50, 2) + rnorm(25, mean=1, sd=1)
x2 &lt;- 2*x1 + rnorm(25, mean=1, sd=1)
x3 &lt;- x1/2 + x2 + rnorm(25, mean=1, sd=1)
x &lt;- data.frame(x1=x1, x2=x2, x3=x3)</p>

<h1 id="section-3">计算协方差矩阵</h1>
<p>Sig &lt;- cov(x)</p>

<h1 id="section-4">计算特征值和特征向量</h1>
<p>eigen(Sig)
```
得到结果如下：</p>

<p>``` r
$values
[1] 2381.3724526    0.3681085    0.1165201</p>

<p>$vectors
           [,1]       [,2]        [,3]
[1,] -0.2982259 -0.4601189  0.83627265
[2,] -0.6002191 -0.5908325 -0.53912331
[3,] -0.7421579  0.6627273  0.09997061
```</p>

<p>得到了特征值为：<script type="math/tex">\lambda_1=238.372, \lambda_2=0.368, \lambda_3=0.11652</script>，可以看到第一主成分<script type="math/tex">y_1=t_1'x=-0.298x_1-0.600x_2-0.742x_3</script>的特征值<script type="math/tex">\lambda_1</script>的值远大于其余的(由于数据构造时就是以<script type="math/tex">x_1</script>为底的)。说明第一主成分能够解释数据的大部分信息，那么如何衡量呢？</p>

<p>这时候就需要使用到<strong>贡献率</strong>这个概念，某一个主成分<script type="math/tex">y_i</script>的贡献率定义为：<script type="math/tex">\frac{\lambda_i}{\sum_{i=1}^{p} \lambda_i}</script>。</p>

<p>贡献率越大说明这个主成分能够解释数据的信息就越多，在具体的问题中，还常常用到一个概念，就是<strong>累积贡献率</strong>。前<script type="math/tex">k</script>个主成分的累计贡献率就是：<script type="math/tex">\frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{p} \lambda_i}</script>。实际中，当前<script type="math/tex">k</script>个主成分的累积贡献率达到某个临界值，比如<script type="math/tex">80\%</script>，就选择前<script type="math/tex">k</script>个主成分进行下一步操作(比如说聚类，回归或者单纯地做分析等等)</p>

<h4 id="section-5">四、基于相关矩阵</h4>
<hr />

<p>考虑这样两种情况：各个变量的单位不全相同，也就是数据的量纲不同；各变量之间的单位相同，但是变量的方差较大，也就是数值大小相差较大。那么这个时候，如果从协方差矩阵出发求解主成分，就显得不大合适了。</p>

<p>在之前的文章中提到过，当所有的变量都进行了标准化之后，协方差矩阵<script type="math/tex">\Sigma</script>就转换成了相关矩阵<script type="math/tex">R</script>！</p>

<p>那么，剩下的求解过程就与上面相同了，这里不再叙述。需要指出的是，标准化与否，所得到的结果可能会有很大的不同，所以，判断一批数据是否需要标准化是很有必要的！</p>

<h4 id="r">五、R语言实现</h4>
<hr />

<p>主成分分析的R语言实现比较简单，可以直接使用<code>eigen()</code>函数求出特征值特征向量。当然也有自带的函数：<code>princomp()</code>以及<code>psych</code>包中的<code>principal()</code>函数，可以自己查找一下帮助文档，这里就不做介绍了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习14: 关联分析之apriori算法]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/07/apriori/"/>
    <updated>2014-05-07T17:00:48+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/07/apriori</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical31.jpg" alt="artical 31" />
<!-- more --></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<p>在上一篇中，我们介绍了关联分析相关的概念，这一节来看看如何使用Apriori算法去寻找满足条件的项集。</p>

<p>首先回顾一个概念，<strong>一个项集的支持度</strong>就是数据集中该项集所占的比例。Apriori算法就是用于寻找数据集中，支持度和可信度超过某一给定值的项集和关联规则。</p>

<hr />

<h4 id="section">一、原理</h4>
<hr />

<p>在介绍算法之前，首先了解一个集合论中的性质定理：集合的向下封闭性。</p>

<p>我们通过一个例子来看看这个定理，见下图：</p>

<p><img src="/images/a31/apriori_1.jpg" alt="" /></p>

<p>集合的向下封闭性，在这边解释的话，也就是说，<strong>如果一个项集的支持度低于某一个值，那么该项集超集的支持度也必定低于这个值。如果一个项集的支持度高于某一个值，那么该项集子集的支持度也必定高于某一个值。</strong></p>

<p><strong>超集</strong>：就是指包含这个集合中所以元素的集合（不包括自身），比如集合ABC就是集合AB的超集。</p>

<p>那么这个定理放在上一个图当中，就有这样的含义：</p>

<p><img src="/images/a31/apriori_2.jpg" alt="" /></p>

<hr />

<h4 id="section-1">二、算法构成</h4>
<hr />

<p>有了上面这个原理，那么就可以利用这个原理去减少我们寻找频繁集的计算量。因为，只要我们找到一个项集，其支持度低于给定的值，那么这个项集的所有超集就可以直接忽略不计了。如上图，项集A的支持度低于指定的值，那么其超集就都不用再考虑了。</p>

<p><strong>Apriori算法由两部分构成：</strong></p>

<ol>
  <li>找到满足最小支持度的项集；</li>
  <li>找到可信度超过最小可信度的关联规则。</li>
</ol>

<p>下面，我们一个一个地解决：</p>

<hr />

<h5 id="section-2">2.1. 寻找频繁项集</h5>
<hr />

<p>利用上面所讲的原理，我们来整理一下这个步骤的流程：</p>

<ol>
  <li>从数据集中构造集合C1，C1满足：大小为1的所有候选项集的集合，例如上图中的：C1 = {A, B, C};</li>
  <li>计算C1中所有项集（单元素项集）是否满足最小支持度，满足的项集构成集合L1，例如上图中的：L1 = {B, C};</li>
  <li>利用L1生成新的候选项集C2，C2满足：大小为2的所有候选项集的集合，例如上图中得：C2 = {BC};</li>
  <li>计算C2中所有项集（双元素项集）是否满足最小支持度，满足的项集构成集合L2；</li>
  <li>重复直到Lk中得元素个数为1。</li>
</ol>

<hr />

<h5 id="section-3">2.2. 寻找关联规则</h5>
<hr />

<p>在得到频繁项集之后，要寻找关联规则就容易多了。可以直接从频繁项集中构造初始的关联规则，计算该关联规则的可信度，然后与给定的最小可信度作比较，若值大于最小可信度，则记录该关联规则。</p>

<p>在实际编程时，需要注意使用<strong>集合的向下封闭性</strong>！！！想想看，在关联规则中，这个性质应该怎样去实现？(可以到Machine Learning in Action中找答案！)</p>

<h4 id="r">三、R语言实现</h4>

<h5 id="section-4">1. 使用自带的程序</h5>

<p>在R语言的<code>arules</code>这个包里面，提供了一个实现Apriori算法的函数：<code>apriori()</code>：</p>

<p><code>r
# 构造数据集
dataSet &lt;- matrix(0, 5, 3)
rownames(dataSet) &lt;- paste("item", 1:5, sep='')
colnames(dataSet) &lt;- c("A", "B", "C")
dataSet[1,] &lt;- c(1, 1, 0)
dataSet[2,] &lt;- c(1, 0, 1)
dataSet[3,] &lt;- c(1, 0, 1)
dataSet[4,] &lt;- c(1, 1, 1)
dataSet[5,] &lt;- c(0, 1, 1)
dataSet
# 转换数据格式(可以?apriori查看数据格式)
dataSet_class &lt;- as(dataSet,"transactions")
# 构造频繁项集
rules&lt;-apriori(dataSet_class,parameter=list(supp=0.5,conf=0.6,target="rules"))
# 查看结果
summary(rules)
# 构造关联规则
inspect(rules)
</code></p>

<h5 id="section-5">2. 自定义函数解决</h5>

<p>相对而言，Apriori算法的函数比较难以编写，原因可想而知，肯定是因为数据结构的问题！但是也只是比其他函数难编一点，毕竟其自带的数据结构功能还是非常强大的。我在<a href="/datascience">我的项目</a>中给出的一种编写方式，是利用R语言的list来实现的。不过，我想，利用Matrix或者data.frame，当然如果你还懂<code>data.table</code>的话，那么肯定也是可以编写的，而且我想应该会比用list简单！(没有亲手编写，只是猜想！)</p>

<p>详见<a href="/datascience">我的项目</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[数据科学之机器学习13: 关联分析]]></title>
    <link href="http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis/"/>
    <updated>2014-05-04T10:12:50+08:00</updated>
    <id>http://jackycode.github.io/blog/2014/05/04/associationg-ananlysis</id>
    <content type="html"><![CDATA[<p><img src="/images/artical/artical29.jpg" alt="artical 29" />
<!-- more --></p>

<p>标题图片出处：<a href="http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter002/section003/blue/page002.html">Rule Generation</a></p>

<p><em>“文章原创，转载请注明出处”</em></p>

<hr />

<h4 id="section">一、简介</h4>
<hr />

<p>在统计学中，变量与变量之间的关系是统计结构的重要参数，统计的核心问题也就是研究变量与变量之间的关系。如果变量与变量之间不独立，那么这两个变量之间肯定存在一定的关联性，那么如何处理度量这个关联性，在统计中就称为关联分析。</p>

<p>列联表是传统统计学中度量两个分类变量之间关系强弱的方法，但是这个方法是对于两个固定的变量进行的一种测量。在机器学习中，常常会遇到大规模的变量，这时候更倾向于从众多的变量中最快地找到关联性最强的两组或多组变量。那么使用列联表就显得不太合适了，此时应该使用什么方法呢？</p>

<hr />

<h4 id="section-1">二、关联规则</h4>
<hr />

<p>首先介绍2个定义：</p>

<ol>
  <li><strong>k项集：</strong>设<script type="math/tex">I = \{ i_1, i_2, \dots, i_m \}</script>是$m$个待研究的项构成的有限项集，对于给定的事物数据表<script type="math/tex">T = \{ T_1, T_2, \dots, T_n \}</script>，其中任意的$T_i$是$I$中的$k$项组成的集合，称之为<strong>k项集</strong>。</li>
  <li><strong>关联规则</strong>：形如<script type="math/tex"> X \rightarrow Y</script>的形式，其中<script type="math/tex"> X \subseteq I </script>, <script type="math/tex">Y \subseteq I</script>，且有<script type="math/tex"> X \bigcap Y = \emptyset </script>。</li>
</ol>

<hr />

<h5 id="section-2">度量方式</h5>
<hr />

<p>对于一个项集，我们正常用支持度来度量它的频繁程度，其实就是其在数据集中出现的比例，这个很容易理解，就不多说了。</p>

<p>那么下面要讨论的就是：<strong>如何度量一个关联规则</strong>。一般使用下面两个概念：</p>

<ol>
  <li><strong>支持度S</strong>：定义为X和Y同时出现在一个事务中得可能性，即：
 <script type="math/tex"> S(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T \mid </script>
 其中，<script type="math/tex">\mid T(X \vee Y) \mid</script>表示同时包含X和Y的事务数，<script type="math/tex">\mid T \mid</script>表示总事务数。</li>
  <li><strong>支持度C</strong>：定义为出现在关联规则前项中得事务中出现关联规则后项的比例，即：
 <script type="math/tex"> C(X \Rightarrow Y) = \mid T(X \vee Y) \mid / \mid T(X) \mid </script>
 其中，<script type="math/tex"> \mid T(X) \mid</script>表示包含X的事务数。</li>
</ol>

<hr />

<h5 id="section-3">例子</h5>

<hr />

<p>这边举个简单地例子，方便理解上面的概念。比如设计一个购物数据：</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">id</th>
      <th style="text-align: center">items</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">AB</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">AC</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">C</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: center">ABC</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: center">BC</td>
    </tr>
  </tbody>
</table>

<p>上面数据的意思，比如第一行就表示，id为1的人购买了A和B两个物品，其他的意思类似。那么这个AB就是一个2项集，因为存在ABC三种物品，这边就有6种关联规则：<script type="math/tex">A \Rightarrow B</script>, <script type="math/tex">A \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow C</script>, <script type="math/tex"> B \Rightarrow A </script>, <script type="math/tex"> C \Rightarrow A</script>, <script type="math/tex"> C \Rightarrow B </script>。</p>

<p>那么如何计算一个关联规则的支持度与可信度呢？</p>

<p>试试关联规则<script type="math/tex">A \Rightarrow B</script>：</p>

<script type="math/tex; mode=display"> S(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T \mid = 2/5 </script>

<script type="math/tex; mode=display"> C(A \Rightarrow B) =  \mid T(A \vee B) \mid / \mid T(A) \mid = 2/3 </script>

<p>是不是很简单呢。试试计算一下其他的关联规则，多算几次就能够很了解其中的含义了。</p>

<hr />

<h5 id="section-4">关联规则的作用</h5>
<hr />

<ol>
  <li>
    <p>关联规则的目的在于，找到变量之间支持度和可信度都比较高的那些关联规则。</p>
  </li>
  <li>
    <p>关联规则的支持度，用于度量关联规则在数据库中得普适程度，是对关联规则重要性(适用性)的一种度量。</p>
  </li>
  <li>
    <p>关联规则的可信度，这是一个相对指标，是对关联规则准确度的一个度量。值越高，代表这个关联规则的后项依赖前项的可能性比较高。</p>
  </li>
</ol>

<hr />

<h4 id="section-5">小节</h4>
<hr />

<p>这篇简单介绍了一下关联分析的一些概念问题，以及如何度量一个关联规则。但是，试想一想，要是$I$中包含的项很多，事物数据表$T$也很大，那么要计算所有关联规则的支持度和可信度，难度可想而知！这时候就需要使用一些算法去解决这个问题，现在比较流行的算法就是：处理静态关联规则的Apriori算法和处理动态关联规则的Carma算法。</p>
]]></content>
  </entry>
  
</feed>
