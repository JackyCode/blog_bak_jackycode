---
layout: post
title: "数据科学之机器学习8: 决策树之ID3"
date: 2014-04-14 16:02:45 +0800
comments: true
categories: DataScience MachineLearning DecisionTree Classification
---

![artical 23](/images/artical/artical23.jpg)
<!-- more -->

*“文章原创，转载请注明出处”*

***

ID3算法的核心问题就在于：如何选取在决策树的每个节点处要测试的属性。那么如何去选择呢？当然，我们要选取**分类能力最好的属性**，那么怎么去确定哪个属性是分类能力最好的呢？ID3算法中，使用**信息增益**作为评判标准。在看信息增益之前，我们先看看这个决策树的构造过程：

### 一、构造过程
***

1. 选取**分类能力最好的属性**作为决策树根部节点的测试；
2. 为根节点属性的每一个可能值产生一个分支；
3. 以各个分支节点为根节点，重复上述过程。

### 二、信息增益
***

#### 1. 熵
***

在看信息增益之前，首先需要介绍一个概念，那就是**香农熵**，简称为**熵**。相信学过物理的应该大都听过这个名词，在热力学中不就有个熵增原理嘛。其实，**熵是信息论中广泛使用的一个度量标准，刻画了任意样例集合的纯度。**

**熵是信息的期望值**，所以可以用熵来刻画一个数据集的纯度。若用$x_i,i=1,2,\dots,n$来表示数据集所包含的属性，那么这个数据集的熵为：

$$ H = - \sum_{i=1}^{n}{p(x_i)l(x_i)} $$

其中，$p(x_i)$表示选取$x_i$作为分类的最终类别的概率；$l(x_i)$为$x_i$的信息，定义为：$$ l(x_i) = - \log_2p(x_i)$$。

#### 2. 信息增益
***

有了熵之后就可以刻画一个数据集的纯度，也就是熵值。那么什么信息增益呢？

简单来说，**一个属性的信息增益就是：使用这个属性分割样例集合而导致的熵值降低**。那么要选取分类能力最好的属性，就是要选取使得信息增益最大的那个属性。

一个属性A对样例集合S的信息增益定义为：

$$ Gain(S, A) = H(S) - \sum_{v \in A} { \frac{\# S_v}{\# S} H(S_v) } $$

其中，$$S_v$$表示集合S中，属性A取值为$v$的那部分数据；$$\# S_v$$表示，集合S中，属性A取值为$v$的个数；$$\# S$$表示集合S中观测的个数。

#### 3. 简单的例子
***

|   序号  | age | income | buy_iphone |
|:-----:|:------:|:------------:|:----------------:|
| 1   | senior | high | yes |
| 2 | senior | low | no |
| 3 | youth | high | yes|
| 4 | youth | low | no |
| 5 | senior | high | yes |
|6 | youth | high | yes |
| 7 | senior | high | no |

考虑上面这个问题，我们来计算一下各个属性的信息增益。

首先，我们可以看到，这个数据集S最终分类buy_iphone有两种取值：$yes,no$。则数据集S的熵值为：

$$ H(S) = -\frac{4}{7} \log_2{\frac{4}{7} } - \frac{3}{7} \log_2{\frac{3}{7} }  \approx 0.985$$

若按照age分类，age有两个属性：$senior, youth$，分别有4个和3个。age = senior时，yes有2个，no有2个则有：

$$ H_{age}(S_{senior}) = -\frac{2}{4} \log_2{\frac{2}{4}} - \frac{2}{4} \log_2{\frac{2}{4}} \approx 1 $$

$$ H_{age}(S_{youth}) = -\frac{2}{3} \log_2 {\frac{2}{3}} - \frac{1}{3} \log_2 {\frac{1}{3}} \approx 0.918 $$

则有：

$$ H_{age}(S) = \frac{4}{7} \times 1 + \frac{3}{7} \times 0.918 = 0.965 $$

则age属性的信息增益为：

$$ H(S) - H_{age}(S) = 0.985 - 0.965 = 0.020 $$

那么属性income的信息增益怎么去计算，可以动手试试。

### 三、ID3算法的伪代码
***

##### 定义：
* data：为训练样本集
* label：为目标属性 （比如例子中的属性buy_iphone）
* attrs：出目标属性外，供算法学习测试使用的其它属性 （比如例子中的age和income属性）

##### 伪代码：
ID3(data, label, attrs)：

1. 创建决策树的Root节点；
2. 若lable中取值单一，则返回 `label=label` 的单节点树；
3. 若attrs为空，则返回 `label=（data中取值最多的那个label）` 的单节点树；
4. 否则：
	1. 选取attrs中分类能力最好的属性作为Root的决策属性，记为A；
	2. 对A的每一个可能取值vi：
		1. 在Root添加一个分支对应 `A = vi `；
		2. data_vi = data中 `A = vi` 的子集，label_vi 表示 data_vi 所对应的目标属性取值；
		3. 若 data_vi 为空集：
			1. 在新分支下加一个叶子节点，节点 `label =（data中取值最多的那个label）` ;
			2. 否则，加一个子树：ID3(data_vi, label_vi, attrs);
5. 结束
6. 返回Root

### 四、R语言实现
***

见[我的项目]({{ root_url }}/datascience)
