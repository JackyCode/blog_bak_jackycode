---
layout: post
title: "因子分析1"
date: 2014-05-14 16:05:45 +0800
comments: true
categories: DataScience
---

![article 34](/images/article/article34_new.jpg)
<!-- more -->

图片来源于[网址](http://software.ssri.co.jp/statweb2/column/column0811.html)

*“文章原创，转载请注明出处”*

***

*updata:*

感谢谢老大的提醒，让我发觉，在机器学习下放置因子分析并不合适。详细情况参见文章的回复板块。

***

前一篇介绍的主成分分析(PCA)，是一种降维技术；这一篇介绍的因子分析也是一种降维的方法，不仅如此，还可以将因子分析看作是主成分分析的一种推广和发展。与之主成分分析相比较，因子分析更为灵活，对变量降维后的解释能够更加清楚。

但因子分析和主成分分析有非常多的不同点。

1. 主成分分析不能作为一个模型来描述，主成分是观测变量的线性组合；
2. 因子分析需要构造因子模型，观测的原始变量是因子的线性组合。

***

#### 初窥
***

在介绍因子模型之前，可以先看看这个因子分析到底是要干什么，以及是怎么干的！

在二维空间中，主成分分析，它想做的是寻找一组新的变量$$y_1,y_2$$，用它去替代原来的变量$$x_1,x_2$$，并且满足$$y_1$$和$$y_2$$这两个变量都是$$x_1,x_2$$两个变量的线性组合！即：

![](/images/a34/eq1.jpg)

而在因子模型中，我们需要做的跟此不同。我们需要找到一组潜在变量(不可观测)，用这组潜在变量的线性组合去表示原始变量$$x_1,x_2$$。这里假设有1个潜在变量$$f_1$$，那么因子模型可以描述成：

![](/images/a34/eq2.png)

其中，$$f_1$$就是因子，称为**公共因子**；$$a_{ij}$$称之为变量$$x_i$$在因子$$f_j$$上的**载荷**；$$\mu_i$$是$$x_i$$的均值；$$\varepsilon_i$$为特殊因子，即不能被公共因子解释的部分。

***

#### 正交因子模型
***

首先看看最基础的因子模型，就是正交假设下的因子模型：

![](/images/a34/eq3.png)

在给出假定之前，我们先将上面式子转换成矩阵形式：

$$ x = \mu + Af + \varepsilon $$

其中，$$x = (x_1, x_2, \dots, x_p)'$$，$$\mu = (\mu_1, \mu_2, \dots, \mu_p)'$$为均值向量，$$\varepsilon = (\varepsilon_1, \varepsilon_2, \dots, \varepsilon_p)'$$为特殊因子向量, $$f = (f_1, f_2, \dots, f_p)'$$为公共因子向量，$$ A = (a_{ij}):p \times m $$为载荷矩阵。那么我们就可以给出如下的正交假设：

![](/images/a34/eq4.png)

在这样的假定下，我们首先来计算一下，原始变量$$x$$的协方差：

$$ \Sigma = V(x) = V(Af+\varepsilon) = Cov(Af+\varepsilon,Af+\varepsilon) $$

又：$$Cov(Af+\varepsilon,Af+\varepsilon)=AV(f)A'+ACov(f,\varepsilon)+Cov(\varepsilon,f)A'+V(\varepsilon)$$

由于$$V(f) = I, Cov(f, \varepsilon) = Cov(\varepsilon, f) = 0$$，所以：

$$ \Sigma = AA' + V(\varepsilon) = AA' + D $$

**显然，我们要处理正交因子模型，最重要的就是求解$$A,D$$的估计值，那么这里就给出了这两个量与原始变量的协方差矩阵间的关系。**

**那么我们开始所说的，因子分析也是一种降维手段体现在哪里呢？**这个就体现在，公共因子的数量上，当公共因子的数量少于原始变量的数量时，使用因子去解释原始变量就达到了一种降维的目的！

***

***载荷矩阵***

显然，载荷矩阵$$A$$是我们关心的一个重点。首先，我们想弄明白$$A$$中的元素$$a_{ij}$$是否有什么具体的含义：

$$Cov(x_i,f_j)=Cov(\sum_{k=1}^{m}a_{ik}f_k + \varepsilon_i, f_j) =a_{ij}Cov(f_j,f_j) = a_{ij} $$

那么可以看到，$$a_{ij}$$是$$x_i$$和$$f_j$$之间的协方差函数。

经过上面的计算，我们容易得到：

$$V(x_i) = a_{i1}^2 + a_{i2}^2 + \dots + a_{1m}^2 + V(\varepsilon_i)$$

记$$h_i^2 = \sum_{j=1}^{m}a_{ij}^2$$，那么上式可转化为：

$$ (V(x_i) =) \sigma_{ii} = h_i^2 + \sigma_i^2, i=1,2,\dots,p$$

这样就将$$x_i$$的方差进行了一个分解，一部分由公共因子解释，即$$h_i^2$$，称为**共性方差**；另一部分由特殊因子解释，即$$\sigma_i^2$$，称为**特殊方差**。

***

至此，因子分析的基础模型就介绍完了，下面剩下的就是如何去进行参数的估计，这一般有三种方法：主成分法、主因子法以及极大似然法。下一篇，我们就来详细说说因子分析的参数估计问题。
