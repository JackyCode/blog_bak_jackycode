---
layout: post
title: "数据科学之机器学习10: 聚类分析1"
date: 2014-04-21 18:46:04 +0800
comments: true
categories: DataScience MachineLearning Clustering
---

![article 25](/images/article/article25.jpg)
<!-- more -->

*“文章原创，转载请注明出处”*

***


上一篇介绍了聚类分析中的KMeans算法，这一节就来具体地说说聚类分析。聚类分析，cluster analysis，是一种研究“物以类聚”现代统计学分析方法，其目的是要把分类对象按照一定的规则分成若干个类。这些类别并非事先给定的，而是根据数据的特征确定的。

***

##### NOTE：聚类的划分：

1. 根据分类对象的不同，可以分为：**Q型聚类分析**和**R型聚类分析**。Q型是指对样品对象进行聚类；而R型则是对变量(属性)进行聚类。

2. 按照分析方法的不同，又可以分为：**系统聚类法**、**快速聚类法**和**模糊聚类法**。上一篇介绍的KMeans法就是快速聚类法中的一种。

***

#### 一、相似性的度量
***

在上一篇中，我们已经介绍过，聚类其实就是将相似度高的样品啊属性啊合并成一个类别。但是，上一篇我们仅仅给出了一种也是最简单的一种相似性的度量方式——欧式距离。这里我们详细看看相似性有哪些度量方式：

除了使用**有序尺度变量**（将属性划分为一级、二级等等的有次序关系的量来表示）和**名义尺度变量**（使用既没有等级关系，又不存在数量关系的量来表示。比如男女）之外，一般采用的测量尺度的方式就是**间隔尺度变量**。

**间隔尺度变量**即是使用连续的量来表示测量尺度，一般都是连续型的，比如欧式距离、重量等等。一般来讲，在应对Q型聚类时会使用**距离**去度量；而对R型聚类来说，则会使用**相似系数**这种方式去度量。下面来分别看一看：

***

##### a. 距离
***

上一篇中使用的欧氏距离即是这里的一种，在介绍各种不同的距离定义之前，首先看看距离的定义需要满足哪些条件：

1. 首先，距离必须是非负的。即：$$d_{ij} \geqslant 0, \forall i,j$$;
2. 对于相同取值的样品，之间的距离必须为0。即：$$d_{ij} = 0$$，当且仅当，第i个样品与第j个样品的各变量值相同；
3. i样品到j样品的距离与j样品到i样品的距离相等。即：$$d_{ij} = d_{ji}, \forall i, j$$；
4. 满足：$$d_{ij} \leqslant d_{ik} + d_{kj}, \forall i,j,k$$。

下面就来看看常用的距离定义，首先看看最常用的Minkowski距离：

***

###### (1). Minkowski距离

$$ d_{ij}(q) = [\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid ^ q}]^{1/q} $$

观察这个距离可以看到，当$q=2$时，上面定义的距离就是常用的欧氏距离。另外：

* $$q=1$$时，$$d_{ij}=\sum_{k=1}^{p} {\mid x_{ik} - x_{jk} \mid}$$称为**绝对值距离**；
* $$q=\infty$$时，$$d_{ij}=\max_{1 \leqslant k \leqslant p} {\mid x_{ik} - x_{jk} \mid}$$称为**切比雪夫距离**。

Minkowski距离存在一个问题，就是当变量的单位不同或者测量值范围相差很大时，直接使用Minkowski距离效果不佳。这个时候，应该先对数据进行**标准化**（就是减去均值除上标准差）之后再计算距离(这个后面还会说到)。

***

###### (2). Lance距离(Lance and Williams)

当$x_{ji} > 0$时，定义第i个样品到第j个样品的距离为：

$$ d_{ij} = \sum_{k=1}^{p} {\frac{\mid x_{ik} - x_{jk} \mid}{x_{ik} + x_{jk}}} $$

从公式就可以看出来，这个距离与变量之间的单位没有什么关系；而且其对异常值也不敏感，因而适用于一些高度偏斜的数据。

***

###### (3). Mahalanobis距离(马氏距离)

上面的两种距离都没有考虑变量之间的相关性问题，马氏距离就可以考虑到这个问题。但是由于马氏距离定义的问题，在聚类分析中使用马氏距离并不合适。但是这里也还是给出马氏距离的定义：

$$ d_{ij} = \sqrt{(x_i - x_j)^TS^{-1}(x_i - x_j)} $$

其中$$x_i = (x_{i1}, \dots, x_{ip})^T$$，$$x_j = (x_{j1}, \dots, x_{jp})^T$$，$$S$$为样本协方差阵。

***注：***为什么说马氏距离不适用与聚类分析呢？

聚类分析是无监督算法中的一种，无监督算法是什么？无监督算法是没有先验信息的，所有的数据拿过来是没有什么目标信息啊什么的。没有不同类之间的先验信息，那么协方差阵$$S$$就无法计算。因而，在实际聚类分析中，马氏距离并不适用。

***

###### (4). 斜交空间距离

$$ d_{ij} = [ \frac{1}{p^2} \sum_{k=1}^{p} \sum_{l=1}^{p} (x_{ik} -x_{jk})(x_{il} - x_{jl})r_{kl} ] ^ {1/2} $$

其中$$r_{kl}$$是变量$$x_k$$与变量$$x_l$$的相关系数。学过高等代数的应该可以很容易看明白这个定义。此外，当变量之间互不相关的时候，这里的$$d_{ij} = [d_{ij}(2)/p]_{Minkowski}$$，也就是退化到了欧氏距离（相差一个常数倍）。

***

##### b. 相似系数
***

对变量进行聚类时，通常使用相似系数来考量其间的相似度。那么相似系数的定义有需要满足哪些条件呢？

1. 完全相关。即：$c_{ij} = \pm 1$，当且仅当$x_i = ax_j + b;a(\neq 0),b$是常数；
2. $$\mid c_{ij} \mid \leqslant 1, \forall i, j $$；
3. $$c_{ij} = c_{ji}, \forall i, j $$。

下面看看常用的两种相似系数：

***

###### (1). 夹角余弦
***

变量$x_i$和$x_j$的夹角余弦的定义为：

$$ c_{ij} = \frac{\sum_{k=1}^{n} {x_{ki}x_{kj}} }{ [ (\sum_{k=1}^{n}{ x^2_{ki} })(\sum_{k=1}^{n} {x^2_{kj}} ) ]^{1/2} } $$

学过解析几何应该很容易看出这个定义的含义所在，其实$$c_{ij} = \cos \theta_{ij}$$。

***

###### (2). 相关系数
***

$$ c_{ij} = \frac { \sum _{ k=1 }^{ n }{ ({ x }_{ ki }-\overline{x_i})({ x }_{ kj }-\overline{x_j}) }  }{ \{ [\sum_{k=1}^{n}({ x }_{ ki }-\overline{x_i})^2][\sum_{k=1}^{n}({ x }_{ kj }-\overline{x_j})^2] \}^{1/2} } $$

这里的相关系数其实就是统计里面通常所说的相关系数。其实，如果变量都是标准化了的，那么夹角余弦就是相关系数，看出来了吗？

***

#### 小节
***

到这边，就把统计中常用的用于度量相似性的定义讲了一些。这些定义，大都有其自身的数学背景。有些来自于几何学，有些来自于线性空间理论。对于使用者来说，搞明白什么时候选择什么样的度量方式更加重要！下一篇，我们讲一讲聚类分析中的一个常用方法：**系统聚类法**。