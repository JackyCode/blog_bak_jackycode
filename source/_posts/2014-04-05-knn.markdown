---
layout: post
title: "数据科学之机器学习5：分类之k-近邻算法"
date: 2014-04-05 19:59:29 +0800
comments: true
categories: DataScience MachineLearning Classification
---

![artical 19](/images/artical/artical19.jpg)
<!-- more -->

*“文章原创，转载请注明出处”*

***

#### 基本思想
***

kNN，k-Nearest Neighbor algorithm，也就这边的k-近邻算法，是数据挖掘十大算法之一，是一个比较简单的分类方法。

其基本的思想是：对于一个输入样本（未知分类的样本），考虑其与测试样本中与之距离最近（特征最相似）的k个样本，用这k个样本中出现最多的分类作为输入样本的分类。

#### 具体流程
***

对于输入样本中的每一个点，进行以下操作：

1. 计算点与测试样本中点的距离；
2. 取出与当前点距离最小的k个点；
3. 确定k个点的分类，计算各个分类的频数；
4. 返回频数最高的类别，作为该输入点的预测分类。

#### 距离的计算
***

上面一直在说，计算输入样本中点与测试样本中点之间的距离，那么这个距离应该怎么计算呢？这个距离一般就是使用欧式距离：

$$ d = \sqrt{(x - y)^T(x - y)} $$

其中$$x^T=[x_1, x_2,\dots,x_n], y^T=[y_1,y_2,\dots,y_n]$$。二维的表示就是：

$$ d = \sqrt{(x - y)^T(x - y)} = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2} $$

#### R语言实现
***

见[我的github](https://github.com/JackyCode/Data_Science/tree/master/kNN)。