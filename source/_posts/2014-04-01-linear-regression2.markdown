---
layout: post
title: "数据科学之机器学习2：线性回归2"
date: 2014-04-01 16:56:57 +0800
comments: true
categories: DataScience MachineLearning
---

![artical 17](/images/artical/artical17.jpg)
<!-- more -->

*“文章原创，转载请注明出处”*

***

上一篇简单介绍了线性回归中系数估计的问题，给出了一元以及多元情况下，系数估计值的表达式！此外，还给出了在R语言中如何使用自带的函数计算系数估计值。

这一篇，打算介绍一下线性回归中的一些显著性检验问题。这个就是完全属于统计学中的理论内容，不过依旧有着很强的实际意义。简单来说，显著性检验不能通过，说明你的线性回归的效果不好，可能你就需要选择其它的方式去处理你手头的数据，而不是选择线性回归了。

***

#### 一、回归方程的显著性检验

***

考虑回归方程是否显著，意思就是查看自变量$$x_1,x_2,\dots,x_p$$从整体上是否对因变量$$y$$有显著的影响。则，我们可以考虑这样的假设检验问题：

$$ H_0:\beta_0=\beta_1=\dots=\beta_p=0;~H_1:\beta_0,\beta_1,\dots,\beta_p\text{不全为0}$$

显然，如果原假设成立的话，自变量对因变量的影响不大，也就是用线性回归模型来解释就显得不合适了。

在正态假设下，原假设$$H_0$$成立时，有$$F$$检验统计量：

$$ F = \frac{SSR/p}{SSE/(n-p-1)} \sim F(p,n-p-1) $$

其中$$SSR=\sum_{i=1}^{n}{(\hat{y}_i-\overline{y})^2}$$为回归平方和，$$SSE=\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}$$为残差平方和。对于给定的显著性水平$\alpha$，拒绝域为：$$\{F \geqslant F_{1-\alpha}(p,n-p-1)\}$$。

***

#### 二、回归系数的显著性检验

***

显然，线性回归中很有可能就存在某个自变量对因变量的影响很小，那么它的回归系数就会接近0.因此有如下的假设检验问题：

$$H_{0j}:\beta_j = 0; ~ H_{1j}:\beta_j \neq 0, ~ j=0,1,\dots,p$$

在原假设成立的条件下，$t$统计量有：

$$t_j = \frac{\hat{\beta}_j}{\sqrt{c_{jj}}\hat{\sigma}} \sim t(n-p-1)$$

其中，$$(c_{ij})=(X^TX)^{-1},i,j=0,1,\dots,p;~\hat{\sigma}=\sqrt{\frac{1}{n-p-1}\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}}$$。对于给定的显著性水平$\alpha$，拒绝域为：$$\{\|t_j\| \geqslant t_{\alpha/2}\}$$。

***

#### 三、R语言中的实现

***

在上一篇中可以看到，`lm`函数加上`summary`函数会有很多的输出内容。其实，那些输出中就含有上述的假设检验的结果，很容易就可以找到，这里不做阐述！

我自己也写了一个关于线性回归的R语言函数，托管在[我的github](https://github.com/JackyCode/Data_Science/tree/master/Linear_Regression)上面，函数内部对于线性回归的过程大都涉及到了，有兴趣可以看看。